{
  "db": [
    {
      "meta": {
        "exported_on": 1755719910360,
        "version": "5.130.3-2-ge799452"
      },
      "data": {
        "benefits": [],
        "custom_theme_settings": [],
        "newsletters": [
          {
            "id": "6270fc10d5472a0031f5f827",
            "name": "Paul Prae",
            "description": "Cognitive computing gives people superpowers.",
            "slug": "default-newsletter",
            "sender_email": "",
            "sender_reply_to": "newsletter",
            "status": "active",
            "visibility": "members",
            "subscribe_on_signup": 1,
            "sort_order": 0,
            "header_image": null,
            "show_header_icon": 1,
            "show_header_title": 1,
            "title_font_category": "sans_serif",
            "title_alignment": "center",
            "show_feature_image": 1,
            "body_font_category": "sans_serif",
            "footer_content": null,
            "show_badge": 1,
            "sender_name": null,
            "created_at": "2022-05-03T10:55:28.000Z",
            "updated_at": null,
            "show_header_name": 0,
            "uuid": "19cfbdd6-8fc8-4a5d-a26d-b20c58ca519e",
            "feedback_enabled": 0,
            "show_post_title_section": 1,
            "show_comment_cta": 1,
            "show_subscription_details": 0,
            "show_latest_posts": 0,
            "background_color": "light",
            "post_title_color": null,
            "show_excerpt": 0,
            "button_corners": "rounded",
            "button_style": "fill",
            "title_font_weight": "bold",
            "link_style": "underline",
            "image_corners": "square",
            "header_background_color": "transparent",
            "section_title_color": null,
            "divider_color": null,
            "button_color": "accent",
            "link_color": "accent"
          }
        ],
        "offer_redemptions": [],
        "offers": [],
        "posts": [
          {
            "id": "5a846c0742d1b300183a2064",
            "uuid": "b6711418-2183-4ae9-9434-d73192f65200",
            "title": "About Paul",
            "slug": "about",
            "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"![paul-prae](__GHOST_URL__/content/images/2021/03/paul-prae.jpg)\\n\\nPaul is a full-stack enterprise application developer and cloud architect who specializes in artificial intelligence. He's worked at Microsoft, Red Ventures, and Slalom Consulting delivering solutions to the world’s biggest brands. He works with Fortune 500 companies on multi-million-dollar projects that often impact thousands of users. He's worked across many industries with most of his experience in digital marketing and healthcare. His current focus is in machine learning, natural language processing, and conversational interfaces. \\n<br/><br/>\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p><img src=\"__GHOST_URL__/content/images/2021/03/paul-prae.jpg\" alt=\"paul-prae\" loading=\"lazy\"></p>\n<p>Paul is a full-stack enterprise application developer and cloud architect who specializes in artificial intelligence. He's worked at Microsoft, Red Ventures, and Slalom Consulting delivering solutions to the world’s biggest brands. He works with Fortune 500 companies on multi-million-dollar projects that often impact thousands of users. He's worked across many industries with most of his experience in digital marketing and healthcare. His current focus is in machine learning, natural language processing, and conversational interfaces.<br>\n<br/><br/></p>\n<!--kg-card-end: markdown-->",
            "comment_id": "1",
            "plaintext": "\n\nPaul is a full-stack enterprise application developer and cloud architect who\nspecializes in artificial intelligence. He's worked at Microsoft, Red Ventures,\nand Slalom Consulting delivering solutions to the world’s biggest brands. He\nworks with Fortune 500 companies on multi-million-dollar projects that often\nimpact thousands of users. He's worked across many industries with most of his\nexperience in digital marketing and healthcare. His current focus is in machine\nlearning, natural language processing, and conversational interfaces.",
            "feature_image": "__GHOST_URL__/content/images/2015/07/20150707_135231-1.jpg",
            "featured": 1,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2015-07-19T21:10:31.000Z",
            "updated_at": "2021-03-09T16:42:01.000Z",
            "published_at": "2015-07-19T21:10:31.000Z",
            "custom_excerpt": "Paul is a full-stack enterprise application developer and cloud architect who specializes in artificial intelligence.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "page",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "5afb75a2435d6900bf1fdad1",
            "uuid": "2252daef-0898-4ab7-9ddc-1cc7d202e31b",
            "title": "Meet Neona, a Chatbot for Learning AI",
            "slug": "meet-neona-a-chatbot-for-learning-ai",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I created a chatbot to help people understand artificial intelligence. She's named [Neona](http://neona.chat/).\\n\\n![neona-flat](__GHOST_URL__/content/images/2018/05/neona-flat.png)\\n\\nBy interacting with her, you can explore concepts from across the field of artificial intelligence. By studying her design and architecture, you can learn how to build conversational agents. In the future, she will be used by students to find courses and jobs in the field of AI. From the perspective of universities and employers, she will be used as a teaching assistant and a recruiting tool. In [one of my blog posts](https://blog.paulprae.com/neona-a-conversational-agent-that-teaches-ai-2/), I describe her technical architecture and the artificial intelligence concepts used in her design.\\n\\n![cloud-bot-architecture-branded](__GHOST_URL__/content/images/2018/06/cloud-bot-architecture-branded.PNG)\\n\\nCheck out her website to learn more: http://neona.chat/. A few demos are available if you'd like to chat with her! You can even [contribute on GitHub](https://github.com/praeducer/conversational-agent) if you'd like.\\n\\n<iframe src=\\\"//www.slideshare.net/slideshow/embed_code/key/nlbmllKCHUnE1c\\\" width=\\\"595\\\" height=\\\"485\\\" frameborder=\\\"0\\\" marginwidth=\\\"0\\\" marginheight=\\\"0\\\" scrolling=\\\"no\\\" style=\\\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\\\" allowfullscreen> </iframe> <div style=\\\"margin-bottom:5px\\\">\\n\\nI also have [a presentation on SlideShare](https://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture) where you can learn more about AI, AI at Microsoft, and how I built Neona on Microsoft Azure.\\n\\nI leveraged Neona to win some business with [Travelers](https://www.travelers.com/), a Fortune 100 insurance company. Through [Slalom Consulting](https://www.slalom.com/), I performed a solution assessment comparing the Azure Bot Service to Amazon Lex. In order to properly compare the Microsoft bot ecosystem to that of Amazon’s, I designed an identical conversational interface for both. The purpose of the bots was to automate common IT support tasks. Though the flow of the conversation was the same for both bots, the back-end implementation was completely different. I led the conversational interface design for the entire project and the development of the Amazon Lex bot. Along the way, I also delivered learning sessions to their team about the bot development life cycle. Just like with Neona, I was out to empower others to build their own AI systems!\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>I created a chatbot to help people understand artificial intelligence. She's named <a href=\"http://neona.chat/\">Neona</a>.</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/05/neona-flat.png\" alt=\"neona-flat\" loading=\"lazy\"></p>\n<p>By interacting with her, you can explore concepts from across the field of artificial intelligence. By studying her design and architecture, you can learn how to build conversational agents. In the future, she will be used by students to find courses and jobs in the field of AI. From the perspective of universities and employers, she will be used as a teaching assistant and a recruiting tool. In <a href=\"https://blog.paulprae.com/neona-a-conversational-agent-that-teaches-ai-2/\">one of my blog posts</a>, I describe her technical architecture and the artificial intelligence concepts used in her design.</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/06/cloud-bot-architecture-branded.PNG\" alt=\"cloud-bot-architecture-branded\" loading=\"lazy\"></p>\n<p>Check out her website to learn more: <a href=\"http://neona.chat/\">http://neona.chat/</a>. A few demos are available if you'd like to chat with her! You can even <a href=\"https://github.com/praeducer/conversational-agent\">contribute on GitHub</a> if you'd like.</p>\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/key/nlbmllKCHUnE1c\" width=\"595\" height=\"485\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\">\n<p>I also have <a href=\"https://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture\">a presentation on SlideShare</a> where you can learn more about AI, AI at Microsoft, and how I built Neona on Microsoft Azure.</p>\n<p>I leveraged Neona to win some business with <a href=\"https://www.travelers.com/\">Travelers</a>, a Fortune 100 insurance company. Through <a href=\"https://www.slalom.com/\">Slalom Consulting</a>, I performed a solution assessment comparing the Azure Bot Service to Amazon Lex. In order to properly compare the Microsoft bot ecosystem to that of Amazon’s, I designed an identical conversational interface for both. The purpose of the bots was to automate common IT support tasks. Though the flow of the conversation was the same for both bots, the back-end implementation was completely different. I led the conversational interface design for the entire project and the development of the Amazon Lex bot. Along the way, I also delivered learning sessions to their team about the bot development life cycle. Just like with Neona, I was out to empower others to build their own AI systems!</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "5afb75a2435d6900bf1fdad1",
            "plaintext": "I created a chatbot to help people understand artificial intelligence. She's\nnamed Neona [http://neona.chat/].\n\n\n\nBy interacting with her, you can explore concepts from across the field of\nartificial intelligence. By studying her design and architecture, you can learn\nhow to build conversational agents. In the future, she will be used by students\nto find courses and jobs in the field of AI. From the perspective of\nuniversities and employers, she will be used as a teaching assistant and a\nrecruiting tool. In one of my blog posts\n[https://blog.paulprae.com/neona-a-conversational-agent-that-teaches-ai-2/], I\ndescribe her technical architecture and the artificial intelligence concepts\nused in her design.\n\n\n\nCheck out her website to learn more: http://neona.chat/. A few demos are\navailable if you'd like to chat with her! You can even contribute on GitHub\n[https://github.com/praeducer/conversational-agent] if you'd like.\n\nI also have a presentation on SlideShare\n[https://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture] \nwhere you can learn more about AI, AI at Microsoft, and how I built Neona on\nMicrosoft Azure.\n\nI leveraged Neona to win some business with Travelers\n[https://www.travelers.com/], a Fortune 100 insurance company. Through Slalom\nConsulting [https://www.slalom.com/], I performed a solution assessment\ncomparing the Azure Bot Service to Amazon Lex. In order to properly compare the\nMicrosoft bot ecosystem to that of Amazon’s, I designed an identical\nconversational interface for both. The purpose of the bots was to automate\ncommon IT support tasks. Though the flow of the conversation was the same for\nboth bots, the back-end implementation was completely different. I led the\nconversational interface design for the entire project and the development of\nthe Amazon Lex bot. Along the way, I also delivered learning sessions to their\nteam about the bot development life cycle. Just like with Neona, I was out to\nempower others to build their own AI systems!",
            "feature_image": "__GHOST_URL__/content/images/2018/05/neona_face.jpg",
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2018-05-16T00:04:50.000Z",
            "updated_at": "2021-03-08T22:41:33.000Z",
            "published_at": "2018-05-16T00:35:17.000Z",
            "custom_excerpt": "By interacting with her, you can explore concepts from across the field of artificial intelligence. By studying her design and architecture, you can learn how to build conversational agents.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "5afb7f78435d6900bf1fdad3",
            "uuid": "80eb4f0d-d77c-4984-b932-c8e1bd20533a",
            "title": "Predicting the Future with Azure Machine Learning",
            "slug": "predicting-the-future-with-azure-machine-learning",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"<iframe src=\\\"//www.slideshare.net/slideshow/embed_code/key/EsRgXr7ZsVMWyH\\\" width=\\\"595\\\" height=\\\"485\\\" frameborder=\\\"0\\\" marginwidth=\\\"0\\\" marginheight=\\\"0\\\" scrolling=\\\"no\\\" style=\\\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\\\" allowfullscreen> </iframe> <div style=\\\"margin-bottom:5px\\\">\\n\\nIn this presentation, I focus on supervised learning, a machine learning technique for performing predictive analytics. After introducing some vocabulary, I discuss the relationship between predictive analytics and machine learning. Next, I describe how you could use a classifier, such as a decision tree, to predict which passengers survived the sinking of the Titanic. Once the machine learning process is clear, I then talk about how Azure Machine Learning is an end-to-end data science solution. Finally, I demo an experiment using real world data from www.healthdata.gov.\\n\\n![substance-abuse-treatment-data-set](__GHOST_URL__/content/images/2018/05/substance-abuse-treatment-data-set.png)\\n\\nMy solution predicts the outcomes of patients who went through substance abuse treatment.\\n\\n![Azure-ML-screenshot](__GHOST_URL__/content/images/2018/05/Azure-ML-screenshot.png)\\n\\nThis demo led to a project at the [Georgia Department of Behavioral Health and Developmental Disabilities](https://dbhdd.georgia.gov/). In order to reduce costs, optimize staffing, and support the budgeting process, we predicted demand for behavioral health crisis services in dozens of facilities across the state of Georgia. I led the data engineering, data visualization, and data storytelling parts of the project. I integrated data into a data warehouse from several previously siloed systems. While performing our analysis for this project, we also helped the organization improve their data governance and data science process. It was fun and rewarding!\\n\\n![final-presentation-at-DBHDD](__GHOST_URL__/content/images/2018/05/final-presentation-at-DBHDD.jpg)\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><iframe src=\"//www.slideshare.net/slideshow/embed_code/key/EsRgXr7ZsVMWyH\" width=\"595\" height=\"485\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\">\n<p>In this presentation, I focus on supervised learning, a machine learning technique for performing predictive analytics. After introducing some vocabulary, I discuss the relationship between predictive analytics and machine learning. Next, I describe how you could use a classifier, such as a decision tree, to predict which passengers survived the sinking of the Titanic. Once the machine learning process is clear, I then talk about how Azure Machine Learning is an end-to-end data science solution. Finally, I demo an experiment using real world data from www.healthdata.gov.</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/05/substance-abuse-treatment-data-set.png\" alt=\"substance-abuse-treatment-data-set\" loading=\"lazy\"></p>\n<p>My solution predicts the outcomes of patients who went through substance abuse treatment.</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/05/Azure-ML-screenshot.png\" alt=\"Azure-ML-screenshot\" loading=\"lazy\"></p>\n<p>This demo led to a project at the <a href=\"https://dbhdd.georgia.gov/\">Georgia Department of Behavioral Health and Developmental Disabilities</a>. In order to reduce costs, optimize staffing, and support the budgeting process, we predicted demand for behavioral health crisis services in dozens of facilities across the state of Georgia. I led the data engineering, data visualization, and data storytelling parts of the project. I integrated data into a data warehouse from several previously siloed systems. While performing our analysis for this project, we also helped the organization improve their data governance and data science process. It was fun and rewarding!</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/05/final-presentation-at-DBHDD.jpg\" alt=\"final-presentation-at-DBHDD\" loading=\"lazy\"></p>\n<!--kg-card-end: markdown-->",
            "comment_id": "5afb7f78435d6900bf1fdad3",
            "plaintext": " In this presentation, I focus on supervised learning, a machine learning\ntechnique for performing predictive analytics. After introducing some\nvocabulary, I discuss the relationship between predictive analytics and machine\nlearning. Next, I describe how you could use a classifier, such as a decision\ntree, to predict which passengers survived the sinking of the Titanic. Once the\nmachine learning process is clear, I then talk about how Azure Machine Learning\nis an end-to-end data science solution. Finally, I demo an experiment using real\nworld data from www.healthdata.gov.\n\n\n\nMy solution predicts the outcomes of patients who went through substance abuse\ntreatment.\n\n\n\nThis demo led to a project at the Georgia Department of Behavioral Health and\nDevelopmental Disabilities [https://dbhdd.georgia.gov/]. In order to reduce\ncosts, optimize staffing, and support the budgeting process, we predicted demand\nfor behavioral health crisis services in dozens of facilities across the state\nof Georgia. I led the data engineering, data visualization, and data\nstorytelling parts of the project. I integrated data into a data warehouse from\nseveral previously siloed systems. While performing our analysis for this\nproject, we also helped the organization improve their data governance and data\nscience process. It was fun and rewarding!",
            "feature_image": "__GHOST_URL__/content/images/2018/05/Azure-ML-screenshot-1.png",
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2018-05-16T00:46:48.000Z",
            "updated_at": "2021-03-08T22:40:44.000Z",
            "published_at": "2018-05-16T01:19:10.000Z",
            "custom_excerpt": "In this presentation, I focus on supervised learning, a machine learning technique for performing predictive analytics. I demo an experiment using real world data. My solution predicts the outcomes of patients who went through substance abuse treatment.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "5b12d98dc338f600bf8b90a7",
            "uuid": "2bc9e82a-0132-4b2d-981c-365f1ba7fa46",
            "title": "Natural Language Processing Systems",
            "slug": "natural-language-processing-systems",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Within the field of artificial intelligence, I specialize in natural language processing (NLP). [According to SAS](https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html), \\\"NLP is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.\\\" Within NLP, I then further focus on conversational interface design and development.\\n\\nOver the years, I captured NLP experience wherever possible. I've been lucky enough to be involved on many projects throughout the sales life cycle and solutions delivery process. I've created marketing materials and sales proposals that won work in the area of NLP. I've also architected and led the development of NLP systems. In this post, I'll share some system and process designs that I've created along my career journey.\\n\\n## High-Level Text Mining Process\\n![high-level-text-mining-process](__GHOST_URL__/content/images/2018/06/high-level-text-mining-process.jpg)\\n\\nI created this design to provide a simple overview of the types of data that can be processed and how that data can be processed. It was used in a sales conversation with [Apple](https://www.apple.com/).\\n\\n## Chatbot Development Phases\\n![chatbot-development-phases](__GHOST_URL__/content/images/2018/06/chatbot-development-phases.png)\\n\\nI created this design to describe the features stakeholders can expect a bot to have during different phases of development. The software development life cycle can be applied to chatbots. With each iteration, the system becomes more and more intelligent!\\n\\n## A Cloud Bot Architecture\\n![a-cloud-bot-architecture](__GHOST_URL__/content/images/2018/06/a-cloud-bot-architecture.jpg)\\n\\nThis diagram describes the architecture I used to build [Neona](http://neona.chat/). Each of these components plays a powerful part in the complete composition.\\n\\n### Azure\\nAn open, flexible, enterprise-grade cloud computing platform. Neona uses Azure Functions to process events with a serverless code architecture. These functions are responsible for extracting data from Wikipedia, processing it into the desired form, and storing it in Cosmos DB.\\n\\n### Node.js\\nAn event-driven I/O server-side JavaScript environment. The Azure Functions were written in Node.js. It was used to call the MediaWiki action API, a web service that provides convenient access to wiki features, data, and meta-data over HTTP. Using Node.js’ request module (and JavaScript Promises), the API is called with very carefully constructed queries to return just the data Neona would need.\\n\\n### Cosmos DB\\nA distributed database service for managing JSON documents at Internet scale. It is a highly-flexible key-value store that integrates closely with other Microsoft systems. It is Neona's knowledge-base.\\n\\n### Azure Search\\nA fully managed search-as-a-service in the cloud. It indexes the contents of the knowledge-base and provides an HTTP endpoint Neona can hit to query the knowledge-base. It natively supports Cosmos DB.\\n\\n### Bot Framework\\nThis framework provided an impressive amount of functionality and was the inspiration for this project. For Neona, it provides the connection to the various channels, manages incoming and outgoing messages, and helps orchestrate the dialogue flow. It also integrates naturally with LUIS, a fast and effective way of adding language understanding to applications.\\n\\nYou can learn more about Neona in [my portfolio post about her](__GHOST_URL__/meet-neona-a-chatbot-for-learning-ai/).\\n\\n## Bot-to-Human Handoff\\nA common request from customers is to design systems where humans can take over the conversation when necessary.\\n\\n### System Requirements\\nThere are several things to consider when transitioning to a human.\\n\\n##### User Scenarios\\nUser scenarios may include triage, escalation, and supervision.\\n\\n##### Transition Initiation\\nThe transition could be initiated by the user, the agent, or an automated system.\\n\\n##### Transition Detection\\nThe chat bot could automatically detect when a transition needs to occur based on sentiment analysis or by detecting certain entities or intents.\\n\\n##### Type of Human Interaction\\nThe human could engage via a support ticket, phone, text, or a chat platform.\\n\\n### System Architecture\\nHere is a technical architecture diagram I created for a bot-to-human hand-off system built on Microsoft Azure.\\n![bot-to-human-handoff](__GHOST_URL__/content/images/2018/06/bot-to-human-handoff.png)\\n\\n##### Features\\n1. A dashboard for agents to view live conversations where the agent can take control if necessary.\\n2. Custom middleware to connect users and agents together.\\n3. A database of conversations where you can perform in-house analytics.\\n\\n## Knowledge Transfer Module\\nWhile working at [Decooda](http://decooda.com/), I built a system, called the Knowledge Transfer Module (KTM), for automating the labeling of data. The KTM creates high-quality training data at enterprise scale. It helps train, test, and tune machine learning algorithms for everything from search relevance and sentiment analysis to conversational agents.\\n\\n![Knowledge-Transfer-Module-Architecture---1.0](__GHOST_URL__/content/images/2018/06/Knowledge-Transfer-Module-Architecture---1.0.png)\\n\\nBy annotating data with this system, business analysts were able to contribute to our knowledge base and improve the quality of predictions made by Decooda’s machine learning classifiers.\\n\\n### Application Architecture\\nThe client application was a native application that initially targeted Windows and Mac desktops with Android and iOS mobile support to follow. I wanted to use a technology stack and architecture that lends itself easily to rapid, iterative development. Any frameworks, libraries, and languages I chose not only needed to be reliable and easily maintainable, but also leading edge and feature rich. This required a careful balance. I wanted to use the best production-ready tools out there, but also wanted to create a technology stack that will have broad community support for years to come.\\n\\n##### Electron\\n“Electron is a framework for creating native applications with web technologies like JavaScript, HTML, and CSS.”\\nhttps://electronjs.org/\\n\\nElectron allowed us to take advantage of the massive web development ecosystem while developing a cross-platform (Mac, Windows, and Linux) desktop application. It not only allowed us to use any front-end development libraries or frameworks to create the user interface, but also allowed us to take advantage of Node.js and its package ecosystem for running more complex processes (like [data processing](https://github.com/adaltas/node-csv)) on the client machine. Plus, since Electron is essentially a container for a client-side web application, the code was written in a way that made it portable to other platforms. This allowed for an easier migration as we targeted web applications. Eventually, we'll also be able to target native mobile applications through tools like [React Native](https://facebook.github.io/react-native/).\\n\\n##### JavaScript, HTML, CSS\\nThe three standard powerhouse languages of the modern web were used to develop the client application. This allowed for common web tooling and design patterns to be applied to the application, improving the speed and ease of development. JavaScript was used for things like application logic, state management, and generating UI components. By also exploiting the availability of Node.js and its package ecosystem (the world’s largest ecosystem of open source libraries) through Electron, JavaScript performed even more complex computation as necessary. Of course, HTML was used to define the structure of the page and CSS was used to describe the presentation.\\n\\n##### React\\n“A JavaScript library for building user interfaces.”\\nhttps://reactjs.org/\\n\\nSince Electron supports the standard web stack, React was a perfect fit. React helps developers create reusable UI components while also encouraging good software design patterns like proper encapsulation and separation of the state from the DOM. To make applications more interactive, React updates and renders the view as data changes. React positioned us well to share any UI components we created with any future web applications we build with React or with any mobile applications we build with React Native. We were also able to exploit React's vibrant community of open-source libraries and reusable components during development.\\n\\n## Conclusion\\nWith almost a decade in the AI space, life has thrown a lot of awesome experience my way. I'm excited to continue building my expertise in natural language processing and machine learning. There is so much potential ahead!\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>Within the field of artificial intelligence, I specialize in natural language processing (NLP). <a href=\"https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html\">According to SAS</a>, &quot;NLP is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.&quot; Within NLP, I then further focus on conversational interface design and development.</p>\n<p>Over the years, I captured NLP experience wherever possible. I've been lucky enough to be involved on many projects throughout the sales life cycle and solutions delivery process. I've created marketing materials and sales proposals that won work in the area of NLP. I've also architected and led the development of NLP systems. In this post, I'll share some system and process designs that I've created along my career journey.</p>\n<h2 id=\"highleveltextminingprocess\">High-Level Text Mining Process</h2>\n<p><img src=\"__GHOST_URL__/content/images/2018/06/high-level-text-mining-process.jpg\" alt=\"high-level-text-mining-process\" loading=\"lazy\"></p>\n<p>I created this design to provide a simple overview of the types of data that can be processed and how that data can be processed. It was used in a sales conversation with <a href=\"https://www.apple.com/\">Apple</a>.</p>\n<h2 id=\"chatbotdevelopmentphases\">Chatbot Development Phases</h2>\n<p><img src=\"__GHOST_URL__/content/images/2018/06/chatbot-development-phases.png\" alt=\"chatbot-development-phases\" loading=\"lazy\"></p>\n<p>I created this design to describe the features stakeholders can expect a bot to have during different phases of development. The software development life cycle can be applied to chatbots. With each iteration, the system becomes more and more intelligent!</p>\n<h2 id=\"acloudbotarchitecture\">A Cloud Bot Architecture</h2>\n<p><img src=\"__GHOST_URL__/content/images/2018/06/a-cloud-bot-architecture.jpg\" alt=\"a-cloud-bot-architecture\" loading=\"lazy\"></p>\n<p>This diagram describes the architecture I used to build <a href=\"http://neona.chat/\">Neona</a>. Each of these components plays a powerful part in the complete composition.</p>\n<h3 id=\"azure\">Azure</h3>\n<p>An open, flexible, enterprise-grade cloud computing platform. Neona uses Azure Functions to process events with a serverless code architecture. These functions are responsible for extracting data from Wikipedia, processing it into the desired form, and storing it in Cosmos DB.</p>\n<h3 id=\"nodejs\">Node.js</h3>\n<p>An event-driven I/O server-side JavaScript environment. The Azure Functions were written in Node.js. It was used to call the MediaWiki action API, a web service that provides convenient access to wiki features, data, and meta-data over HTTP. Using Node.js’ request module (and JavaScript Promises), the API is called with very carefully constructed queries to return just the data Neona would need.</p>\n<h3 id=\"cosmosdb\">Cosmos DB</h3>\n<p>A distributed database service for managing JSON documents at Internet scale. It is a highly-flexible key-value store that integrates closely with other Microsoft systems. It is Neona's knowledge-base.</p>\n<h3 id=\"azuresearch\">Azure Search</h3>\n<p>A fully managed search-as-a-service in the cloud. It indexes the contents of the knowledge-base and provides an HTTP endpoint Neona can hit to query the knowledge-base. It natively supports Cosmos DB.</p>\n<h3 id=\"botframework\">Bot Framework</h3>\n<p>This framework provided an impressive amount of functionality and was the inspiration for this project. For Neona, it provides the connection to the various channels, manages incoming and outgoing messages, and helps orchestrate the dialogue flow. It also integrates naturally with LUIS, a fast and effective way of adding language understanding to applications.</p>\n<p>You can learn more about Neona in <a href=\"__GHOST_URL__/meet-neona-a-chatbot-for-learning-ai/\">my portfolio post about her</a>.</p>\n<h2 id=\"bottohumanhandoff\">Bot-to-Human Handoff</h2>\n<p>A common request from customers is to design systems where humans can take over the conversation when necessary.</p>\n<h3 id=\"systemrequirements\">System Requirements</h3>\n<p>There are several things to consider when transitioning to a human.</p>\n<h5 id=\"userscenarios\">User Scenarios</h5>\n<p>User scenarios may include triage, escalation, and supervision.</p>\n<h5 id=\"transitioninitiation\">Transition Initiation</h5>\n<p>The transition could be initiated by the user, the agent, or an automated system.</p>\n<h5 id=\"transitiondetection\">Transition Detection</h5>\n<p>The chat bot could automatically detect when a transition needs to occur based on sentiment analysis or by detecting certain entities or intents.</p>\n<h5 id=\"typeofhumaninteraction\">Type of Human Interaction</h5>\n<p>The human could engage via a support ticket, phone, text, or a chat platform.</p>\n<h3 id=\"systemarchitecture\">System Architecture</h3>\n<p>Here is a technical architecture diagram I created for a bot-to-human hand-off system built on Microsoft Azure.<br>\n<img src=\"__GHOST_URL__/content/images/2018/06/bot-to-human-handoff.png\" alt=\"bot-to-human-handoff\" loading=\"lazy\"></p>\n<h5 id=\"features\">Features</h5>\n<ol>\n<li>A dashboard for agents to view live conversations where the agent can take control if necessary.</li>\n<li>Custom middleware to connect users and agents together.</li>\n<li>A database of conversations where you can perform in-house analytics.</li>\n</ol>\n<h2 id=\"knowledgetransfermodule\">Knowledge Transfer Module</h2>\n<p>While working at <a href=\"http://decooda.com/\">Decooda</a>, I built a system, called the Knowledge Transfer Module (KTM), for automating the labeling of data. The KTM creates high-quality training data at enterprise scale. It helps train, test, and tune machine learning algorithms for everything from search relevance and sentiment analysis to conversational agents.</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/06/Knowledge-Transfer-Module-Architecture---1.0.png\" alt=\"Knowledge-Transfer-Module-Architecture---1.0\" loading=\"lazy\"></p>\n<p>By annotating data with this system, business analysts were able to contribute to our knowledge base and improve the quality of predictions made by Decooda’s machine learning classifiers.</p>\n<h3 id=\"applicationarchitecture\">Application Architecture</h3>\n<p>The client application was a native application that initially targeted Windows and Mac desktops with Android and iOS mobile support to follow. I wanted to use a technology stack and architecture that lends itself easily to rapid, iterative development. Any frameworks, libraries, and languages I chose not only needed to be reliable and easily maintainable, but also leading edge and feature rich. This required a careful balance. I wanted to use the best production-ready tools out there, but also wanted to create a technology stack that will have broad community support for years to come.</p>\n<h5 id=\"electron\">Electron</h5>\n<p>“Electron is a framework for creating native applications with web technologies like JavaScript, HTML, and CSS.”<br>\n<a href=\"https://electronjs.org/\">https://electronjs.org/</a></p>\n<p>Electron allowed us to take advantage of the massive web development ecosystem while developing a cross-platform (Mac, Windows, and Linux) desktop application. It not only allowed us to use any front-end development libraries or frameworks to create the user interface, but also allowed us to take advantage of Node.js and its package ecosystem for running more complex processes (like <a href=\"https://github.com/adaltas/node-csv\">data processing</a>) on the client machine. Plus, since Electron is essentially a container for a client-side web application, the code was written in a way that made it portable to other platforms. This allowed for an easier migration as we targeted web applications. Eventually, we'll also be able to target native mobile applications through tools like <a href=\"https://facebook.github.io/react-native/\">React Native</a>.</p>\n<h5 id=\"javascripthtmlcss\">JavaScript, HTML, CSS</h5>\n<p>The three standard powerhouse languages of the modern web were used to develop the client application. This allowed for common web tooling and design patterns to be applied to the application, improving the speed and ease of development. JavaScript was used for things like application logic, state management, and generating UI components. By also exploiting the availability of Node.js and its package ecosystem (the world’s largest ecosystem of open source libraries) through Electron, JavaScript performed even more complex computation as necessary. Of course, HTML was used to define the structure of the page and CSS was used to describe the presentation.</p>\n<h5 id=\"react\">React</h5>\n<p>“A JavaScript library for building user interfaces.”<br>\n<a href=\"https://reactjs.org/\">https://reactjs.org/</a></p>\n<p>Since Electron supports the standard web stack, React was a perfect fit. React helps developers create reusable UI components while also encouraging good software design patterns like proper encapsulation and separation of the state from the DOM. To make applications more interactive, React updates and renders the view as data changes. React positioned us well to share any UI components we created with any future web applications we build with React or with any mobile applications we build with React Native. We were also able to exploit React's vibrant community of open-source libraries and reusable components during development.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>With almost a decade in the AI space, life has thrown a lot of awesome experience my way. I'm excited to continue building my expertise in natural language processing and machine learning. There is so much potential ahead!</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "5b12d98dc338f600bf8b90a7",
            "plaintext": "Within the field of artificial intelligence, I specialize in natural language\nprocessing (NLP). According to SAS\n[https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html]\n, \"NLP is a branch of artificial intelligence that helps computers understand,\ninterpret and manipulate human language. NLP draws from many disciplines,\nincluding computer science and computational linguistics, in its pursuit to fill\nthe gap between human communication and computer understanding.\" Within NLP, I\nthen further focus on conversational interface design and development.\n\nOver the years, I captured NLP experience wherever possible. I've been lucky\nenough to be involved on many projects throughout the sales life cycle and\nsolutions delivery process. I've created marketing materials and sales proposals\nthat won work in the area of NLP. I've also architected and led the development\nof NLP systems. In this post, I'll share some system and process designs that\nI've created along my career journey.\n\nHigh-Level Text Mining Process\n\n\nI created this design to provide a simple overview of the types of data that can\nbe processed and how that data can be processed. It was used in a sales\nconversation with Apple [https://www.apple.com/].\n\nChatbot Development Phases\n\n\nI created this design to describe the features stakeholders can expect a bot to\nhave during different phases of development. The software development life cycle\ncan be applied to chatbots. With each iteration, the system becomes more and\nmore intelligent!\n\nA Cloud Bot Architecture\n\n\nThis diagram describes the architecture I used to build Neona\n[http://neona.chat/]. Each of these components plays a powerful part in the\ncomplete composition.\n\nAzure\nAn open, flexible, enterprise-grade cloud computing platform. Neona uses Azure\nFunctions to process events with a serverless code architecture. These functions\nare responsible for extracting data from Wikipedia, processing it into the\ndesired form, and storing it in Cosmos DB.\n\nNode.js\nAn event-driven I/O server-side JavaScript environment. The Azure Functions were\nwritten in Node.js. It was used to call the MediaWiki action API, a web service\nthat provides convenient access to wiki features, data, and meta-data over HTTP.\nUsing Node.js’ request module (and JavaScript Promises), the API is called with\nvery carefully constructed queries to return just the data Neona would need.\n\nCosmos DB\nA distributed database service for managing JSON documents at Internet scale. It\nis a highly-flexible key-value store that integrates closely with other\nMicrosoft systems. It is Neona's knowledge-base.\n\nAzure Search\nA fully managed search-as-a-service in the cloud. It indexes the contents of the\nknowledge-base and provides an HTTP endpoint Neona can hit to query the\nknowledge-base. It natively supports Cosmos DB.\n\nBot Framework\nThis framework provided an impressive amount of functionality and was the\ninspiration for this project. For Neona, it provides the connection to the\nvarious channels, manages incoming and outgoing messages, and helps orchestrate\nthe dialogue flow. It also integrates naturally with LUIS, a fast and effective\nway of adding language understanding to applications.\n\nYou can learn more about Neona in my portfolio post about her\n[__GHOST_URL__/meet-neona-a-chatbot-for-learning-ai/].\n\nBot-to-Human Handoff\nA common request from customers is to design systems where humans can take over\nthe conversation when necessary.\n\nSystem Requirements\nThere are several things to consider when transitioning to a human.\n\nUser Scenarios\nUser scenarios may include triage, escalation, and supervision.\n\nTransition Initiation\nThe transition could be initiated by the user, the agent, or an automated\nsystem.\n\nTransition Detection\nThe chat bot could automatically detect when a transition needs to occur based\non sentiment analysis or by detecting certain entities or intents.\n\nType of Human Interaction\nThe human could engage via a support ticket, phone, text, or a chat platform.\n\nSystem Architecture\nHere is a technical architecture diagram I created for a bot-to-human hand-off\nsystem built on Microsoft Azure.\n\n\nFeatures\n 1. A dashboard for agents to view live conversations where the agent can take\n    control if necessary.\n 2. Custom middleware to connect users and agents together.\n 3. A database of conversations where you can perform in-house analytics.\n\nKnowledge Transfer Module\nWhile working at Decooda [http://decooda.com/], I built a system, called the\nKnowledge Transfer Module (KTM), for automating the labeling of data. The KTM\ncreates high-quality training data at enterprise scale. It helps train, test,\nand tune machine learning algorithms for everything from search relevance and\nsentiment analysis to conversational agents.\n\n\n\nBy annotating data with this system, business analysts were able to contribute\nto our knowledge base and improve the quality of predictions made by Decooda’s\nmachine learning classifiers.\n\nApplication Architecture\nThe client application was a native application that initially targeted Windows\nand Mac desktops with Android and iOS mobile support to follow. I wanted to use\na technology stack and architecture that lends itself easily to rapid, iterative\ndevelopment. Any frameworks, libraries, and languages I chose not only needed to\nbe reliable and easily maintainable, but also leading edge and feature rich.\nThis required a careful balance. I wanted to use the best production-ready tools\nout there, but also wanted to create a technology stack that will have broad\ncommunity support for years to come.\n\nElectron\n“Electron is a framework for creating native applications with web technologies\nlike JavaScript, HTML, and CSS.”\nhttps://electronjs.org/\n\nElectron allowed us to take advantage of the massive web development ecosystem\nwhile developing a cross-platform (Mac, Windows, and Linux) desktop application.\nIt not only allowed us to use any front-end development libraries or frameworks\nto create the user interface, but also allowed us to take advantage of Node.js\nand its package ecosystem for running more complex processes (like data\nprocessing [https://github.com/adaltas/node-csv]) on the client machine. Plus,\nsince Electron is essentially a container for a client-side web application, the\ncode was written in a way that made it portable to other platforms. This allowed\nfor an easier migration as we targeted web applications. Eventually, we'll also\nbe able to target native mobile applications through tools like React Native\n[https://facebook.github.io/react-native/].\n\nJavaScript, HTML, CSS\nThe three standard powerhouse languages of the modern web were used to develop\nthe client application. This allowed for common web tooling and design patterns\nto be applied to the application, improving the speed and ease of development.\nJavaScript was used for things like application logic, state management, and\ngenerating UI components. By also exploiting the availability of Node.js and its\npackage ecosystem (the world’s largest ecosystem of open source libraries)\nthrough Electron, JavaScript performed even more complex computation as\nnecessary. Of course, HTML was used to define the structure of the page and CSS\nwas used to describe the presentation.\n\nReact\n“A JavaScript library for building user interfaces.”\nhttps://reactjs.org/\n\nSince Electron supports the standard web stack, React was a perfect fit. React\nhelps developers create reusable UI components while also encouraging good\nsoftware design patterns like proper encapsulation and separation of the state\nfrom the DOM. To make applications more interactive, React updates and renders\nthe view as data changes. React positioned us well to share any UI components we\ncreated with any future web applications we build with React or with any mobile\napplications we build with React Native. We were also able to exploit React's\nvibrant community of open-source libraries and reusable components during\ndevelopment.\n\nConclusion\nWith almost a decade in the AI space, life has thrown a lot of awesome\nexperience my way. I'm excited to continue building my expertise in natural\nlanguage processing and machine learning. There is so much potential ahead!",
            "feature_image": "__GHOST_URL__/content/images/2018/06/bot-to-human-handoff-1.png",
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2018-06-02T17:53:17.000Z",
            "updated_at": "2021-03-08T22:40:00.000Z",
            "published_at": "2018-06-02T21:45:27.000Z",
            "custom_excerpt": "Over the years, I captured NLP experience wherever possible. I've worked on NLP projects throughout the sales and solutions delivery process. I've architected and led the development of many NLP systems. In this post, I'll share my favorite system and process designs.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "5b7596bac573be00bf3bd062",
            "uuid": "cdac5a04-cb1b-4d30-99a7-9dcc56a2094a",
            "title": "Data Science Prototypes",
            "slug": "data-science-prototypes",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"For as long as I can remember, there are two things true about me: I'm always creating something and I'm always learning something. When I was a kid, I expressed myself through drawing. During college, it was music. In my professional life, it's building software. My art is always accompanied by schooling. Since entering into industry from undergrad, I'm consistently taking courses in [an evening grad program](https://blog.paulprae.com/my-statement-of-purpose/), participating in massive open online courses, or pursuing [certifications](https://www.certmetrics.com/amazon/public/transcript.aspx?transcript=BZK8S5SKKFV4Q5WJ).\\n\\nThere are lots of analogies between music production and software engineering. One of my favorite analogies is having the ability to 'jam' with others. It often takes years before a person's abilities get good enough to improvise creative solutions with a team in real time. It was an epic moment in my career when I reached that milestone. According to [Wikipedia](https://en.wikipedia.org/wiki/Jam_session), a jam session in music is \\\"a relatively informal musical event, process, or activity where musicians, typically instrumentalists, play improvised solos and vamp on tunes, songs and chord progressions. To 'jam' is to improvise music without extensive preparation or predefined arrangements.\\\" An equivalent to this in the software world, and more recently in the data science world, is the idea of [hackathons](https://en.wikipedia.org/wiki/Hackathon).\\n\\nIn this post, I'll share one of my favorite hackathon projects.\\n\\n# IBM Watson Hackathon\\n![The stage at the IBM Watson Hackathon](__GHOST_URL__/content/images/2018/08/ibm_watson_stage.jpg)\\n\\nBack in 2015, the data science industry started booming in all kinds of new ways. AI was making a transition to the cloud. Big tech companies, like IBM, were making huge aquisitions in the AI space. More companies than ever before were forming data science teams. My company at the time, [Red Ventures](https://www.redventures.com/), was just forming their first data science team. My goal at the time was to create innovations for Red Ventures in my favorite sub-field of AI, natural language processing. To make that happen, I wanted to show Red Ventures the value of cognitive computing by performing research at IBM's Watson Hackathon. Once Red Ventures got the potential, I knew I'd have a chance to lead the way.\\n\\n### Our goal\\nI formed a team of some of my most talented colleages. From Red Ventures, I recruited [Renato Pereyra](https://www.linkedin.com/in/rpereyra14/) and Nathan Johnson. From UGA's Master of Science in Artificial Intelligence program, I recruited my favorite computational linguist, [Thomas Bailey](https://www.linkedin.com/in/thomas-bailey-bb418b54/).\\n\\nWe aimed to augment and optimize conversations between customers and sales professionals using natural language processing and the Watson Developer Cloud. More specifically, we set out to explore chat data and dig up insights. Moving forward, we planned to turn those insights into tools and recommendations for Red Venture's sales teams.\\n\\n### How it works\\nFirst, we used what was then called the Alchemy API (now called IBM's [Natural Language Understanding](https://www.ibm.com/watson/services/natural-language-understanding/) and [Classifier](https://www.ibm.com/watson/services/natural-language-classifier/) services) to enrich thousands of sales chat logs with linguistic metrics (sentiment, key words, etc). Second, we looked for correlation between linguistic metrics and successful chat outcomes. Unlike most hackathon projects, we made sure to follow the scientific method when performing our work.\\n\\n### Challenges we ran into\\n+ Alchemy API rate limits\\n+ Formatting data for Alchemy analysis\\n+ Data visualization (some representations hide important patterns)\\n\\n### Accomplishments that we're proud of\\n+ Verified three out of three hypotheses\\n+ Discovered trends that can be acted on to increase chat agent success\\n\\n### What we learned\\nWe verified these hypotheses:\\n+ Chats with positive sentiment generate more follow-up calls from customers\\n+ Agents who talk like the customer generate more follow-up calls\\n+ Keywords from sales agent speech are good for predicting call outcomes\\n\\n## Visualizations of Watson Hackathon Results\\nWe set forth to test specific hypotheses. Through the testing of these hypotheses, we turned raw data into insights. One powerful thing about natural language processing is the ability to scale out insight across many conversations simultaneously. During this hackathon, we were able to automate linguistic insight with software to perform analysis on over 3,000 chat conversations. We verified all three hypotheses we tested (though we still need to measure things like statistical significance and error rates).\\n \\nBelow, you will find some visualizations we used to better understand the results of our experiments. The scripts and modules we wrote for our hack mostly involved data transformation and analysis. To explore and understand the output, we visualized our results with Excel, R, and Tableau.\\n \\n### Chats with positive sentiment generate more follow-up calls from customers\\n\\nWe used Alchemy API's Sentiment Analysis for this portion. \\\"Generally speaking, sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document\\\" (http://en.wikipedia.org/wiki/Sentiment_analysis). We made sure to distinguish between the agent and the customer in the conversation. Here is a visualization of our results using Tableau:\\n\\n![Chats with positive sentiment generate more follow-up calls from customers](__GHOST_URL__/content/images/2018/08/chats-with-positive-sentiment.png)\\n\\n\\n#### Insights gained\\n+ In general, you can see that the higher the sentiment rating for both the agent and the customer, the more successful the conversation. This is shown by the many chats in the top right quadrant.\\n+ We can also see a drop off in success when the agent is too positive. It seems if the agent is over zealous, the chats are less successful. The sweet spot seems to be between a 0.3 and 0.5 agent sentiment.\\n+ The agent's sentiment is consistently positive but the customer's can vary greatly. This makes sense since our agents are trying to make a sale and need to keep a positive vibe. The customer does not have to be positive.\\n+ Also notice how the customer's sentiment is commonly around zero. Zero reflects a neutral sentiment. The customer may just want to get down to business in these cases, showing no emotion.\\n\\n\\n### Agents who talk like the customer generate more follow-up calls\\nIn this case, we wanted to see what happens when the agent talks like the customer. To do this, we compared the vocabulary, or word choice, of both sides. An example of an agent matching the vocabulary of a customer would be if the agent repeated a question back to the customer for clarification. Our analysis clearly shows a benefit when an agent reflects the vocabulary of the customer.\\n \\nWe chose to compare vocabulary using cosine similarity (http://en.wikipedia.org/wiki/Cosine_similarity). Since we were doing our processing in Node.js, we were happy to find a package to do just this type of measurement: https://www.npmjs.com/package/cosine. Once the data was processed, we did a visual analysis of our results and created the following graph in Excel:\\n![Agents who talk like the customer generate more follow-up calls](__GHOST_URL__/content/images/2018/08/Agents-who-talk-like-the-customer-generate-more-follow-up-calls.png)\\n\\n#### Insights gained\\n+ We can see the success ratio increase as we move right on the graph. This shows a strong positive trend with the greatest increase in success occurring between 0.2 and 0.4 similarity.\\n+ According to this graph, the more similar both sides of the conversation are to each other the better for our metrics. This probably won't hold true at extremely high values like 1.0 where the agent would literally be echoing the customer. We need to experiment with more data on the higher side of the similarity measure to see where success drops off.\\n\\n## Conclusion\\nWe learned a lot as individuals and a company. Today, data science is still thriving at [Red Ventures](https://medium.com/rv-data). After this event, I felt like a true member of the cognitive computing community. It marked the beginning of my journey as a cloud architect of AI solutions. I've now built AI solutions across Microsoft Azure, Amazon Web Services, the Google Cloud Platform, and the IBM Watson Developer Cloud. It's empowering!\\n\\nMy team is still active in the field. Just as we explored New York City during the World of Watson conference that year, we continue to explore the world of AI today.\\n<blockquote class=\\\"instagram-media\\\" data-instgrm-captioned data-instgrm-permalink=\\\"https://www.instagram.com/p/2QsrZ9O0Jq/?utm_source=ig_embed\\\" data-instgrm-version=\\\"9\\\" style=\\\" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:540px; min-width:326px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);\\\"><div style=\\\"padding:8px;\\\"> <div style=\\\" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;\\\"> <div style=\\\" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;\\\"></div></div> <p style=\\\" margin:8px 0 0 0; padding:0 4px;\\\"> <a href=\\\"https://www.instagram.com/p/2QsrZ9O0Jq/?utm_source=ig_embed\\\" style=\\\" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;\\\" target=\\\"_blank\\\">Time to innovate!</a></p> <p style=\\\" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;\\\">A post shared by <a href=\\\"https://www.instagram.com/praeducer/?utm_source=ig_embed\\\" style=\\\" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;\\\" target=\\\"_blank\\\"> Paul Prae</a> (@praeducer) on <time style=\\\" font-family:Arial,sans-serif; font-size:14px; line-height:17px;\\\" datetime=\\\"2015-05-04T13:00:33+00:00\\\">May 4, 2015 at 6:00am PDT</time></p></div></blockquote> <script async defer src=\\\"//www.instagram.com/embed.js\\\"></script>\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>For as long as I can remember, there are two things true about me: I'm always creating something and I'm always learning something. When I was a kid, I expressed myself through drawing. During college, it was music. In my professional life, it's building software. My art is always accompanied by schooling. Since entering into industry from undergrad, I'm consistently taking courses in <a href=\"https://blog.paulprae.com/my-statement-of-purpose/\">an evening grad program</a>, participating in massive open online courses, or pursuing <a href=\"https://www.certmetrics.com/amazon/public/transcript.aspx?transcript=BZK8S5SKKFV4Q5WJ\">certifications</a>.</p>\n<p>There are lots of analogies between music production and software engineering. One of my favorite analogies is having the ability to 'jam' with others. It often takes years before a person's abilities get good enough to improvise creative solutions with a team in real time. It was an epic moment in my career when I reached that milestone. According to <a href=\"https://en.wikipedia.org/wiki/Jam_session\">Wikipedia</a>, a jam session in music is &quot;a relatively informal musical event, process, or activity where musicians, typically instrumentalists, play improvised solos and vamp on tunes, songs and chord progressions. To 'jam' is to improvise music without extensive preparation or predefined arrangements.&quot; An equivalent to this in the software world, and more recently in the data science world, is the idea of <a href=\"https://en.wikipedia.org/wiki/Hackathon\">hackathons</a>.</p>\n<p>In this post, I'll share one of my favorite hackathon projects.</p>\n<h1 id=\"ibmwatsonhackathon\">IBM Watson Hackathon</h1>\n<p><img src=\"__GHOST_URL__/content/images/2018/08/ibm_watson_stage.jpg\" alt=\"The stage at the IBM Watson Hackathon\" loading=\"lazy\"></p>\n<p>Back in 2015, the data science industry started booming in all kinds of new ways. AI was making a transition to the cloud. Big tech companies, like IBM, were making huge aquisitions in the AI space. More companies than ever before were forming data science teams. My company at the time, <a href=\"https://www.redventures.com/\">Red Ventures</a>, was just forming their first data science team. My goal at the time was to create innovations for Red Ventures in my favorite sub-field of AI, natural language processing. To make that happen, I wanted to show Red Ventures the value of cognitive computing by performing research at IBM's Watson Hackathon. Once Red Ventures got the potential, I knew I'd have a chance to lead the way.</p>\n<h3 id=\"ourgoal\">Our goal</h3>\n<p>I formed a team of some of my most talented colleages. From Red Ventures, I recruited <a href=\"https://www.linkedin.com/in/rpereyra14/\">Renato Pereyra</a> and Nathan Johnson. From UGA's Master of Science in Artificial Intelligence program, I recruited my favorite computational linguist, <a href=\"https://www.linkedin.com/in/thomas-bailey-bb418b54/\">Thomas Bailey</a>.</p>\n<p>We aimed to augment and optimize conversations between customers and sales professionals using natural language processing and the Watson Developer Cloud. More specifically, we set out to explore chat data and dig up insights. Moving forward, we planned to turn those insights into tools and recommendations for Red Venture's sales teams.</p>\n<h3 id=\"howitworks\">How it works</h3>\n<p>First, we used what was then called the Alchemy API (now called IBM's <a href=\"https://www.ibm.com/watson/services/natural-language-understanding/\">Natural Language Understanding</a> and <a href=\"https://www.ibm.com/watson/services/natural-language-classifier/\">Classifier</a> services) to enrich thousands of sales chat logs with linguistic metrics (sentiment, key words, etc). Second, we looked for correlation between linguistic metrics and successful chat outcomes. Unlike most hackathon projects, we made sure to follow the scientific method when performing our work.</p>\n<h3 id=\"challengesweraninto\">Challenges we ran into</h3>\n<ul>\n<li>Alchemy API rate limits</li>\n<li>Formatting data for Alchemy analysis</li>\n<li>Data visualization (some representations hide important patterns)</li>\n</ul>\n<h3 id=\"accomplishmentsthatwereproudof\">Accomplishments that we're proud of</h3>\n<ul>\n<li>Verified three out of three hypotheses</li>\n<li>Discovered trends that can be acted on to increase chat agent success</li>\n</ul>\n<h3 id=\"whatwelearned\">What we learned</h3>\n<p>We verified these hypotheses:</p>\n<ul>\n<li>Chats with positive sentiment generate more follow-up calls from customers</li>\n<li>Agents who talk like the customer generate more follow-up calls</li>\n<li>Keywords from sales agent speech are good for predicting call outcomes</li>\n</ul>\n<h2 id=\"visualizationsofwatsonhackathonresults\">Visualizations of Watson Hackathon Results</h2>\n<p>We set forth to test specific hypotheses. Through the testing of these hypotheses, we turned raw data into insights. One powerful thing about natural language processing is the ability to scale out insight across many conversations simultaneously. During this hackathon, we were able to automate linguistic insight with software to perform analysis on over 3,000 chat conversations. We verified all three hypotheses we tested (though we still need to measure things like statistical significance and error rates).</p>\n<p>Below, you will find some visualizations we used to better understand the results of our experiments. The scripts and modules we wrote for our hack mostly involved data transformation and analysis. To explore and understand the output, we visualized our results with Excel, R, and Tableau.</p>\n<h3 id=\"chatswithpositivesentimentgeneratemorefollowupcallsfromcustomers\">Chats with positive sentiment generate more follow-up calls from customers</h3>\n<p>We used Alchemy API's Sentiment Analysis for this portion. &quot;Generally speaking, sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document&quot; (<a href=\"http://en.wikipedia.org/wiki/Sentiment_analysis\">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). We made sure to distinguish between the agent and the customer in the conversation. Here is a visualization of our results using Tableau:</p>\n<p><img src=\"__GHOST_URL__/content/images/2018/08/chats-with-positive-sentiment.png\" alt=\"Chats with positive sentiment generate more follow-up calls from customers\" loading=\"lazy\"></p>\n<h4 id=\"insightsgained\">Insights gained</h4>\n<ul>\n<li>In general, you can see that the higher the sentiment rating for both the agent and the customer, the more successful the conversation. This is shown by the many chats in the top right quadrant.</li>\n<li>We can also see a drop off in success when the agent is too positive. It seems if the agent is over zealous, the chats are less successful. The sweet spot seems to be between a 0.3 and 0.5 agent sentiment.</li>\n<li>The agent's sentiment is consistently positive but the customer's can vary greatly. This makes sense since our agents are trying to make a sale and need to keep a positive vibe. The customer does not have to be positive.</li>\n<li>Also notice how the customer's sentiment is commonly around zero. Zero reflects a neutral sentiment. The customer may just want to get down to business in these cases, showing no emotion.</li>\n</ul>\n<h3 id=\"agentswhotalklikethecustomergeneratemorefollowupcalls\">Agents who talk like the customer generate more follow-up calls</h3>\n<p>In this case, we wanted to see what happens when the agent talks like the customer. To do this, we compared the vocabulary, or word choice, of both sides. An example of an agent matching the vocabulary of a customer would be if the agent repeated a question back to the customer for clarification. Our analysis clearly shows a benefit when an agent reflects the vocabulary of the customer.</p>\n<p>We chose to compare vocabulary using cosine similarity (<a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\">http://en.wikipedia.org/wiki/Cosine_similarity</a>). Since we were doing our processing in Node.js, we were happy to find a package to do just this type of measurement: <a href=\"https://www.npmjs.com/package/cosine\">https://www.npmjs.com/package/cosine</a>. Once the data was processed, we did a visual analysis of our results and created the following graph in Excel:<br>\n<img src=\"__GHOST_URL__/content/images/2018/08/Agents-who-talk-like-the-customer-generate-more-follow-up-calls.png\" alt=\"Agents who talk like the customer generate more follow-up calls\" loading=\"lazy\"></p>\n<h4 id=\"insightsgained\">Insights gained</h4>\n<ul>\n<li>We can see the success ratio increase as we move right on the graph. This shows a strong positive trend with the greatest increase in success occurring between 0.2 and 0.4 similarity.</li>\n<li>According to this graph, the more similar both sides of the conversation are to each other the better for our metrics. This probably won't hold true at extremely high values like 1.0 where the agent would literally be echoing the customer. We need to experiment with more data on the higher side of the similarity measure to see where success drops off.</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>We learned a lot as individuals and a company. Today, data science is still thriving at <a href=\"https://medium.com/rv-data\">Red Ventures</a>. After this event, I felt like a true member of the cognitive computing community. It marked the beginning of my journey as a cloud architect of AI solutions. I've now built AI solutions across Microsoft Azure, Amazon Web Services, the Google Cloud Platform, and the IBM Watson Developer Cloud. It's empowering!</p>\n<p>My team is still active in the field. Just as we explored New York City during the World of Watson conference that year, we continue to explore the world of AI today.</p>\n<blockquote class=\"instagram-media\" data-instgrm-captioned data-instgrm-permalink=\"https://www.instagram.com/p/2QsrZ9O0Jq/?utm_source=ig_embed\" data-instgrm-version=\"9\" style=\" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:540px; min-width:326px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);\"><div style=\"padding:8px;\"> <div style=\" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;\"> <div style=\" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;\"></div></div> <p style=\" margin:8px 0 0 0; padding:0 4px;\"> <a href=\"https://www.instagram.com/p/2QsrZ9O0Jq/?utm_source=ig_embed\" style=\" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;\" target=\"_blank\">Time to innovate!</a></p> <p style=\" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;\">A post shared by <a href=\"https://www.instagram.com/praeducer/?utm_source=ig_embed\" style=\" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;\" target=\"_blank\"> Paul Prae</a> (@praeducer) on <time style=\" font-family:Arial,sans-serif; font-size:14px; line-height:17px;\" datetime=\"2015-05-04T13:00:33+00:00\">May 4, 2015 at 6:00am PDT</time></p></div></blockquote> <script async defer src=\"//www.instagram.com/embed.js\"></script><!--kg-card-end: markdown-->",
            "comment_id": "5b7596bac573be00bf3bd062",
            "plaintext": "For as long as I can remember, there are two things true about me: I'm always\ncreating something and I'm always learning something. When I was a kid, I\nexpressed myself through drawing. During college, it was music. In my\nprofessional life, it's building software. My art is always accompanied by\nschooling. Since entering into industry from undergrad, I'm consistently taking\ncourses in an evening grad program\n[https://blog.paulprae.com/my-statement-of-purpose/], participating in massive\nopen online courses, or pursuing certifications\n[https://www.certmetrics.com/amazon/public/transcript.aspx?transcript=BZK8S5SKKFV4Q5WJ]\n.\n\nThere are lots of analogies between music production and software engineering.\nOne of my favorite analogies is having the ability to 'jam' with others. It\noften takes years before a person's abilities get good enough to improvise\ncreative solutions with a team in real time. It was an epic moment in my career\nwhen I reached that milestone. According to Wikipedia\n[https://en.wikipedia.org/wiki/Jam_session], a jam session in music is \"a\nrelatively informal musical event, process, or activity where musicians,\ntypically instrumentalists, play improvised solos and vamp on tunes, songs and\nchord progressions. To 'jam' is to improvise music without extensive preparation\nor predefined arrangements.\" An equivalent to this in the software world, and\nmore recently in the data science world, is the idea of hackathons\n[https://en.wikipedia.org/wiki/Hackathon].\n\nIn this post, I'll share one of my favorite hackathon projects.\n\nIBM Watson Hackathon\n\n\nBack in 2015, the data science industry started booming in all kinds of new\nways. AI was making a transition to the cloud. Big tech companies, like IBM,\nwere making huge aquisitions in the AI space. More companies than ever before\nwere forming data science teams. My company at the time, Red Ventures\n[https://www.redventures.com/], was just forming their first data science team.\nMy goal at the time was to create innovations for Red Ventures in my favorite\nsub-field of AI, natural language processing. To make that happen, I wanted to\nshow Red Ventures the value of cognitive computing by performing research at\nIBM's Watson Hackathon. Once Red Ventures got the potential, I knew I'd have a\nchance to lead the way.\n\nOur goal\nI formed a team of some of my most talented colleages. From Red Ventures, I\nrecruited Renato Pereyra [https://www.linkedin.com/in/rpereyra14/] and Nathan\nJohnson. From UGA's Master of Science in Artificial Intelligence program, I\nrecruited my favorite computational linguist, Thomas Bailey\n[https://www.linkedin.com/in/thomas-bailey-bb418b54/].\n\nWe aimed to augment and optimize conversations between customers and sales\nprofessionals using natural language processing and the Watson Developer Cloud.\nMore specifically, we set out to explore chat data and dig up insights. Moving\nforward, we planned to turn those insights into tools and recommendations for\nRed Venture's sales teams.\n\nHow it works\nFirst, we used what was then called the Alchemy API (now called IBM's Natural\nLanguage Understanding\n[https://www.ibm.com/watson/services/natural-language-understanding/] and \nClassifier [https://www.ibm.com/watson/services/natural-language-classifier/] \nservices) to enrich thousands of sales chat logs with linguistic metrics\n(sentiment, key words, etc). Second, we looked for correlation between\nlinguistic metrics and successful chat outcomes. Unlike most hackathon projects,\nwe made sure to follow the scientific method when performing our work.\n\nChallenges we ran into\n * Alchemy API rate limits\n * Formatting data for Alchemy analysis\n * Data visualization (some representations hide important patterns)\n\nAccomplishments that we're proud of\n * Verified three out of three hypotheses\n * Discovered trends that can be acted on to increase chat agent success\n\nWhat we learned\nWe verified these hypotheses:\n\n * Chats with positive sentiment generate more follow-up calls from customers\n * Agents who talk like the customer generate more follow-up calls\n * Keywords from sales agent speech are good for predicting call outcomes\n\nVisualizations of Watson Hackathon Results\nWe set forth to test specific hypotheses. Through the testing of these\nhypotheses, we turned raw data into insights. One powerful thing about natural\nlanguage processing is the ability to scale out insight across many\nconversations simultaneously. During this hackathon, we were able to automate\nlinguistic insight with software to perform analysis on over 3,000 chat\nconversations. We verified all three hypotheses we tested (though we still need\nto measure things like statistical significance and error rates).\n\nBelow, you will find some visualizations we used to better understand the\nresults of our experiments. The scripts and modules we wrote for our hack mostly\ninvolved data transformation and analysis. To explore and understand the output,\nwe visualized our results with Excel, R, and Tableau.\n\nChats with positive sentiment generate more follow-up calls from customers\nWe used Alchemy API's Sentiment Analysis for this portion. \"Generally speaking,\nsentiment analysis aims to determine the attitude of a speaker or a writer with\nrespect to some topic or the overall contextual polarity of a document\" (\nhttp://en.wikipedia.org/wiki/Sentiment_analysis). We made sure to distinguish\nbetween the agent and the customer in the conversation. Here is a visualization\nof our results using Tableau:\n\n\n\nInsights gained\n * In general, you can see that the higher the sentiment rating for both the\n   agent and the customer, the more successful the conversation. This is shown\n   by the many chats in the top right quadrant.\n * We can also see a drop off in success when the agent is too positive. It\n   seems if the agent is over zealous, the chats are less successful. The sweet\n   spot seems to be between a 0.3 and 0.5 agent sentiment.\n * The agent's sentiment is consistently positive but the customer's can vary\n   greatly. This makes sense since our agents are trying to make a sale and need\n   to keep a positive vibe. The customer does not have to be positive.\n * Also notice how the customer's sentiment is commonly around zero. Zero\n   reflects a neutral sentiment. The customer may just want to get down to\n   business in these cases, showing no emotion.\n\nAgents who talk like the customer generate more follow-up calls\nIn this case, we wanted to see what happens when the agent talks like the\ncustomer. To do this, we compared the vocabulary, or word choice, of both sides.\nAn example of an agent matching the vocabulary of a customer would be if the\nagent repeated a question back to the customer for clarification. Our analysis\nclearly shows a benefit when an agent reflects the vocabulary of the customer.\n\nWe chose to compare vocabulary using cosine similarity (\nhttp://en.wikipedia.org/wiki/Cosine_similarity). Since we were doing our\nprocessing in Node.js, we were happy to find a package to do just this type of\nmeasurement: https://www.npmjs.com/package/cosine. Once the data was processed,\nwe did a visual analysis of our results and created the following graph in\nExcel:\n\n\nInsights gained\n * We can see the success ratio increase as we move right on the graph. This\n   shows a strong positive trend with the greatest increase in success occurring\n   between 0.2 and 0.4 similarity.\n * According to this graph, the more similar both sides of the conversation are\n   to each other the better for our metrics. This probably won't hold true at\n   extremely high values like 1.0 where the agent would literally be echoing the\n   customer. We need to experiment with more data on the higher side of the\n   similarity measure to see where success drops off.\n\nConclusion\nWe learned a lot as individuals and a company. Today, data science is still\nthriving at Red Ventures [https://medium.com/rv-data]. After this event, I felt\nlike a true member of the cognitive computing community. It marked the beginning\nof my journey as a cloud architect of AI solutions. I've now built AI solutions\nacross Microsoft Azure, Amazon Web Services, the Google Cloud Platform, and the\nIBM Watson Developer Cloud. It's empowering!\n\nMy team is still active in the field. Just as we explored New York City during\nthe World of Watson conference that year, we continue to explore the world of AI\ntoday.\n\n>   Time to innovate! [https://www.instagram.com/p/2QsrZ9O0Jq/?utm_source=ig_embed]\n\nA post shared by Paul Prae\n[https://www.instagram.com/praeducer/?utm_source=ig_embed] (@praeducer) on May\n4, 2015 at 6:00am PDT",
            "feature_image": "__GHOST_URL__/content/images/2018/08/chats-with-positive-sentiment-2.png",
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2018-08-16T15:22:34.000Z",
            "updated_at": "2021-03-08T22:37:53.000Z",
            "published_at": "2018-08-16T18:33:46.000Z",
            "custom_excerpt": "Back when AI was making a transition to the cloud, I wanted to show the value of cognitive computing. In this post, I'll share some of my favorite hackathon and capstone projects.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e7e",
            "uuid": "ccf6ed50-4cef-4b21-a390-5e7beae41392",
            "title": "Advancing the Human Intellect",
            "slug": "human-intellect",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I help society keep up with its own exponential self complication. The future landscape is only going to be increasingly more difficult to traverse for any single human mind. My solutions walk beside you to supplement the perception of your environment. The intelligent machines I integrate into my solutions watch and understand the world with you. I architect processes and connections that recursively consume and generate actionable information faster than you need it. It is difficult for any organization to solve the world’s increasingly tangled problems within a competitive timeline. For any single entity to succeed in a way that maximizes its usefulness, it needs to simultaneously exploit all relevant and available data, people, and technology.\\n\\nI identify the best resources to solve a problem and then architect them into a cohesive solution. These resources include:\\n\\n* The abilities of people and machines.\\n* The data generated by people and machines.\\n* The relationships between data, people, and machines.\\n\\nThe resulting solution is a system of processes that integrate all of the above. I build connected systems that integrate the minds of many people with the power of many machines. These systems are meant to solve complex problems involving massive amounts of data. My systems are socially and emotionally aware web applications that perpetually process and exchange information using nature-based computation. Every system I build extends the minds of its users into an infinite feedback loop with the intelligent machines in the system. This results in an intelligently augmented social interaction system. Information is constantly pushed into and pulled from the available pool of knowledge. Over time, the pool of knowledge grows, refines itself, and is improved. This pool of knowledge is synchronized with the minds of its users, further enhancing their cognitive abilities.\\n\\nThese systems benefit the users by:\\n\\n* Lubricating communication and collaboration between people.\\n* Making the ideal solutions to various problems obvious.\\n* Augmenting decision making.\\n\\nConsider these systems an extension of the minds of its users. Human collective consciousness is great at identifying problems and creating solutions. I augment collective consciousness to find better solutions faster. Domain experts help me recognize rich problem domains with hidden solutions. These experts provide the details of the domain so I can create innovative systems in a competitive manner. I partner with domain experts to build interactive human machine systems to solve specialized problems of the highest complexity.\\n\\nMy solutions are communities where great minds augment themselves with machine intelligence to turn massive amounts of data and intricate relationships into immediately useful information. I teach machines to teach people how to improve the lives of society as they desire. I let the machines learn from the people and the people learn from the machines. With my approach, machines will become more interactive, reactive, proactive, and communicative allowing them to better augment people in all aspects of life.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>I help society keep up with its own exponential self complication. The future landscape is only going to be increasingly more difficult to traverse for any single human mind. My solutions walk beside you to supplement the perception of your environment. The intelligent machines I integrate into my solutions watch and understand the world with you. I architect processes and connections that recursively consume and generate actionable information faster than you need it. It is difficult for any organization to solve the world’s increasingly tangled problems within a competitive timeline. For any single entity to succeed in a way that maximizes its usefulness, it needs to simultaneously exploit all relevant and available data, people, and technology.</p>\n<p>I identify the best resources to solve a problem and then architect them into a cohesive solution. These resources include:</p>\n<ul>\n<li>The abilities of people and machines.</li>\n<li>The data generated by people and machines.</li>\n<li>The relationships between data, people, and machines.</li>\n</ul>\n<p>The resulting solution is a system of processes that integrate all of the above. I build connected systems that integrate the minds of many people with the power of many machines. These systems are meant to solve complex problems involving massive amounts of data. My systems are socially and emotionally aware web applications that perpetually process and exchange information using nature-based computation. Every system I build extends the minds of its users into an infinite feedback loop with the intelligent machines in the system. This results in an intelligently augmented social interaction system. Information is constantly pushed into and pulled from the available pool of knowledge. Over time, the pool of knowledge grows, refines itself, and is improved. This pool of knowledge is synchronized with the minds of its users, further enhancing their cognitive abilities.</p>\n<p>These systems benefit the users by:</p>\n<ul>\n<li>Lubricating communication and collaboration between people.</li>\n<li>Making the ideal solutions to various problems obvious.</li>\n<li>Augmenting decision making.</li>\n</ul>\n<p>Consider these systems an extension of the minds of its users. Human collective consciousness is great at identifying problems and creating solutions. I augment collective consciousness to find better solutions faster. Domain experts help me recognize rich problem domains with hidden solutions. These experts provide the details of the domain so I can create innovative systems in a competitive manner. I partner with domain experts to build interactive human machine systems to solve specialized problems of the highest complexity.</p>\n<p>My solutions are communities where great minds augment themselves with machine intelligence to turn massive amounts of data and intricate relationships into immediately useful information. I teach machines to teach people how to improve the lives of society as they desire. I let the machines learn from the people and the people learn from the machines. With my approach, machines will become more interactive, reactive, proactive, and communicative allowing them to better augment people in all aspects of life.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "1",
            "plaintext": "I help society keep up with its own exponential self complication. The future\nlandscape is only going to be increasingly more difficult to traverse for any\nsingle human mind. My solutions walk beside you to supplement the perception of\nyour environment. The intelligent machines I integrate into my solutions watch\nand understand the world with you. I architect processes and connections that\nrecursively consume and generate actionable information faster than you need it.\nIt is difficult for any organization to solve the world’s increasingly tangled\nproblems within a competitive timeline. For any single entity to succeed in a\nway that maximizes its usefulness, it needs to simultaneously exploit all\nrelevant and available data, people, and technology.\n\nI identify the best resources to solve a problem and then architect them into a\ncohesive solution. These resources include:\n\n * The abilities of people and machines.\n * The data generated by people and machines.\n * The relationships between data, people, and machines.\n\nThe resulting solution is a system of processes that integrate all of the above.\nI build connected systems that integrate the minds of many people with the power\nof many machines. These systems are meant to solve complex problems involving\nmassive amounts of data. My systems are socially and emotionally aware web\napplications that perpetually process and exchange information using\nnature-based computation. Every system I build extends the minds of its users\ninto an infinite feedback loop with the intelligent machines in the system. This\nresults in an intelligently augmented social interaction system. Information is\nconstantly pushed into and pulled from the available pool of knowledge. Over\ntime, the pool of knowledge grows, refines itself, and is improved. This pool of\nknowledge is synchronized with the minds of its users, further enhancing their\ncognitive abilities.\n\nThese systems benefit the users by:\n\n * Lubricating communication and collaboration between people.\n * Making the ideal solutions to various problems obvious.\n * Augmenting decision making.\n\nConsider these systems an extension of the minds of its users. Human collective\nconsciousness is great at identifying problems and creating solutions. I augment\ncollective consciousness to find better solutions faster. Domain experts help me\nrecognize rich problem domains with hidden solutions. These experts provide the\ndetails of the domain so I can create innovative systems in a competitive\nmanner. I partner with domain experts to build interactive human machine systems\nto solve specialized problems of the highest complexity.\n\nMy solutions are communities where great minds augment themselves with machine\nintelligence to turn massive amounts of data and intricate relationships into\nimmediately useful information. I teach machines to teach people how to improve\nthe lives of society as they desire. I let the machines learn from the people\nand the people learn from the machines. With my approach, machines will become\nmore interactive, reactive, proactive, and communicative allowing them to better\naugment people in all aspects of life.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2013-12-23T12:57:46.000Z",
            "updated_at": "2021-03-09T16:52:00.000Z",
            "published_at": "2013-12-23T12:57:46.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e7f",
            "uuid": "669000e9-330c-4995-aa70-3490cca9e0df",
            "title": "Team Building",
            "slug": "team-building",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Team building should not be an assignment by management. It should be an introduction to potential teammates. The team members should take it from there.\\n\\nI have experienced it so many times where teams are basically randomly generated based on skill sets. Skills are not as important as personalities. Almost all friction on a team I observe is political, not ability related (If I remember right, the number one reason people quit is because of management. Your leader is the most important team mate that you need to vibe with). Building a new skill is much easier than sacrificing a behavior change to kluge yourself into a temporary local system.\\n\\nI really wish self-managed and self-organizing systems were more widely implemented. We need to eliminate sociopathic oligarchies.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>Team building should not be an assignment by management. It should be an introduction to potential teammates. The team members should take it from there.</p>\n<p>I have experienced it so many times where teams are basically randomly generated based on skill sets. Skills are not as important as personalities. Almost all friction on a team I observe is political, not ability related (If I remember right, the number one reason people quit is because of management. Your leader is the most important team mate that you need to vibe with). Building a new skill is much easier than sacrificing a behavior change to kluge yourself into a temporary local system.</p>\n<p>I really wish self-managed and self-organizing systems were more widely implemented. We need to eliminate sociopathic oligarchies.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "2",
            "plaintext": "Team building should not be an assignment by management. It should be an\nintroduction to potential teammates. The team members should take it from there.\n\nI have experienced it so many times where teams are basically randomly generated\nbased on skill sets. Skills are not as important as personalities. Almost all\nfriction on a team I observe is political, not ability related (If I remember\nright, the number one reason people quit is because of management. Your leader\nis the most important team mate that you need to vibe with). Building a new\nskill is much easier than sacrificing a behavior change to kluge yourself into a\ntemporary local system.\n\nI really wish self-managed and self-organizing systems were more widely\nimplemented. We need to eliminate sociopathic oligarchies.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2013-12-23T13:46:06.000Z",
            "updated_at": "2021-03-09T16:51:41.000Z",
            "published_at": "2013-12-23T13:46:30.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e80",
            "uuid": "114ed6d3-fb19-4f69-ade3-69d5a350d9ff",
            "title": "Rules and Laws are Secondary",
            "slug": "rules-and-laws-are-secondary-to-their-creators",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Rules and laws are only secondary to their creators: intentions and goals. This concept is often lost in blind obedience. People become so attached to the rule of law that they forget the real reasons behind why a particular rule is in place. Rules should not be followed in every single situation. You must understand a rule and law before you follow it, not after. They should be treated as a heuristic, not a command.\\n\\nI have to know why a rule is in place before I can follow it. I notice many people do not care to do this, maybe out of apathy or faith. You can make this situation complicated, especially in the case of where an individual does not understand the system they are in enough to be an effective decision maker. The only issue I can see beyond that, is the issue of time (life is short ya know?). It is quicker to assume a rule is right so you can move beyond it. I am so confident enough in my abilities to quickly understand the intentions of a rule that I completely lack faith in any rule presented to me. A rule is only a conclusion. In many cases, rules lack a description of their premise(s). Every action or inaction you take must be analyzed to its root. Rules should not be hard limits until they are broken apart and fully comprehended; only then should they act as a shortcut.\\n\\nHere is a simple example: \\\"Never lie\\\"\\nA rule that is often taught to children is to 'never lie'. This sounds like it makes sense and is often used to simplify the child's ethical system. I would like to argue that these types of rules, over time, complicate the system unnecessarily. It causes the rule creator to create many sub-cases where it is OK to break a rule. In this example, I immediately think of a few specific use cases where this rule has to be amended. 1. Humor, such as sarcasm or exaggeration. 2. Telling a fictional story. 3. Protection from evil (such as telling a genocidal militant that no one of their targeted race is in the house despite them being hidden in the attic). This list could go on and on.\\n\\nThe Problem: This concept of a nearly infinite amount of specific rules and laws is the primary cause of things like governmental bureaucratic bloat (e.g. America's tax laws) and the confusion and anger that erupts from the existence of so many different belief systems despite their almost exact intentions (think about the actual differences between Christianity's many denominations. Does any difference exist except exact rules derived from specific use cases (historical 'facts' and methods of worship aside.)?).\\n\\nConclusion: Our guiding social systems need to design their form of control around goals and intentions and not by simply blacklisting or whitelisting an infinite amount of specific (in)actions.\\n\\nSolution: Sounds like a good education is all that is needed. How about we extend the school hours and add in a study of ethics, epistemology, and decision making?\\n\\nWhat do you think?\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>Rules and laws are only secondary to their creators: intentions and goals. This concept is often lost in blind obedience. People become so attached to the rule of law that they forget the real reasons behind why a particular rule is in place. Rules should not be followed in every single situation. You must understand a rule and law before you follow it, not after. They should be treated as a heuristic, not a command.</p>\n<p>I have to know why a rule is in place before I can follow it. I notice many people do not care to do this, maybe out of apathy or faith. You can make this situation complicated, especially in the case of where an individual does not understand the system they are in enough to be an effective decision maker. The only issue I can see beyond that, is the issue of time (life is short ya know?). It is quicker to assume a rule is right so you can move beyond it. I am so confident enough in my abilities to quickly understand the intentions of a rule that I completely lack faith in any rule presented to me. A rule is only a conclusion. In many cases, rules lack a description of their premise(s). Every action or inaction you take must be analyzed to its root. Rules should not be hard limits until they are broken apart and fully comprehended; only then should they act as a shortcut.</p>\n<p>Here is a simple example: &quot;Never lie&quot;<br>\nA rule that is often taught to children is to 'never lie'. This sounds like it makes sense and is often used to simplify the child's ethical system. I would like to argue that these types of rules, over time, complicate the system unnecessarily. It causes the rule creator to create many sub-cases where it is OK to break a rule. In this example, I immediately think of a few specific use cases where this rule has to be amended. 1. Humor, such as sarcasm or exaggeration. 2. Telling a fictional story. 3. Protection from evil (such as telling a genocidal militant that no one of their targeted race is in the house despite them being hidden in the attic). This list could go on and on.</p>\n<p>The Problem: This concept of a nearly infinite amount of specific rules and laws is the primary cause of things like governmental bureaucratic bloat (e.g. America's tax laws) and the confusion and anger that erupts from the existence of so many different belief systems despite their almost exact intentions (think about the actual differences between Christianity's many denominations. Does any difference exist except exact rules derived from specific use cases (historical 'facts' and methods of worship aside.)?).</p>\n<p>Conclusion: Our guiding social systems need to design their form of control around goals and intentions and not by simply blacklisting or whitelisting an infinite amount of specific (in)actions.</p>\n<p>Solution: Sounds like a good education is all that is needed. How about we extend the school hours and add in a study of ethics, epistemology, and decision making?</p>\n<p>What do you think?</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "3",
            "plaintext": "Rules and laws are only secondary to their creators: intentions and goals. This\nconcept is often lost in blind obedience. People become so attached to the rule\nof law that they forget the real reasons behind why a particular rule is in\nplace. Rules should not be followed in every single situation. You must\nunderstand a rule and law before you follow it, not after. They should be\ntreated as a heuristic, not a command.\n\nI have to know why a rule is in place before I can follow it. I notice many\npeople do not care to do this, maybe out of apathy or faith. You can make this\nsituation complicated, especially in the case of where an individual does not\nunderstand the system they are in enough to be an effective decision maker. The\nonly issue I can see beyond that, is the issue of time (life is short ya know?).\nIt is quicker to assume a rule is right so you can move beyond it. I am so\nconfident enough in my abilities to quickly understand the intentions of a rule\nthat I completely lack faith in any rule presented to me. A rule is only a\nconclusion. In many cases, rules lack a description of their premise(s). Every\naction or inaction you take must be analyzed to its root. Rules should not be\nhard limits until they are broken apart and fully comprehended; only then should\nthey act as a shortcut.\n\nHere is a simple example: \"Never lie\"\nA rule that is often taught to children is to 'never lie'. This sounds like it\nmakes sense and is often used to simplify the child's ethical system. I would\nlike to argue that these types of rules, over time, complicate the system\nunnecessarily. It causes the rule creator to create many sub-cases where it is\nOK to break a rule. In this example, I immediately think of a few specific use\ncases where this rule has to be amended. 1. Humor, such as sarcasm or\nexaggeration. 2. Telling a fictional story. 3. Protection from evil (such as\ntelling a genocidal militant that no one of their targeted race is in the house\ndespite them being hidden in the attic). This list could go on and on.\n\nThe Problem: This concept of a nearly infinite amount of specific rules and laws\nis the primary cause of things like governmental bureaucratic bloat (e.g.\nAmerica's tax laws) and the confusion and anger that erupts from the existence\nof so many different belief systems despite their almost exact intentions (think\nabout the actual differences between Christianity's many denominations. Does any\ndifference exist except exact rules derived from specific use cases (historical\n'facts' and methods of worship aside.)?).\n\nConclusion: Our guiding social systems need to design their form of control\naround goals and intentions and not by simply blacklisting or whitelisting an\ninfinite amount of specific (in)actions.\n\nSolution: Sounds like a good education is all that is needed. How about we\nextend the school hours and add in a study of ethics, epistemology, and decision\nmaking?\n\nWhat do you think?",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2013-12-23T13:47:57.000Z",
            "updated_at": "2021-03-09T16:51:22.000Z",
            "published_at": "2013-12-23T13:48:00.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e81",
            "uuid": "147844cb-0c98-46af-ab06-5306decc0f80",
            "title": "Who You Are",
            "slug": "who-you-are",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"What you spend your time doing is who you are. It is everything that matters about you. Deciding how to spend your time is quite the challenge.\\n\\nA good requirement for the purpose of your actions would be to ensure that what you spend your time doing improves your life or that of any part of the cosmos at all (deciding whether something is an improvement is a whole other discussion). Another good requirement to decide if you should dedicate much of your valuable time towards something is to ask yourself if you will be one of the best at such a thing. I can think of two basic heuristics I use to decide if some habit is worth my effort: if many people agree that I am excellent at whatever I am doing and if I can spend many hours in a row for days in a row doing this certain thing. It is also always a good sign if you become above average at something within a short amount of time. One problem with these requirements is if you are awesome at many things. This may cause confusion. Really though, only one set of actions will be the absolute best in regards to overall benefit (probably).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>What you spend your time doing is who you are. It is everything that matters about you. Deciding how to spend your time is quite the challenge.</p>\n<p>A good requirement for the purpose of your actions would be to ensure that what you spend your time doing improves your life or that of any part of the cosmos at all (deciding whether something is an improvement is a whole other discussion). Another good requirement to decide if you should dedicate much of your valuable time towards something is to ask yourself if you will be one of the best at such a thing. I can think of two basic heuristics I use to decide if some habit is worth my effort: if many people agree that I am excellent at whatever I am doing and if I can spend many hours in a row for days in a row doing this certain thing. It is also always a good sign if you become above average at something within a short amount of time. One problem with these requirements is if you are awesome at many things. This may cause confusion. Really though, only one set of actions will be the absolute best in regards to overall benefit (probably).</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "4",
            "plaintext": "What you spend your time doing is who you are. It is everything that matters\nabout you. Deciding how to spend your time is quite the challenge.\n\nA good requirement for the purpose of your actions would be to ensure that what\nyou spend your time doing improves your life or that of any part of the cosmos\nat all (deciding whether something is an improvement is a whole other\ndiscussion). Another good requirement to decide if you should dedicate much of\nyour valuable time towards something is to ask yourself if you will be one of\nthe best at such a thing. I can think of two basic heuristics I use to decide if\nsome habit is worth my effort: if many people agree that I am excellent at\nwhatever I am doing and if I can spend many hours in a row for days in a row\ndoing this certain thing. It is also always a good sign if you become above\naverage at something within a short amount of time. One problem with these\nrequirements is if you are awesome at many things. This may cause confusion.\nReally though, only one set of actions will be the absolute best in regards to\noverall benefit (probably).",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2013-12-23T13:48:34.000Z",
            "updated_at": "2021-03-09T16:51:05.000Z",
            "published_at": "2013-12-23T13:48:39.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e82",
            "uuid": "7822b654-f36d-464e-ab89-0ceccf010f67",
            "title": "Innovation and Knowledge",
            "slug": "innovation-and-knowledge",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Innovation is the concept that proves how human beings can perpetually travel farther and farther down the path of reasoning. Every new innovation is built upon another and consistently increases our intellectual power. We are truly in exponential times. There is so much information being discovered and stored that it is only leading to even more discoveries and information that can again be stored for the future. This recursive effect is what is causing so many new technologies to be brought to our commercial forefront. There are an immense amount of innovations and entrepreneurship happening within the realm of new media. Imagine the amount within all the other fields around the world! This overwhelming breach into this proliferation of new ideas is constantly affecting every day we live. Cultures around the world are changing and will only see more change. It is such a beautiful thing. New media is an extremely powerful tool for commercial purposes or for altering culture in many different ways.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>Innovation is the concept that proves how human beings can perpetually travel farther and farther down the path of reasoning. Every new innovation is built upon another and consistently increases our intellectual power. We are truly in exponential times. There is so much information being discovered and stored that it is only leading to even more discoveries and information that can again be stored for the future. This recursive effect is what is causing so many new technologies to be brought to our commercial forefront. There are an immense amount of innovations and entrepreneurship happening within the realm of new media. Imagine the amount within all the other fields around the world! This overwhelming breach into this proliferation of new ideas is constantly affecting every day we live. Cultures around the world are changing and will only see more change. It is such a beautiful thing. New media is an extremely powerful tool for commercial purposes or for altering culture in many different ways.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "5",
            "plaintext": "Innovation is the concept that proves how human beings can perpetually travel\nfarther and farther down the path of reasoning. Every new innovation is built\nupon another and consistently increases our intellectual power. We are truly in\nexponential times. There is so much information being discovered and stored that\nit is only leading to even more discoveries and information that can again be\nstored for the future. This recursive effect is what is causing so many new\ntechnologies to be brought to our commercial forefront. There are an immense\namount of innovations and entrepreneurship happening within the realm of new\nmedia. Imagine the amount within all the other fields around the world! This\noverwhelming breach into this proliferation of new ideas is constantly affecting\nevery day we live. Cultures around the world are changing and will only see more\nchange. It is such a beautiful thing. New media is an extremely powerful tool\nfor commercial purposes or for altering culture in many different ways.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2013-12-23T13:50:28.000Z",
            "updated_at": "2021-03-09T16:50:43.000Z",
            "published_at": "2013-12-23T13:50:43.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e83",
            "uuid": "3b27e688-b61c-4ec9-ac4b-22997b3c2489",
            "title": "The Three Most Important Qualities of a Developer",
            "slug": "the-three-most-important-qualities-of-a-developer",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"tl;dr\\n1. Displaying clear, concise, and constant communication.\\n2. Properly setting priorities with good time management.\\n3. Effective exploitation of available tools and people.\\nBonus: Proper emotional management of yourself and stakeholders.\\n\\nDescribing the most important qualities of a developer is difficult to do objectively without specific limitations or context. Many variables can influence the answer to this question. These variables include:\\n* The project the developer is working on.\\n* The role or roles the developer has on the team.\\n* The team the developer is working with.\\n* The stakeholders the developer is working for.\\n* The environment the developer is working in.\\n\\nTo come up with a ranked set of universally applicable set of qualities, all of these factors must be considered. Since we do not have any limitations in place, these traits will end up being applicable to many job roles involving the same variables.\\n\\nThe first trait I will discuss is significant for any activity that involves the cooperation of two or more people (and is also applicable to systems involving computer agents or machines). It is the quality of clear, concise, and constant communication. For any project to go well, a developer needs to fully understand what the goals of the project are and how they will contribute to these goals. These goals should be determined by the visionaries, designers, architects, and, ultimately, the end user or customer. Communication needs to be clear and accurate so the developer is able to extract the appropriate vision from the minds of the stakeholders. Communication needs to be concise because time is often limited. No need to be verbose unless necessary, verbosity can often lead to confusion. Finally, communication needs to be constant because, after all, the world is approaching the technological singularity and is getting exponentially complicated at an increasingly faster pace. Demands and requirements are changing constantly.\\n\\nThere are many project management models that focus on encouraging good behavior in communication practices. Through a solid application of the agile software development model, the developer would always have an updated view of what needs to be done for all relevant stakeholders. This, in combination with a return of excellent communication to the rest of the team, will inevitably be core to a developer’s success. I have seen the importance of this over and over again within every project I have worked on. Agile influenced models are meant to address the failures and delays in communication that were often present in past workflows.\\n\\nThe second trait that has proven to be particularly important for my own growth is that of time management and properly setting priorities. Whether we like it or not, our business world is on a schedule. The only truly finite resource we have is time. Not only is life short, but often project timelines are too. It is important for any single developer to make sure time sensitive issues or functionalities are addressed first. Every activity that consumes a developer’s time must be monitored and constantly reassessed for its own individual ranking on the developer’s todo list. This will require the developer to use the first quality I mentioned to ensure they are on the right track. It is not uncommon to receive conflicting task items from multiple stakeholders where priorities are not easily comparable. In these moments, the developer may need to jump into a project management role for a moment and make sure everyone else is communicating the team’s goals properly within the group.\\n\\nPrioritizing is not the only part of good time management. You must also make sure to put a proportionate amount of time and emotional effort into addressing each task. A huge part that plays into this is the avoidance of absolute perfection or “analysis paralysis” as many put it. There may be times where perfection is necessary, such as in the implementation of a secure data transfer system, and other times where functionality must take definite precedence, such as in creating log entries. Sometimes “good enough” hurts to admit; we just need to remember to focus on the big picture and accept this as the best choice. Keep moving forward! It does not help if one portion of the project is perfect when several dependencies to that piece are not even functional yet. As the Director of my Artificial Intelligence Institute at UGA always said: functional first, optimize later!\\n\\nThe third quality that is essential for any highly productive developer is the effective use of available tools and people. As developers, we are tool builders. We cannot forget this! We create things to make our lives easier. The world is full of all kinds of fantastic technology to augment our knowledge and abilities. Our tools are intrinsically connected to our minds at this point. Our minds extend into these tools and increase our cognitive potential. Use them wisely! At least use them for goodness sake, no need to brute force something if it can be automated or augmented. Sometimes I see libraries I could have built better but it is often not worth the time to rebuild something if you have a more significant project to accomplish.\\n\\n[As an aside, my favorite tools are the programming languages I work with. My worst and best habit as a developer is that I love to study how a language works. At this point I can learn a language in a few days and then move on to another language (and usually a new technology stack). After enough repetition of this, I found it best to master a couple well-documented languages from popular paradigms, like ‘object-oriented’ or ‘functional’, and use them to build a few awesome projects. It will be much more enjoyable and you will become much more employable since most recruiters hire towards the language and not the paradigm. Learn towards executing on significant projects! Many widely-varied, little accomplishments tend not to get noticed.]\\n\\nPeople are the most important tools we can use. Working with a team extends our minds into the collective consciousness of the world. It allows their knowledge and abilities to become our own. This is especially useful when you and the individuals you are working with have the same mission to accomplish but different skills or experience. The greatest increase in the quality and speed of my work at Microsoft came from exploiting the incredible expertise I was surrounded by. It took a leap of humbleness and resulted in productive happiness. With the effective use of tools and people, a developer can do anything better.\\n\\nThese three traits are inexorably intertwined. A developer must use all available resources, most importantly time, tools, and people, to attack the highest priority tasks. These tasks are cyclically defined by users, designers, project managers, and many other stakeholders. The definition of the final product realized by the developer must be updated regularly through masterful communication. With their mind set to improve these three traits, any developer would be keeping with a focused vision for success.\\n\\nBONUS QUALITY\\nAnother quality I found to be important and underrepresented is that of emotional management. This is important in relation to yourself and to others. It is essential for making sound decisions and building solid relationships. Always keep the end goal in mind with every action! I will save more comments on this quality for another post.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>tl;dr</p>\n<ol>\n<li>Displaying clear, concise, and constant communication.</li>\n<li>Properly setting priorities with good time management.</li>\n<li>Effective exploitation of available tools and people.<br>\nBonus: Proper emotional management of yourself and stakeholders.</li>\n</ol>\n<p>Describing the most important qualities of a developer is difficult to do objectively without specific limitations or context. Many variables can influence the answer to this question. These variables include:</p>\n<ul>\n<li>The project the developer is working on.</li>\n<li>The role or roles the developer has on the team.</li>\n<li>The team the developer is working with.</li>\n<li>The stakeholders the developer is working for.</li>\n<li>The environment the developer is working in.</li>\n</ul>\n<p>To come up with a ranked set of universally applicable set of qualities, all of these factors must be considered. Since we do not have any limitations in place, these traits will end up being applicable to many job roles involving the same variables.</p>\n<p>The first trait I will discuss is significant for any activity that involves the cooperation of two or more people (and is also applicable to systems involving computer agents or machines). It is the quality of clear, concise, and constant communication. For any project to go well, a developer needs to fully understand what the goals of the project are and how they will contribute to these goals. These goals should be determined by the visionaries, designers, architects, and, ultimately, the end user or customer. Communication needs to be clear and accurate so the developer is able to extract the appropriate vision from the minds of the stakeholders. Communication needs to be concise because time is often limited. No need to be verbose unless necessary, verbosity can often lead to confusion. Finally, communication needs to be constant because, after all, the world is approaching the technological singularity and is getting exponentially complicated at an increasingly faster pace. Demands and requirements are changing constantly.</p>\n<p>There are many project management models that focus on encouraging good behavior in communication practices. Through a solid application of the agile software development model, the developer would always have an updated view of what needs to be done for all relevant stakeholders. This, in combination with a return of excellent communication to the rest of the team, will inevitably be core to a developer’s success. I have seen the importance of this over and over again within every project I have worked on. Agile influenced models are meant to address the failures and delays in communication that were often present in past workflows.</p>\n<p>The second trait that has proven to be particularly important for my own growth is that of time management and properly setting priorities. Whether we like it or not, our business world is on a schedule. The only truly finite resource we have is time. Not only is life short, but often project timelines are too. It is important for any single developer to make sure time sensitive issues or functionalities are addressed first. Every activity that consumes a developer’s time must be monitored and constantly reassessed for its own individual ranking on the developer’s todo list. This will require the developer to use the first quality I mentioned to ensure they are on the right track. It is not uncommon to receive conflicting task items from multiple stakeholders where priorities are not easily comparable. In these moments, the developer may need to jump into a project management role for a moment and make sure everyone else is communicating the team’s goals properly within the group.</p>\n<p>Prioritizing is not the only part of good time management. You must also make sure to put a proportionate amount of time and emotional effort into addressing each task. A huge part that plays into this is the avoidance of absolute perfection or “analysis paralysis” as many put it. There may be times where perfection is necessary, such as in the implementation of a secure data transfer system, and other times where functionality must take definite precedence, such as in creating log entries. Sometimes “good enough” hurts to admit; we just need to remember to focus on the big picture and accept this as the best choice. Keep moving forward! It does not help if one portion of the project is perfect when several dependencies to that piece are not even functional yet. As the Director of my Artificial Intelligence Institute at UGA always said: functional first, optimize later!</p>\n<p>The third quality that is essential for any highly productive developer is the effective use of available tools and people. As developers, we are tool builders. We cannot forget this! We create things to make our lives easier. The world is full of all kinds of fantastic technology to augment our knowledge and abilities. Our tools are intrinsically connected to our minds at this point. Our minds extend into these tools and increase our cognitive potential. Use them wisely! At least use them for goodness sake, no need to brute force something if it can be automated or augmented. Sometimes I see libraries I could have built better but it is often not worth the time to rebuild something if you have a more significant project to accomplish.</p>\n<p>[As an aside, my favorite tools are the programming languages I work with. My worst and best habit as a developer is that I love to study how a language works. At this point I can learn a language in a few days and then move on to another language (and usually a new technology stack). After enough repetition of this, I found it best to master a couple well-documented languages from popular paradigms, like ‘object-oriented’ or ‘functional’, and use them to build a few awesome projects. It will be much more enjoyable and you will become much more employable since most recruiters hire towards the language and not the paradigm. Learn towards executing on significant projects! Many widely-varied, little accomplishments tend not to get noticed.]</p>\n<p>People are the most important tools we can use. Working with a team extends our minds into the collective consciousness of the world. It allows their knowledge and abilities to become our own. This is especially useful when you and the individuals you are working with have the same mission to accomplish but different skills or experience. The greatest increase in the quality and speed of my work at Microsoft came from exploiting the incredible expertise I was surrounded by. It took a leap of humbleness and resulted in productive happiness. With the effective use of tools and people, a developer can do anything better.</p>\n<p>These three traits are inexorably intertwined. A developer must use all available resources, most importantly time, tools, and people, to attack the highest priority tasks. These tasks are cyclically defined by users, designers, project managers, and many other stakeholders. The definition of the final product realized by the developer must be updated regularly through masterful communication. With their mind set to improve these three traits, any developer would be keeping with a focused vision for success.</p>\n<p>BONUS QUALITY<br>\nAnother quality I found to be important and underrepresented is that of emotional management. This is important in relation to yourself and to others. It is essential for making sound decisions and building solid relationships. Always keep the end goal in mind with every action! I will save more comments on this quality for another post.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "6",
            "plaintext": "tl;dr\n\n 1. Displaying clear, concise, and constant communication.\n 2. Properly setting priorities with good time management.\n 3. Effective exploitation of available tools and people.\n    Bonus: Proper emotional management of yourself and stakeholders.\n\nDescribing the most important qualities of a developer is difficult to do\nobjectively without specific limitations or context. Many variables can\ninfluence the answer to this question. These variables include:\n\n * The project the developer is working on.\n * The role or roles the developer has on the team.\n * The team the developer is working with.\n * The stakeholders the developer is working for.\n * The environment the developer is working in.\n\nTo come up with a ranked set of universally applicable set of qualities, all of\nthese factors must be considered. Since we do not have any limitations in place,\nthese traits will end up being applicable to many job roles involving the same\nvariables.\n\nThe first trait I will discuss is significant for any activity that involves the\ncooperation of two or more people (and is also applicable to systems involving\ncomputer agents or machines). It is the quality of clear, concise, and constant\ncommunication. For any project to go well, a developer needs to fully understand\nwhat the goals of the project are and how they will contribute to these goals.\nThese goals should be determined by the visionaries, designers, architects, and,\nultimately, the end user or customer. Communication needs to be clear and\naccurate so the developer is able to extract the appropriate vision from the\nminds of the stakeholders. Communication needs to be concise because time is\noften limited. No need to be verbose unless necessary, verbosity can often lead\nto confusion. Finally, communication needs to be constant because, after all,\nthe world is approaching the technological singularity and is getting\nexponentially complicated at an increasingly faster pace. Demands and\nrequirements are changing constantly.\n\nThere are many project management models that focus on encouraging good behavior\nin communication practices. Through a solid application of the agile software\ndevelopment model, the developer would always have an updated view of what needs\nto be done for all relevant stakeholders. This, in combination with a return of\nexcellent communication to the rest of the team, will inevitably be core to a\ndeveloper’s success. I have seen the importance of this over and over again\nwithin every project I have worked on. Agile influenced models are meant to\naddress the failures and delays in communication that were often present in past\nworkflows.\n\nThe second trait that has proven to be particularly important for my own growth\nis that of time management and properly setting priorities. Whether we like it\nor not, our business world is on a schedule. The only truly finite resource we\nhave is time. Not only is life short, but often project timelines are too. It is\nimportant for any single developer to make sure time sensitive issues or\nfunctionalities are addressed first. Every activity that consumes a developer’s\ntime must be monitored and constantly reassessed for its own individual ranking\non the developer’s todo list. This will require the developer to use the first\nquality I mentioned to ensure they are on the right track. It is not uncommon to\nreceive conflicting task items from multiple stakeholders where priorities are\nnot easily comparable. In these moments, the developer may need to jump into a\nproject management role for a moment and make sure everyone else is\ncommunicating the team’s goals properly within the group.\n\nPrioritizing is not the only part of good time management. You must also make\nsure to put a proportionate amount of time and emotional effort into addressing\neach task. A huge part that plays into this is the avoidance of absolute\nperfection or “analysis paralysis” as many put it. There may be times where\nperfection is necessary, such as in the implementation of a secure data transfer\nsystem, and other times where functionality must take definite precedence, such\nas in creating log entries. Sometimes “good enough” hurts to admit; we just need\nto remember to focus on the big picture and accept this as the best choice. Keep\nmoving forward! It does not help if one portion of the project is perfect when\nseveral dependencies to that piece are not even functional yet. As the Director\nof my Artificial Intelligence Institute at UGA always said: functional first,\noptimize later!\n\nThe third quality that is essential for any highly productive developer is the\neffective use of available tools and people. As developers, we are tool\nbuilders. We cannot forget this! We create things to make our lives easier. The\nworld is full of all kinds of fantastic technology to augment our knowledge and\nabilities. Our tools are intrinsically connected to our minds at this point. Our\nminds extend into these tools and increase our cognitive potential. Use them\nwisely! At least use them for goodness sake, no need to brute force something if\nit can be automated or augmented. Sometimes I see libraries I could have built\nbetter but it is often not worth the time to rebuild something if you have a\nmore significant project to accomplish.\n\n[As an aside, my favorite tools are the programming languages I work with. My\nworst and best habit as a developer is that I love to study how a language\nworks. At this point I can learn a language in a few days and then move on to\nanother language (and usually a new technology stack). After enough repetition\nof this, I found it best to master a couple well-documented languages from\npopular paradigms, like ‘object-oriented’ or ‘functional’, and use them to build\na few awesome projects. It will be much more enjoyable and you will become much\nmore employable since most recruiters hire towards the language and not the\nparadigm. Learn towards executing on significant projects! Many widely-varied,\nlittle accomplishments tend not to get noticed.]\n\nPeople are the most important tools we can use. Working with a team extends our\nminds into the collective consciousness of the world. It allows their knowledge\nand abilities to become our own. This is especially useful when you and the\nindividuals you are working with have the same mission to accomplish but\ndifferent skills or experience. The greatest increase in the quality and speed\nof my work at Microsoft came from exploiting the incredible expertise I was\nsurrounded by. It took a leap of humbleness and resulted in productive\nhappiness. With the effective use of tools and people, a developer can do\nanything better.\n\nThese three traits are inexorably intertwined. A developer must use all\navailable resources, most importantly time, tools, and people, to attack the\nhighest priority tasks. These tasks are cyclically defined by users, designers,\nproject managers, and many other stakeholders. The definition of the final\nproduct realized by the developer must be updated regularly through masterful\ncommunication. With their mind set to improve these three traits, any developer\nwould be keeping with a focused vision for success.\n\nBONUS QUALITY\nAnother quality I found to be important and underrepresented is that of\nemotional management. This is important in relation to yourself and to others.\nIt is essential for making sound decisions and building solid relationships.\nAlways keep the end goal in mind with every action! I will save more comments on\nthis quality for another post.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2013-12-23T13:51:49.000Z",
            "updated_at": "2021-03-09T16:50:24.000Z",
            "published_at": "2013-12-23T13:52:00.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e84",
            "uuid": "a31bd400-f380-4103-9cad-12c8ca294a76",
            "title": "An Unlimited Mode of Living",
            "slug": "an-unlimited-mode-of-living",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\\"There is no real excellence in all this world which can be separated from right living.\\\" ~ David Starr Jordan\\n\\nMy everyday habits have proved to me that excellence can arise out of a few simple habits. A person of excellence must unceasingly acquire new good habits, build and exercise established habits, and eradicate bad habits. I spend all day focusing on building and maintaining good habits and attitudes while checking myself constantly to avoid bad habits and attitudes. This process itself is a great habit! It can feel heavy at times, especially when I recently have discovered several new habits I want to form or a particularly bothersome one that I need to dispose of. This practice reaps many rewards in all areas of my life!\\n\\nThe rewards reaped from good habits require patience to achieve. When I speak of these attitudes and habits, the most important ones I focus on are quite fundamental.\\n\\nHere is a small sample of my current working habits:\\n\\n+ One habit I am currently trying to get rid of involves productive procrastination. Everything I ever do is always worth doing (at least I think so at the time I am doing it). My problem is that the task I am currently working on is not always of the highest priority. Many times I tackle things that I do need to do but are not important and are easy to accomplish. This is a creative form of seemingly well justified procrastination. I should always work on the most important task that I am able to work on at any given moment.\\n+ One habit that I maintain regularly is to make sure that I openly appreciate all of the people I encounter. With my girlfriend, I make sure I tell her that she is beautiful in some form everyday. With my little brother, I always make sure I tell him one thing I really like that he is doing with his life every time I see him. With a new acquaintance, I always make sure that if an idea or criticism they present to me is good, they know about it immediately. These are only a few examples! Appreciation of my peers is super important.\\n+ One habit I am currently establishing within myself involves my own focused self-education. I tend to be interested in, what at times seems to be, everything. It is good to be well-rounded but our current economic model tends to reward individuals for having a small group of interrelated specialties. My current pool of skill and knowledge that is under development relates to human productivity and technology that augments human productivity.\\n\\nThese are only a few habits I am developing. I am also working on my attitude fervently; it always seems to need adjustment! What are you working on?\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>&quot;There is no real excellence in all this world which can be separated from right living.&quot; ~ David Starr Jordan</p>\n<p>My everyday habits have proved to me that excellence can arise out of a few simple habits. A person of excellence must unceasingly acquire new good habits, build and exercise established habits, and eradicate bad habits. I spend all day focusing on building and maintaining good habits and attitudes while checking myself constantly to avoid bad habits and attitudes. This process itself is a great habit! It can feel heavy at times, especially when I recently have discovered several new habits I want to form or a particularly bothersome one that I need to dispose of. This practice reaps many rewards in all areas of my life!</p>\n<p>The rewards reaped from good habits require patience to achieve. When I speak of these attitudes and habits, the most important ones I focus on are quite fundamental.</p>\n<p>Here is a small sample of my current working habits:</p>\n<ul>\n<li>One habit I am currently trying to get rid of involves productive procrastination. Everything I ever do is always worth doing (at least I think so at the time I am doing it). My problem is that the task I am currently working on is not always of the highest priority. Many times I tackle things that I do need to do but are not important and are easy to accomplish. This is a creative form of seemingly well justified procrastination. I should always work on the most important task that I am able to work on at any given moment.</li>\n<li>One habit that I maintain regularly is to make sure that I openly appreciate all of the people I encounter. With my girlfriend, I make sure I tell her that she is beautiful in some form everyday. With my little brother, I always make sure I tell him one thing I really like that he is doing with his life every time I see him. With a new acquaintance, I always make sure that if an idea or criticism they present to me is good, they know about it immediately. These are only a few examples! Appreciation of my peers is super important.</li>\n<li>One habit I am currently establishing within myself involves my own focused self-education. I tend to be interested in, what at times seems to be, everything. It is good to be well-rounded but our current economic model tends to reward individuals for having a small group of interrelated specialties. My current pool of skill and knowledge that is under development relates to human productivity and technology that augments human productivity.</li>\n</ul>\n<p>These are only a few habits I am developing. I am also working on my attitude fervently; it always seems to need adjustment! What are you working on?</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "12",
            "plaintext": "\"There is no real excellence in all this world which can be separated from right\nliving.\" ~ David Starr Jordan\n\nMy everyday habits have proved to me that excellence can arise out of a few\nsimple habits. A person of excellence must unceasingly acquire new good habits,\nbuild and exercise established habits, and eradicate bad habits. I spend all day\nfocusing on building and maintaining good habits and attitudes while checking\nmyself constantly to avoid bad habits and attitudes. This process itself is a\ngreat habit! It can feel heavy at times, especially when I recently have\ndiscovered several new habits I want to form or a particularly bothersome one\nthat I need to dispose of. This practice reaps many rewards in all areas of my\nlife!\n\nThe rewards reaped from good habits require patience to achieve. When I speak of\nthese attitudes and habits, the most important ones I focus on are quite\nfundamental.\n\nHere is a small sample of my current working habits:\n\n * One habit I am currently trying to get rid of involves productive\n   procrastination. Everything I ever do is always worth doing (at least I think\n   so at the time I am doing it). My problem is that the task I am currently\n   working on is not always of the highest priority. Many times I tackle things\n   that I do need to do but are not important and are easy to accomplish. This\n   is a creative form of seemingly well justified procrastination. I should\n   always work on the most important task that I am able to work on at any given\n   moment.\n * One habit that I maintain regularly is to make sure that I openly appreciate\n   all of the people I encounter. With my girlfriend, I make sure I tell her\n   that she is beautiful in some form everyday. With my little brother, I always\n   make sure I tell him one thing I really like that he is doing with his life\n   every time I see him. With a new acquaintance, I always make sure that if an\n   idea or criticism they present to me is good, they know about it immediately.\n   These are only a few examples! Appreciation of my peers is super important.\n * One habit I am currently establishing within myself involves my own focused\n   self-education. I tend to be interested in, what at times seems to be,\n   everything. It is good to be well-rounded but our current economic model\n   tends to reward individuals for having a small group of interrelated\n   specialties. My current pool of skill and knowledge that is under development\n   relates to human productivity and technology that augments human\n   productivity.\n\nThese are only a few habits I am developing. I am also working on my attitude\nfervently; it always seems to need adjustment! What are you working on?",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2014-01-09T20:13:53.000Z",
            "updated_at": "2021-03-09T16:50:05.000Z",
            "published_at": "2014-01-09T20:13:58.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e85",
            "uuid": "e47999ee-026e-4000-b0e2-80539bbcbf66",
            "title": "Four Ways to Build a Personal Site",
            "slug": "4-ways-to-build-a-personal-site",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"For those looking to hack together a personal site for branding and marketing themselves, there are a few ways to go about it. Let’s go from least time consuming and less creative control to more time consuming and more creative control:\\n\\n##Tier 4\\nFor the ultra lazy or busy you can go overboard filling out and polishing up a profile on a popular social media channel (e.g. https://www.linkedin.com/in/paulprae). If you want to keep it really simple, choose something lean like Twitter (e.g. https://twitter.com/praeducer). The more specialized of a channel you choose the better. Find a community that has the same set of passions as you if you can (e.g I used https://github.com/praeducer/ since I am a developer). An important part of this approach is setting up pretty URLs. Another important part is exploiting every feature of the social media tool that involves presenting yourself (e.g. bio, profile pic, job history etc). I like this approach because it is easy to maintain and you probably already have a head start. Plus, the social media sites will do all kinds of SEO and online networking for you.\\n\\n##Tier 3\\nFor those that want a little more creative control but still do not want to put in much time, there are some platforms meant for simply presenting yourself or your material. One social media site that is a great digital business card and personal splash page is http://about.me/ (e.g. http://about.me/paulprae). This is what I went with for paulprae.com for awhile. It's great because you can setup an alias to a custom domain. I have built many personal sites in the past but this was the biggest win for time spent ever. You can also use some of the more simple blogging platforms for this kind of approach. I like https://medium.com/, https://ghost.org/ (e.g. http://blog.paulprae.com/ and the current /), and https://www.tumblr.com/ (e.g. http://www.unkemptsilence.com/). If done right, these tools can really feel like a personal website (unlike the Tier 4 options). Using custom URLs and linking out to your social media platforms will be good to keep in mind with this technique.\\n\\n##Tier 2\\nFor the more adventurous hackers you can use a web site builder or hosted content management system to build a custom site. This will allow for much more creative control over your site than the previous options. Some of the best options out there for getting something beautiful up quick are http://wordpress.com/, https://prosite.com/, and http://squarespace.com. These tools can seem like a lot at first but once you learn their basic functionality you can make something respectable with ease. Plus, people will respect you for really building something. You can even put these tools on your resume!\\n\\n##Tier 1\\nFor those that love torturing themselves, you can build something more or less from scratch. Beware that building a web site in a self-hosted or cloud-hosted environment can get complicated quickly. You can make things easier by spinning up popular and well supported content management systems like WordPress or Drupal (https://www.drupal.org/) on a cloud server (e.g. https://bitnami.com/stack/wordpress, http://www.rackspace.com/cloud/sites/web-hosting/drupal/, https://developer.rackspace.com/blog/launch-ghost-with-rackspace-deployments/). Then, after the install, this option is not much more complicated than the Tier 2 option. If you want to use more barebone web development frameworks, cloud computing and server images come together to make this not completely crazy to do either (for Windows Azure see: http://vmdepot.msopentech.com/List/Index). Many cloud services have one-click install packages that can spin up servers with everything you need to build your site (the main advantage of https://bitnami.com/stacks). If you dig around enough, you may even be able to get a free cloud server for awhile. Rackspace has done this in the past and Amazon and Microsoft are doing it right now: http://aws.amazon.com/free/ and http://azure.microsoft.com/en-us/develop/net/aspnet/. If you want to build a feature rich web site, better get studying on modern web application architecture. For something quick consider https://www.meteor.com/ and for something unnecesarily complicated and robust for a personal site consider Angular, Express, and Node (http://briantford.com/blog/angular-express).\\n\\nIt is important to have a personal site to support your professional networking, especially if you are out in the job market. Choose a Tier above that meets your time and skill level, then get it going! Focus on quality (proofread everything!) and simplicity. An important part about having a web presence is uploading quality content regularly. Make sure whatever route you choose is easy for you to update and maintain. Have some fun with it and make it your own!\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>For those looking to hack together a personal site for branding and marketing themselves, there are a few ways to go about it. Let’s go from least time consuming and less creative control to more time consuming and more creative control:</p>\n<h2 id=\"tier4\">Tier 4</h2>\n<p>For the ultra lazy or busy you can go overboard filling out and polishing up a profile on a popular social media channel (e.g. <a href=\"https://www.linkedin.com/in/paulprae\">https://www.linkedin.com/in/paulprae</a>). If you want to keep it really simple, choose something lean like Twitter (e.g. <a href=\"https://twitter.com/praeducer\">https://twitter.com/praeducer</a>). The more specialized of a channel you choose the better. Find a community that has the same set of passions as you if you can (e.g I used <a href=\"https://github.com/praeducer/\">https://github.com/praeducer/</a> since I am a developer). An important part of this approach is setting up pretty URLs. Another important part is exploiting every feature of the social media tool that involves presenting yourself (e.g. bio, profile pic, job history etc). I like this approach because it is easy to maintain and you probably already have a head start. Plus, the social media sites will do all kinds of SEO and online networking for you.</p>\n<h2 id=\"tier3\">Tier 3</h2>\n<p>For those that want a little more creative control but still do not want to put in much time, there are some platforms meant for simply presenting yourself or your material. One social media site that is a great digital business card and personal splash page is <a href=\"http://about.me/\">http://about.me/</a> (e.g. <a href=\"http://about.me/paulprae\">http://about.me/paulprae</a>). This is what I went with for paulprae.com for awhile. It's great because you can setup an alias to a custom domain. I have built many personal sites in the past but this was the biggest win for time spent ever. You can also use some of the more simple blogging platforms for this kind of approach. I like <a href=\"https://medium.com/\">https://medium.com/</a>, <a href=\"https://ghost.org/\">https://ghost.org/</a> (e.g. <a href=\"http://blog.paulprae.com/\">http://blog.paulprae.com/</a> and the current /), and <a href=\"https://www.tumblr.com/\">https://www.tumblr.com/</a> (e.g. <a href=\"http://www.unkemptsilence.com/\">http://www.unkemptsilence.com/</a>). If done right, these tools can really feel like a personal website (unlike the Tier 4 options). Using custom URLs and linking out to your social media platforms will be good to keep in mind with this technique.</p>\n<h2 id=\"tier2\">Tier 2</h2>\n<p>For the more adventurous hackers you can use a web site builder or hosted content management system to build a custom site. This will allow for much more creative control over your site than the previous options. Some of the best options out there for getting something beautiful up quick are <a href=\"http://wordpress.com/\">http://wordpress.com/</a>, <a href=\"https://prosite.com/\">https://prosite.com/</a>, and <a href=\"http://squarespace.com\">http://squarespace.com</a>. These tools can seem like a lot at first but once you learn their basic functionality you can make something respectable with ease. Plus, people will respect you for really building something. You can even put these tools on your resume!</p>\n<h2 id=\"tier1\">Tier 1</h2>\n<p>For those that love torturing themselves, you can build something more or less from scratch. Beware that building a web site in a self-hosted or cloud-hosted environment can get complicated quickly. You can make things easier by spinning up popular and well supported content management systems like WordPress or Drupal (<a href=\"https://www.drupal.org/\">https://www.drupal.org/</a>) on a cloud server (e.g. <a href=\"https://bitnami.com/stack/wordpress\">https://bitnami.com/stack/wordpress</a>, <a href=\"http://www.rackspace.com/cloud/sites/web-hosting/drupal/\">http://www.rackspace.com/cloud/sites/web-hosting/drupal/</a>, <a href=\"https://developer.rackspace.com/blog/launch-ghost-with-rackspace-deployments/\">https://developer.rackspace.com/blog/launch-ghost-with-rackspace-deployments/</a>). Then, after the install, this option is not much more complicated than the Tier 2 option. If you want to use more barebone web development frameworks, cloud computing and server images come together to make this not completely crazy to do either (for Windows Azure see: <a href=\"http://vmdepot.msopentech.com/List/Index\">http://vmdepot.msopentech.com/List/Index</a>). Many cloud services have one-click install packages that can spin up servers with everything you need to build your site (the main advantage of <a href=\"https://bitnami.com/stacks\">https://bitnami.com/stacks</a>). If you dig around enough, you may even be able to get a free cloud server for awhile. Rackspace has done this in the past and Amazon and Microsoft are doing it right now: <a href=\"http://aws.amazon.com/free/\">http://aws.amazon.com/free/</a> and <a href=\"http://azure.microsoft.com/en-us/develop/net/aspnet/\">http://azure.microsoft.com/en-us/develop/net/aspnet/</a>. If you want to build a feature rich web site, better get studying on modern web application architecture. For something quick consider <a href=\"https://www.meteor.com/\">https://www.meteor.com/</a> and for something unnecesarily complicated and robust for a personal site consider Angular, Express, and Node (<a href=\"http://briantford.com/blog/angular-express\">http://briantford.com/blog/angular-express</a>).</p>\n<p>It is important to have a personal site to support your professional networking, especially if you are out in the job market. Choose a Tier above that meets your time and skill level, then get it going! Focus on quality (proofread everything!) and simplicity. An important part about having a web presence is uploading quality content regularly. Make sure whatever route you choose is easy for you to update and maintain. Have some fun with it and make it your own!</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "13",
            "plaintext": "For those looking to hack together a personal site for branding and marketing\nthemselves, there are a few ways to go about it. Let’s go from least time\nconsuming and less creative control to more time consuming and more creative\ncontrol:\n\nTier 4\nFor the ultra lazy or busy you can go overboard filling out and polishing up a\nprofile on a popular social media channel (e.g. \nhttps://www.linkedin.com/in/paulprae). If you want to keep it really simple,\nchoose something lean like Twitter (e.g. https://twitter.com/praeducer). The\nmore specialized of a channel you choose the better. Find a community that has\nthe same set of passions as you if you can (e.g I used \nhttps://github.com/praeducer/ since I am a developer). An important part of this\napproach is setting up pretty URLs. Another important part is exploiting every\nfeature of the social media tool that involves presenting yourself (e.g. bio,\nprofile pic, job history etc). I like this approach because it is easy to\nmaintain and you probably already have a head start. Plus, the social media\nsites will do all kinds of SEO and online networking for you.\n\nTier 3\nFor those that want a little more creative control but still do not want to put\nin much time, there are some platforms meant for simply presenting yourself or\nyour material. One social media site that is a great digital business card and\npersonal splash page is http://about.me/ (e.g. http://about.me/paulprae). This\nis what I went with for paulprae.com for awhile. It's great because you can\nsetup an alias to a custom domain. I have built many personal sites in the past\nbut this was the biggest win for time spent ever. You can also use some of the\nmore simple blogging platforms for this kind of approach. I like \nhttps://medium.com/, https://ghost.org/ (e.g. http://blog.paulprae.com/ and the\ncurrent /), and https://www.tumblr.com/ (e.g. http://www.unkemptsilence.com/).\nIf done right, these tools can really feel like a personal website (unlike the\nTier 4 options). Using custom URLs and linking out to your social media\nplatforms will be good to keep in mind with this technique.\n\nTier 2\nFor the more adventurous hackers you can use a web site builder or hosted\ncontent management system to build a custom site. This will allow for much more\ncreative control over your site than the previous options. Some of the best\noptions out there for getting something beautiful up quick are \nhttp://wordpress.com/, https://prosite.com/, and http://squarespace.com. These\ntools can seem like a lot at first but once you learn their basic functionality\nyou can make something respectable with ease. Plus, people will respect you for\nreally building something. You can even put these tools on your resume!\n\nTier 1\nFor those that love torturing themselves, you can build something more or less\nfrom scratch. Beware that building a web site in a self-hosted or cloud-hosted\nenvironment can get complicated quickly. You can make things easier by spinning\nup popular and well supported content management systems like WordPress or\nDrupal (https://www.drupal.org/) on a cloud server (e.g. \nhttps://bitnami.com/stack/wordpress, \nhttp://www.rackspace.com/cloud/sites/web-hosting/drupal/, \nhttps://developer.rackspace.com/blog/launch-ghost-with-rackspace-deployments/).\nThen, after the install, this option is not much more complicated than the Tier\n2 option. If you want to use more barebone web development frameworks, cloud\ncomputing and server images come together to make this not completely crazy to\ndo either (for Windows Azure see: http://vmdepot.msopentech.com/List/Index).\nMany cloud services have one-click install packages that can spin up servers\nwith everything you need to build your site (the main advantage of \nhttps://bitnami.com/stacks). If you dig around enough, you may even be able to\nget a free cloud server for awhile. Rackspace has done this in the past and\nAmazon and Microsoft are doing it right now: http://aws.amazon.com/free/ and \nhttp://azure.microsoft.com/en-us/develop/net/aspnet/. If you want to build a\nfeature rich web site, better get studying on modern web application\narchitecture. For something quick consider https://www.meteor.com/ and for\nsomething unnecesarily complicated and robust for a personal site consider\nAngular, Express, and Node (http://briantford.com/blog/angular-express).\n\nIt is important to have a personal site to support your professional networking,\nespecially if you are out in the job market. Choose a Tier above that meets your\ntime and skill level, then get it going! Focus on quality (proofread\neverything!) and simplicity. An important part about having a web presence is\nuploading quality content regularly. Make sure whatever route you choose is easy\nfor you to update and maintain. Have some fun with it and make it your own!",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2014-08-17T22:07:30.000Z",
            "updated_at": "2021-03-09T16:49:44.000Z",
            "published_at": "2014-08-17T22:07:39.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e86",
            "uuid": "dc3bd37d-ee70-4adc-bf81-bc27647f0f80",
            "title": "My Path to Grad School",
            "slug": "the-specialist-that-i-want-to-become",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I live to make the most positive impact on human existence. As population grows, as higher education is more accessible, and as we continue to automate as much of our work as possible, it will be increasingly difficult to stand out. For the benefit of myself and society at large, I am closely listening to the needs of society that I can best satisfy. To compete in this quickly changing economic climate, we need to specialize in work that will be in high demand for years to come. I am building a stable career path that leads towards the impact I want to make. I am ready to start applying to part-time graduate school programs to help me fulfill my vision of the future.\\n    \\nFor several years now I have researched the job market to find the most in demand skills and knowledge that live at the intersection of my interests and abilities. I am defining myself as a solutions architect who solves digital marketing problems with [biologically inspired machine learning](http://en.wikipedia.org/wiki/Natural_computing#Nature-inspired_models_of_computation) techniques. The size, volatility, and overall complexity of the online space makes digital marketing incredibly challenging. I want to help connect people to brands using software solutions optimized to find insights in such complex systems. Pursuing a graduate education in machine learning or predictive marketing analytics will further hone the skills and knowledge I need to create these solutions.\\n    \\nMy drive towards a machine learning career began with my studies in artificial intelligence during my undergraduate program. I combined programs in [Computer Science and Cognitive Science](http://www.ai.uga.edu/) to lay the necessary foundation I would need to create intelligent software solutions. My coursework focused on how to design and build software that effectively interacts with or emulates the human mind. Particularly, my projects and papers often explored human computer interaction, natural language processing, or algorithms that emulated biological processes (especially [neural networks](http://en.wikipedia.org/wiki/Artificial_neural_network) and [genetic algorithms](http://en.wikipedia.org/wiki/Genetic_algorithm)). The more I researched these subjects, the more I wanted to learn and apply my learnings to real-world problems.\\n    \\nDuring college, the work experience I acquired across engineering disciplines complemented my academics in mind opening ways. My first two internships were dedicated to working in agriculture and water treatment. During the first internship, I built out working prototypes of sprayers based on circuit diagrams I drafted in AutoCAD. During the second, I created designs for multi-million dollar water treatment systems based on my own cost, flow, and power analyses in Excel. Following these internships, [my creative interests](https://soundcloud.com/praeducer) and technical abilities eventually led me into audio engineering for recording studios and music venues in Athens, Georgia. Bootstrapping myself as an audio engineer while studying software engineering allowed me to see the parallels between the fields. I began to notice the importance of abstraction, extensibility, communication protocol, signal flow, and so much more. I was becoming a practiced interdisciplinarian well suited for solution architecture.\\n    \\nTowards the end of my college career, freelancing as an audio engineer and studying computer science led me directly into software and [digital strategy](http://en.wikipedia.org/wiki/Digital_strategy) consulting for local startup companies. My entrepreneurial mindset and my courage as a hacker allowed me to acquire enough gigs developing web applications to support myself. I learned whatever technologies I needed to learn to get the job done. My associate degree in business administration proved useful as I learned all members of a successful startup team must exercise business analysis (this degree also helped me deal with account management and billing). During all of these early work experiences, I practiced many parts of the systems development lifecycle. These various pursuits further strengthened my abilities as a systems thinker. I developed the confidence to solve any problem.\\n    \\nI entered into industry immediately upon graduation, taking a job as a Support Engineer at Microsoft. There I was placed into the Microsoft Academy of College Hires where I learned enterprise level systems administration. The training focused on the design and development of massive server farms. I further refined my understanding of systems development with a special focus on security, performance, and scalability. After four months on the job, I began working daily with Fortune 100 IT personnel on scores of data intensive projects at various stages of the content management lifecycle. Within my first eighteen months I was promoted to a Senior position and took on so many side projects that I was nominated for Entrepreneur of the Year in my group. The journey within Microsoft Premier Support was awesome, but I knew I needed more relevant experience if I were to pursue a career in machine learning there or elsewhere. Any position that truly interested me within Microsoft where I could practice machine learning required a master’s degree.\\n    \\nMy search for the right opportunity led me outside of Microsoft to the much more agile and culture rich landscape of [Red Ventures](http://www.redventures.com/). Here, I work daily with business analysts primarily implementing web analytics solutions and optimizing sites for search. The tremendous amount of knowledge and skill I have gained in open source software development can only be matched by everything I am learning about digital marketing. When I gather the requirements behind the solutions I am building, I make sure the business strategy is clear to me first (even challenging it sometimes). This habit ensures I am capturing the correct data, managing it appropriately, and presenting it in a way that provides the expected results.\\n    \\nWhat I provide using analytics at Red Ventures is only an enlightening description of *what has happened* or, in the best case, *what is currently happening* in the marketplace. In order to go beyond mere incremental improvements, I will need to help the company see *what will happen* in the future. We are pushing our business intelligence and emerging media teams to develop skills in predictive analytics. We are on the hunt right now for data scientists to do this and more. My favorite open position, Director of Data Science, at Red Ventures requires a master’s degree (and a lot more hands on experience). Many of the most exciting opportunities I have explored in the Charlotte area, at companies like IBM, [Tresata](http://tresata.com/), Bank of America, and [Premier Inc.](https://www.premierinc.com/), require the education and experience a master’s program would provide me.\\n    \\nI have reached a point in my career where I must accelerate towards a specialization in order to qualify for the opportunities I want to pursue. I have pushed myself in the evenings to take several courses offered through massively open online courseware across the field of data science. It is time that I start following a strict and complete academic curriculum under the advisement of world class faculty. If I were to enter into a graduate program, I would choose courses to deepen my understanding of machine learning and predictive marketing analytics, like [Machine Learning, Consumer Analytics, and Complex Adaptive Systems](http://analytics.uncc.edu/academics/course-description) from UNCC’s Master’s Degree in Data Science and Business Analytics. These courses would allow me to better compete in the marketplace while also affording me more opportunities in work that I would enjoy.\\n    \\nI would prefer a program’s faculty members, students, and industry partners to be just down the road from my home. The single most valuable takeaway from my undergraduate program was my network. To this day I still gain numerous benefits from the people and companies I worked with throughout my college career. I would love to have regular consultation from professors that have both industry and research experience in intelligent systems and marketing. Shaking hands with them after class and being able to attend their office hours in person would be a huge win for me.\\n    \\nAnother goal I have is to broaden and deepen strategic innovation within the South East. I am already captivated by various local startup scenes. Being an entrepreneur at heart, I cannot get enough of the events around my current hometown of Charlotte, North Carolina (go [Packard Place](http://packardplace.us/)!). My career started in the startup scene in Athens and Atlanta, Georgia (I still regularly consult companies, like [Routine](https://routi.ne/), from those areas). I plan to return to entrepreneurship full-time once my knowledge-base and network is strong enough. The South East is where I plan to start my family and my next data-driven business. I am dedicated to expanding and empowering my local community so we can make the world better together. These points and more show that the momentum of my life is leading me to attend a local graduate program, maybe in the evenings after work.\\n    \\nIf you explore my web presence, starting with [paulprae.com](http://paulprae.com/) or simply by Googling my name, you will see a trail of evidence that proves my preparation and career intentions. My [LinkedIn profile](https://www.linkedin.com/in/paulprae) details my work experience and education. Application of my knowledge and ability is exemplified through my creative works and code displayed at [behance.net/paulprae](http://www.behance.net/paulprae) and on [GitHub](http://www.github.com/praeducer). You can read insights from my experiences on this blog, [blog.paulprae.com](http://blog.paulprae.com), and see the depth of my learnings in my undergraduate papers on [Academia.edu](http://uga.academia.edu/paulprae). You are also welcome to witness some of my greatest conclusions and see links to my favorite articles of interest by running through my Twitter feed (follow me [@praeducer](https://twitter.com/praeducer)). Collectively, this web presence not only shows you who I am but also represents my abilities to execute on what I have learned regarding personal branding and digital marketing.\\n    \\nAs was the intention of this essay, the digital content I have on display tells my story. I have been working towards a degree in machine learning or predictive marketing analytics for years now. I applied to [UNCC’s Data Science and Business Analytics program](http://analytics.uncc.edu) recently and plan on applying to others soon. The need for data scientists is growing at an explosive pace for a reason—the world needs the specialist that I want to become. Program’s like these will help me increase the positive impact I will make on existence. I am ready to start this monumental step towards a better future for my family and for society.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>I live to make the most positive impact on human existence. As population grows, as higher education is more accessible, and as we continue to automate as much of our work as possible, it will be increasingly difficult to stand out. For the benefit of myself and society at large, I am closely listening to the needs of society that I can best satisfy. To compete in this quickly changing economic climate, we need to specialize in work that will be in high demand for years to come. I am building a stable career path that leads towards the impact I want to make. I am ready to start applying to part-time graduate school programs to help me fulfill my vision of the future.</p>\n<p>For several years now I have researched the job market to find the most in demand skills and knowledge that live at the intersection of my interests and abilities. I am defining myself as a solutions architect who solves digital marketing problems with <a href=\"http://en.wikipedia.org/wiki/Natural_computing#Nature-inspired_models_of_computation\">biologically inspired machine learning</a> techniques. The size, volatility, and overall complexity of the online space makes digital marketing incredibly challenging. I want to help connect people to brands using software solutions optimized to find insights in such complex systems. Pursuing a graduate education in machine learning or predictive marketing analytics will further hone the skills and knowledge I need to create these solutions.</p>\n<p>My drive towards a machine learning career began with my studies in artificial intelligence during my undergraduate program. I combined programs in <a href=\"http://www.ai.uga.edu/\">Computer Science and Cognitive Science</a> to lay the necessary foundation I would need to create intelligent software solutions. My coursework focused on how to design and build software that effectively interacts with or emulates the human mind. Particularly, my projects and papers often explored human computer interaction, natural language processing, or algorithms that emulated biological processes (especially <a href=\"http://en.wikipedia.org/wiki/Artificial_neural_network\">neural networks</a> and <a href=\"http://en.wikipedia.org/wiki/Genetic_algorithm\">genetic algorithms</a>). The more I researched these subjects, the more I wanted to learn and apply my learnings to real-world problems.</p>\n<p>During college, the work experience I acquired across engineering disciplines complemented my academics in mind opening ways. My first two internships were dedicated to working in agriculture and water treatment. During the first internship, I built out working prototypes of sprayers based on circuit diagrams I drafted in AutoCAD. During the second, I created designs for multi-million dollar water treatment systems based on my own cost, flow, and power analyses in Excel. Following these internships, <a href=\"https://soundcloud.com/praeducer\">my creative interests</a> and technical abilities eventually led me into audio engineering for recording studios and music venues in Athens, Georgia. Bootstrapping myself as an audio engineer while studying software engineering allowed me to see the parallels between the fields. I began to notice the importance of abstraction, extensibility, communication protocol, signal flow, and so much more. I was becoming a practiced interdisciplinarian well suited for solution architecture.</p>\n<p>Towards the end of my college career, freelancing as an audio engineer and studying computer science led me directly into software and <a href=\"http://en.wikipedia.org/wiki/Digital_strategy\">digital strategy</a> consulting for local startup companies. My entrepreneurial mindset and my courage as a hacker allowed me to acquire enough gigs developing web applications to support myself. I learned whatever technologies I needed to learn to get the job done. My associate degree in business administration proved useful as I learned all members of a successful startup team must exercise business analysis (this degree also helped me deal with account management and billing). During all of these early work experiences, I practiced many parts of the systems development lifecycle. These various pursuits further strengthened my abilities as a systems thinker. I developed the confidence to solve any problem.</p>\n<p>I entered into industry immediately upon graduation, taking a job as a Support Engineer at Microsoft. There I was placed into the Microsoft Academy of College Hires where I learned enterprise level systems administration. The training focused on the design and development of massive server farms. I further refined my understanding of systems development with a special focus on security, performance, and scalability. After four months on the job, I began working daily with Fortune 100 IT personnel on scores of data intensive projects at various stages of the content management lifecycle. Within my first eighteen months I was promoted to a Senior position and took on so many side projects that I was nominated for Entrepreneur of the Year in my group. The journey within Microsoft Premier Support was awesome, but I knew I needed more relevant experience if I were to pursue a career in machine learning there or elsewhere. Any position that truly interested me within Microsoft where I could practice machine learning required a master’s degree.</p>\n<p>My search for the right opportunity led me outside of Microsoft to the much more agile and culture rich landscape of <a href=\"http://www.redventures.com/\">Red Ventures</a>. Here, I work daily with business analysts primarily implementing web analytics solutions and optimizing sites for search. The tremendous amount of knowledge and skill I have gained in open source software development can only be matched by everything I am learning about digital marketing. When I gather the requirements behind the solutions I am building, I make sure the business strategy is clear to me first (even challenging it sometimes). This habit ensures I am capturing the correct data, managing it appropriately, and presenting it in a way that provides the expected results.</p>\n<p>What I provide using analytics at Red Ventures is only an enlightening description of <em>what has happened</em> or, in the best case, <em>what is currently happening</em> in the marketplace. In order to go beyond mere incremental improvements, I will need to help the company see <em>what will happen</em> in the future. We are pushing our business intelligence and emerging media teams to develop skills in predictive analytics. We are on the hunt right now for data scientists to do this and more. My favorite open position, Director of Data Science, at Red Ventures requires a master’s degree (and a lot more hands on experience). Many of the most exciting opportunities I have explored in the Charlotte area, at companies like IBM, <a href=\"http://tresata.com/\">Tresata</a>, Bank of America, and <a href=\"https://www.premierinc.com/\">Premier Inc.</a>, require the education and experience a master’s program would provide me.</p>\n<p>I have reached a point in my career where I must accelerate towards a specialization in order to qualify for the opportunities I want to pursue. I have pushed myself in the evenings to take several courses offered through massively open online courseware across the field of data science. It is time that I start following a strict and complete academic curriculum under the advisement of world class faculty. If I were to enter into a graduate program, I would choose courses to deepen my understanding of machine learning and predictive marketing analytics, like <a href=\"http://analytics.uncc.edu/academics/course-description\">Machine Learning, Consumer Analytics, and Complex Adaptive Systems</a> from UNCC’s Master’s Degree in Data Science and Business Analytics. These courses would allow me to better compete in the marketplace while also affording me more opportunities in work that I would enjoy.</p>\n<p>I would prefer a program’s faculty members, students, and industry partners to be just down the road from my home. The single most valuable takeaway from my undergraduate program was my network. To this day I still gain numerous benefits from the people and companies I worked with throughout my college career. I would love to have regular consultation from professors that have both industry and research experience in intelligent systems and marketing. Shaking hands with them after class and being able to attend their office hours in person would be a huge win for me.</p>\n<p>Another goal I have is to broaden and deepen strategic innovation within the South East. I am already captivated by various local startup scenes. Being an entrepreneur at heart, I cannot get enough of the events around my current hometown of Charlotte, North Carolina (go <a href=\"http://packardplace.us/\">Packard Place</a>!). My career started in the startup scene in Athens and Atlanta, Georgia (I still regularly consult companies, like <a href=\"https://routi.ne/\">Routine</a>, from those areas). I plan to return to entrepreneurship full-time once my knowledge-base and network is strong enough. The South East is where I plan to start my family and my next data-driven business. I am dedicated to expanding and empowering my local community so we can make the world better together. These points and more show that the momentum of my life is leading me to attend a local graduate program, maybe in the evenings after work.</p>\n<p>If you explore my web presence, starting with <a href=\"http://paulprae.com/\">paulprae.com</a> or simply by Googling my name, you will see a trail of evidence that proves my preparation and career intentions. My <a href=\"https://www.linkedin.com/in/paulprae\">LinkedIn profile</a> details my work experience and education. Application of my knowledge and ability is exemplified through my creative works and code displayed at <a href=\"http://www.behance.net/paulprae\">behance.net/paulprae</a> and on <a href=\"http://www.github.com/praeducer\">GitHub</a>. You can read insights from my experiences on this blog, <a href=\"http://blog.paulprae.com\">blog.paulprae.com</a>, and see the depth of my learnings in my undergraduate papers on <a href=\"http://uga.academia.edu/paulprae\">Academia.edu</a>. You are also welcome to witness some of my greatest conclusions and see links to my favorite articles of interest by running through my Twitter feed (follow me <a href=\"https://twitter.com/praeducer\">@praeducer</a>). Collectively, this web presence not only shows you who I am but also represents my abilities to execute on what I have learned regarding personal branding and digital marketing.</p>\n<p>As was the intention of this essay, the digital content I have on display tells my story. I have been working towards a degree in machine learning or predictive marketing analytics for years now. I applied to <a href=\"http://analytics.uncc.edu\">UNCC’s Data Science and Business Analytics program</a> recently and plan on applying to others soon. The need for data scientists is growing at an explosive pace for a reason—the world needs the specialist that I want to become. Program’s like these will help me increase the positive impact I will make on existence. I am ready to start this monumental step towards a better future for my family and for society.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "14",
            "plaintext": "I live to make the most positive impact on human existence. As population grows,\nas higher education is more accessible, and as we continue to automate as much\nof our work as possible, it will be increasingly difficult to stand out. For the\nbenefit of myself and society at large, I am closely listening to the needs of\nsociety that I can best satisfy. To compete in this quickly changing economic\nclimate, we need to specialize in work that will be in high demand for years to\ncome. I am building a stable career path that leads towards the impact I want to\nmake. I am ready to start applying to part-time graduate school programs to help\nme fulfill my vision of the future.\n\nFor several years now I have researched the job market to find the most in\ndemand skills and knowledge that live at the intersection of my interests and\nabilities. I am defining myself as a solutions architect who solves digital\nmarketing problems with biologically inspired machine learning\n[http://en.wikipedia.org/wiki/Natural_computing#Nature-inspired_models_of_computation] \ntechniques. The size, volatility, and overall complexity of the online space\nmakes digital marketing incredibly challenging. I want to help connect people to\nbrands using software solutions optimized to find insights in such complex\nsystems. Pursuing a graduate education in machine learning or predictive\nmarketing analytics will further hone the skills and knowledge I need to create\nthese solutions.\n\nMy drive towards a machine learning career began with my studies in artificial\nintelligence during my undergraduate program. I combined programs in Computer\nScience and Cognitive Science [http://www.ai.uga.edu/] to lay the necessary\nfoundation I would need to create intelligent software solutions. My coursework\nfocused on how to design and build software that effectively interacts with or\nemulates the human mind. Particularly, my projects and papers often explored\nhuman computer interaction, natural language processing, or algorithms that\nemulated biological processes (especially neural networks\n[http://en.wikipedia.org/wiki/Artificial_neural_network] and genetic algorithms\n[http://en.wikipedia.org/wiki/Genetic_algorithm]). The more I researched these\nsubjects, the more I wanted to learn and apply my learnings to real-world\nproblems.\n\nDuring college, the work experience I acquired across engineering disciplines\ncomplemented my academics in mind opening ways. My first two internships were\ndedicated to working in agriculture and water treatment. During the first\ninternship, I built out working prototypes of sprayers based on circuit diagrams\nI drafted in AutoCAD. During the second, I created designs for multi-million\ndollar water treatment systems based on my own cost, flow, and power analyses in\nExcel. Following these internships, my creative interests\n[https://soundcloud.com/praeducer] and technical abilities eventually led me\ninto audio engineering for recording studios and music venues in Athens,\nGeorgia. Bootstrapping myself as an audio engineer while studying software\nengineering allowed me to see the parallels between the fields. I began to\nnotice the importance of abstraction, extensibility, communication protocol,\nsignal flow, and so much more. I was becoming a practiced interdisciplinarian\nwell suited for solution architecture.\n\nTowards the end of my college career, freelancing as an audio engineer and\nstudying computer science led me directly into software and digital strategy\n[http://en.wikipedia.org/wiki/Digital_strategy] consulting for local startup\ncompanies. My entrepreneurial mindset and my courage as a hacker allowed me to\nacquire enough gigs developing web applications to support myself. I learned\nwhatever technologies I needed to learn to get the job done. My associate degree\nin business administration proved useful as I learned all members of a\nsuccessful startup team must exercise business analysis (this degree also helped\nme deal with account management and billing). During all of these early work\nexperiences, I practiced many parts of the systems development lifecycle. These\nvarious pursuits further strengthened my abilities as a systems thinker. I\ndeveloped the confidence to solve any problem.\n\nI entered into industry immediately upon graduation, taking a job as a Support\nEngineer at Microsoft. There I was placed into the Microsoft Academy of College\nHires where I learned enterprise level systems administration. The training\nfocused on the design and development of massive server farms. I further refined\nmy understanding of systems development with a special focus on security,\nperformance, and scalability. After four months on the job, I began working\ndaily with Fortune 100 IT personnel on scores of data intensive projects at\nvarious stages of the content management lifecycle. Within my first eighteen\nmonths I was promoted to a Senior position and took on so many side projects\nthat I was nominated for Entrepreneur of the Year in my group. The journey\nwithin Microsoft Premier Support was awesome, but I knew I needed more relevant\nexperience if I were to pursue a career in machine learning there or elsewhere.\nAny position that truly interested me within Microsoft where I could practice\nmachine learning required a master’s degree.\n\nMy search for the right opportunity led me outside of Microsoft to the much more\nagile and culture rich landscape of Red Ventures [http://www.redventures.com/].\nHere, I work daily with business analysts primarily implementing web analytics\nsolutions and optimizing sites for search. The tremendous amount of knowledge\nand skill I have gained in open source software development can only be matched\nby everything I am learning about digital marketing. When I gather the\nrequirements behind the solutions I am building, I make sure the business\nstrategy is clear to me first (even challenging it sometimes). This habit\nensures I am capturing the correct data, managing it appropriately, and\npresenting it in a way that provides the expected results.\n\nWhat I provide using analytics at Red Ventures is only an enlightening\ndescription of what has happened or, in the best case, what is currently\nhappening in the marketplace. In order to go beyond mere incremental\nimprovements, I will need to help the company see what will happen in the\nfuture. We are pushing our business intelligence and emerging media teams to\ndevelop skills in predictive analytics. We are on the hunt right now for data\nscientists to do this and more. My favorite open position, Director of Data\nScience, at Red Ventures requires a master’s degree (and a lot more hands on\nexperience). Many of the most exciting opportunities I have explored in the\nCharlotte area, at companies like IBM, Tresata [http://tresata.com/], Bank of\nAmerica, and Premier Inc. [https://www.premierinc.com/], require the education\nand experience a master’s program would provide me.\n\nI have reached a point in my career where I must accelerate towards a\nspecialization in order to qualify for the opportunities I want to pursue. I\nhave pushed myself in the evenings to take several courses offered through\nmassively open online courseware across the field of data science. It is time\nthat I start following a strict and complete academic curriculum under the\nadvisement of world class faculty. If I were to enter into a graduate program, I\nwould choose courses to deepen my understanding of machine learning and\npredictive marketing analytics, like Machine Learning, Consumer Analytics, and\nComplex Adaptive Systems\n[http://analytics.uncc.edu/academics/course-description] from UNCC’s Master’s\nDegree in Data Science and Business Analytics. These courses would allow me to\nbetter compete in the marketplace while also affording me more opportunities in\nwork that I would enjoy.\n\nI would prefer a program’s faculty members, students, and industry partners to\nbe just down the road from my home. The single most valuable takeaway from my\nundergraduate program was my network. To this day I still gain numerous benefits\nfrom the people and companies I worked with throughout my college career. I\nwould love to have regular consultation from professors that have both industry\nand research experience in intelligent systems and marketing. Shaking hands with\nthem after class and being able to attend their office hours in person would be\na huge win for me.\n\nAnother goal I have is to broaden and deepen strategic innovation within the\nSouth East. I am already captivated by various local startup scenes. Being an\nentrepreneur at heart, I cannot get enough of the events around my current\nhometown of Charlotte, North Carolina (go Packard Place\n[http://packardplace.us/]!). My career started in the startup scene in Athens\nand Atlanta, Georgia (I still regularly consult companies, like Routine\n[https://routi.ne/], from those areas). I plan to return to entrepreneurship\nfull-time once my knowledge-base and network is strong enough. The South East is\nwhere I plan to start my family and my next data-driven business. I am dedicated\nto expanding and empowering my local community so we can make the world better\ntogether. These points and more show that the momentum of my life is leading me\nto attend a local graduate program, maybe in the evenings after work.\n\nIf you explore my web presence, starting with paulprae.com\n[http://paulprae.com/] or simply by Googling my name, you will see a trail of\nevidence that proves my preparation and career intentions. My LinkedIn profile\n[https://www.linkedin.com/in/paulprae] details my work experience and education.\nApplication of my knowledge and ability is exemplified through my creative works\nand code displayed at behance.net/paulprae [http://www.behance.net/paulprae] and\non GitHub [http://www.github.com/praeducer]. You can read insights from my\nexperiences on this blog, blog.paulprae.com [http://blog.paulprae.com], and see\nthe depth of my learnings in my undergraduate papers on Academia.edu\n[http://uga.academia.edu/paulprae]. You are also welcome to witness some of my\ngreatest conclusions and see links to my favorite articles of interest by\nrunning through my Twitter feed (follow me @praeducer\n[https://twitter.com/praeducer]). Collectively, this web presence not only shows\nyou who I am but also represents my abilities to execute on what I have learned\nregarding personal branding and digital marketing.\n\nAs was the intention of this essay, the digital content I have on display tells\nmy story. I have been working towards a degree in machine learning or predictive\nmarketing analytics for years now. I applied to UNCC’s Data Science and\nBusiness\nAnalytics program [http://analytics.uncc.edu] recently and plan on applying to\nothers soon. The need for data scientists is growing at an explosive pace for a\nreason—the world needs the specialist that I want to become. Program’s like\nthese will help me increase the positive impact I will make on existence. I am\nready to start this monumental step towards a better future for my family and\nfor society.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2014-10-09T16:38:50.000Z",
            "updated_at": "2021-03-09T16:49:21.000Z",
            "published_at": "2014-10-09T16:58:01.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e87",
            "uuid": "13976bb6-8346-4e39-8e15-f0536ac687f8",
            "title": "A Discussion on Data Science",
            "slug": "a-discussion-on-data-science",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"During a recent session of UNCC’s “Big Data Analytics for Competitive Advantage” we had a general discussion on the state of the field of data science. Here are some of my responses.\\n\\n#### 1. Based on your investigation, what is data science?\\nAs with all words, descriptive definitions arise from general usage. Much of what I have read and heard (some of which I shared on Twitter, https://twitter.com/praeducer) can be broken down into a few main points:\\n\\n+ Data science is an interdisciplinary field that brings together computer science, mathematics, and statistics to solve problems in data-generating domains.\\n+ It augments the decision-making, often with predictive analytics, of domain experts using data.\\n+ It is generally performed by teams, or rare individuals, that have a variety of skills and knowledge including: systems engineering, software engineering, database administration, machine learning, data visualization, and business analysis.\\n+ It can be performed with traditional tools such as spreadsheets (e.g. Excel) and relational database systems (e.g. SQL Server or MySQL) but is most often associated with modern scalable cloud computing systems such SAS and Hadoop. A more interactive exploration of the data in these systems is also common, using visualization tools (like SPSS Modeler and Tableau) or statistical programming languages (like R).\\n\\n#### 2. Is data science a science?\\nBeing a science across many disciplines, it is not just a “hard science” (like physics or chemistry) where a rigorous execution of the scientific method is necessary. It is also a humanity, where ethics and social impact must be considered. So, yes, it is at least a science. Work in data science should produce testable predictions with the highest degree of accuracy and objectivity possible. Even when integrating more heavily with social sciences (which can still undergo the rigor of any natural science), it should rely heavily on quantifiable data and mathematical models.\\n\\nIt could be described as a meta-science (i.e. science about science). While there may be some generalizations specific to the field of data science, many of the concepts and techniques associated with data science can be applied to any science. Research and application in this field could benefit any other field that can act on observable data. Skills, knowledge, and tools developed in data science are useful to, and shared across, many other sciences.\\n\\n#### 3. (a) What are your strengths as an aspiring data scientist?\\nMy background is primarily in software engineering and web systems administration. I will excel at any programming or scripting tasks particularly in object-oriented, procedural, or functional languages. My strongest languages are JavaScript, PHP, and bash scripting since I use them everyday at Red Ventures. Academically, I am also experienced in Java and C++ with a basic understanding of Python and Prolog. My understanding of networking and cloud computing is solid. I know how to manage Microsoft and Linux-based server clusters to support web applications (especially content management systems).\\n\\nThe problem domains I know best are digital marketing, the music business, and mental health. I currently work in the Digital Marketing division of Red Ventures, tackling tasks in natural search (SEO), paid search (SEM), and customer experience (web UI/UX optimization). Much of my career to date was spent in the music industry as an audio engineer (both in studio and for live sound) and as a production manager (organizing teams to produce entertaining events). My work at a mental health clinic and my academics in cognitive science also make me well suited for solving human resource problems involving happiness, purpose, and productivity.\\n\\n#### 3. (b) How would you demonstrate them to the prospective employer?\\nMy love for content marketing gives me a great place to start: my web presence. To visualize my skills, I have code up on GitHub (https://github.com/praeducer) and a portfolio available on Behance (https://www.behance.net/paulprae). To visualize my knowledge, they can read my academic papers at Academia.edu (https://uga.academia.edu/PaulPrae) or my professional blog (http://blog.paulprae.com/).\\n\\nFor a live demonstration, I am comfortable scripting out data processing solutions in languages I am familiar with at anytime. I could also architect a basic infrastructure for managing their data and even collecting it if it is web-based data. If the problems we were solving were in marketing, I could layout the foundation for a digital marketing strategy and emphasize any solutions particularly beneficial to them. If we were in the human resource domain, I could ask the right questions to see how they could boost morale or productivity. If the company was unsure of the emotional state of their workforce, I could develop a strategy and a system for surveying their colleagues.\\n\\n#### 4. Choose a company to analyze:\\nRed Ventures is a data-driven company utilizing many proprietary sales and marketing technologies. We are always forward-thinking, looking to aggressively fill any skill gaps and exploit any existing strengths as fast as we can. Our data science team only emerged recently. They are currently focusing on learning and exploration.\\n\\n#### 4. (a) List the data skills you think are needed:\\nIt is a combination of gaining skills and applying skills to reach certain end results that may be needed. They are potentially missing or acquiring:\\n\\n+ A dedicated software engineer.\\n+ A central system for advanced predictive analytics.\\n+ A cohesive data management lifecycle. \\n+ Confidence and understanding of the best tools to get data science work done.\\n\\n#### 4. (b) List the skills the company has:\\nRed Ventures has incredible domain expertise in direct sales and digital marketing. Our business analysts tend to be fairly technical with a good grasp on the advanced features of Excel and enough SQL skills to make them dangerous. Our engineers are fantastic at building and supporting advanced web applications. Most of our software engineers are full-stack developers. Any engineer is capable of collecting, processing, storing, and presenting data from across our sales and marketing activities. Our IT operations staff is excellent at automating work and scaling out systems. We also have the foundation for enterprise-level business intelligence in place.\\n\\n#### 5. (a) How many data scientists are there in the US?\\nThis is a really hard question because there is no definitive, collectively agreed upon definition for a data scientist (e.g. like one who performs data science as I described it above). Besides this, there are many potentially synonymous titles or job roles that have heavily overlapping skill and knowledge requirements. Some titles in contention are analyst, statistician, and research scientist. We may be able to apply some natural language processing to job or career sites, like Indeed.com or LinkedIn.com, to figure these points out.\\n\\nIn general, calculating this number would take a few challenging steps. After strictly defining the role of a data scientist, we would have to come up with some measure for how closely related other job roles and their title are to this definition. Then we would accept other titles based on a certain amount of accuracy or closeness to our definition of a data scientist. Finally, we would have to find a database of the titles of people living in the US, say using LinkedIn. Assuming this is a sound sample of the US population, we could then take a count of all data scientist titles and increase it proportionately to the total population of the US.\\n\\nI looked for a study that attempted to run through a process like this. Some of the basic steps were performed by someone at NC State a few years ago for a few related titles (http://analytics.ncsu.edu/?page_id=4025). At that point in October of 2011, 394 results came back for the exact title “data scientist”. I repeated the same thing now, in January of 2015, and I am getting 14,171 results. If I limit my search to the United States, I get 7,872 results. That is a huge increase that could be influenced in part by the increased popularity of data science (so people are switching to that title for personal branding reasons) or from other things such as improved search relevancy by LinkedIn. Either way, there are still a lot of other titles that could potentially be included plus those individuals that are not publicly searchable on LinkedIn.\\n\\n#### 5. (b) How many job openings are there for data science?\\nThis has many of the same issues as the last question. There may be more data points to work with, though, since companies are aggressively recruiting people in this field (shown through the amount of recruiters that message me). It looks like about 2% of all job postings on Indeed.com are related to data science according to the first graph below. I searched for all jobs in the United States without any keywords and found 2,351,687 results. That could mean there are about 47,033 jobs related to data science available. Of course, a lot of this depends on many things like how Indeed.com performs these queries and on where they get this information.\\n\\n<div style=\\\"width:540px\\\">\\n<a href=\\\"http://www.indeed.com/jobtrends?q=data+science\\\" title=\\\"data science Job Trends\\\">\\n<img width=\\\"540\\\" height=\\\"300\\\" src=\\\"http://www.indeed.com/trendgraph/jobgraph.png?q=data+science\\\" border=\\\"0\\\" alt=\\\"data science Job Trends graph\\\">\\n</a>\\n<table width=\\\"100%\\\" cellpadding=\\\"6\\\" cellspacing=\\\"0\\\" border=\\\"0\\\" style=\\\"font-size:80%\\\"><tr>\\n<td><a href=\\\"http://www.indeed.com/jobtrends?q=data+science\\\">data science Job Trends</a></td>\\n<td align=\\\"right\\\"><a href=\\\"http://www.indeed.com/jobs?q=Data+Science\\\">Data Science jobs</a></td>\\n</tr></table>\\n</div>\\n<div style=\\\"width:540px\\\">\\n<a href=\\\"http://www.indeed.com/jobtrends?q=%22data+scientist%22\\\" title=\\\"&#034;data scientist&#034; Job Trends\\\">\\n<img width=\\\"540\\\" height=\\\"300\\\" src=\\\"http://www.indeed.com/trendgraph/jobgraph.png?q=%22data+scientist%22\\\" border=\\\"0\\\" alt=\\\"&#034;data scientist&#034; Job Trends graph\\\">\\n</a>\\n<table width=\\\"100%\\\" cellpadding=\\\"6\\\" cellspacing=\\\"0\\\" border=\\\"0\\\" style=\\\"font-size:80%\\\"><tr>\\n<td><a href=\\\"http://www.indeed.com/jobtrends?q=%22data+scientist%22\\\">&#034;data scientist&#034; Job Trends</a></td>\\n<td align=\\\"right\\\"><a href=\\\"http://www.indeed.com/jobs?q=%22data+Scientist%22\\\">&#034;data Scientist&#034; jobs</a></td>\\n</tr></table>\\n</div>\\n\\nHere are some other example query results:\\n\\n+ http://www.indeed.com/jobtrends/statistician.html\\n+ http://www.indeed.com/jobtrends/analyst.html \\n+ http://www.indeed.com/jobtrends/Hadoop.html \\n+ http://www.indeed.com/jobtrends/apache+pig.html \\n+ http://www.indeed.com/jobtrends/scala.html\\n+ http://www.indeed.com/jobtrends/apache+spark.html \\n\\n#### 5. (c) What are the trends?\\nAs you can see above, depending on my queries, the trends vary. The generic job titles appear to be slowly declining overall. I would guess this could be due to people hiring more specific skill sets or due to jobs actually getting filled. The trends tend to be up and to the right for specific big data technologies. Some of these results increase rapidly after 2010, then have a small drop around 2014, and then a spike upwards again. It is interesting that big data technologies and “data scientist” all share a similar curve. \\n\\nSome of the graphs follow an almost exponential curve. This could almost make sense since there are several analysts plotting an exponential curve in the growth of data (e.g. http://blog.thomsonreuters.com/index.php/big-data-graphic-of-the-day/, and page 2 of https://www.atkearney.com/documents/10192/698536/Big+Data+and+the+Creative+Destruction+of+Todays+Business+Models.pdf/f05aed38-6c26-431d-8500-d75a2c384919). Hopefully, good data science will prevent the need for data scientists to increase proportionately to the growth of data ;P\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>During a recent session of UNCC’s “Big Data Analytics for Competitive Advantage” we had a general discussion on the state of the field of data science. Here are some of my responses.</p>\n<h4 id=\"1basedonyourinvestigationwhatisdatascience\">1. Based on your investigation, what is data science?</h4>\n<p>As with all words, descriptive definitions arise from general usage. Much of what I have read and heard (some of which I shared on Twitter, <a href=\"https://twitter.com/praeducer\">https://twitter.com/praeducer</a>) can be broken down into a few main points:</p>\n<ul>\n<li>Data science is an interdisciplinary field that brings together computer science, mathematics, and statistics to solve problems in data-generating domains.</li>\n<li>It augments the decision-making, often with predictive analytics, of domain experts using data.</li>\n<li>It is generally performed by teams, or rare individuals, that have a variety of skills and knowledge including: systems engineering, software engineering, database administration, machine learning, data visualization, and business analysis.</li>\n<li>It can be performed with traditional tools such as spreadsheets (e.g. Excel) and relational database systems (e.g. SQL Server or MySQL) but is most often associated with modern scalable cloud computing systems such SAS and Hadoop. A more interactive exploration of the data in these systems is also common, using visualization tools (like SPSS Modeler and Tableau) or statistical programming languages (like R).</li>\n</ul>\n<h4 id=\"2isdatascienceascience\">2. Is data science a science?</h4>\n<p>Being a science across many disciplines, it is not just a “hard science” (like physics or chemistry) where a rigorous execution of the scientific method is necessary. It is also a humanity, where ethics and social impact must be considered. So, yes, it is at least a science. Work in data science should produce testable predictions with the highest degree of accuracy and objectivity possible. Even when integrating more heavily with social sciences (which can still undergo the rigor of any natural science), it should rely heavily on quantifiable data and mathematical models.</p>\n<p>It could be described as a meta-science (i.e. science about science). While there may be some generalizations specific to the field of data science, many of the concepts and techniques associated with data science can be applied to any science. Research and application in this field could benefit any other field that can act on observable data. Skills, knowledge, and tools developed in data science are useful to, and shared across, many other sciences.</p>\n<h4 id=\"3awhatareyourstrengthsasanaspiringdatascientist\">3. (a) What are your strengths as an aspiring data scientist?</h4>\n<p>My background is primarily in software engineering and web systems administration. I will excel at any programming or scripting tasks particularly in object-oriented, procedural, or functional languages. My strongest languages are JavaScript, PHP, and bash scripting since I use them everyday at Red Ventures. Academically, I am also experienced in Java and C++ with a basic understanding of Python and Prolog. My understanding of networking and cloud computing is solid. I know how to manage Microsoft and Linux-based server clusters to support web applications (especially content management systems).</p>\n<p>The problem domains I know best are digital marketing, the music business, and mental health. I currently work in the Digital Marketing division of Red Ventures, tackling tasks in natural search (SEO), paid search (SEM), and customer experience (web UI/UX optimization). Much of my career to date was spent in the music industry as an audio engineer (both in studio and for live sound) and as a production manager (organizing teams to produce entertaining events). My work at a mental health clinic and my academics in cognitive science also make me well suited for solving human resource problems involving happiness, purpose, and productivity.</p>\n<h4 id=\"3bhowwouldyoudemonstratethemtotheprospectiveemployer\">3. (b) How would you demonstrate them to the prospective employer?</h4>\n<p>My love for content marketing gives me a great place to start: my web presence. To visualize my skills, I have code up on GitHub (<a href=\"https://github.com/praeducer\">https://github.com/praeducer</a>) and a portfolio available on Behance (<a href=\"https://www.behance.net/paulprae\">https://www.behance.net/paulprae</a>). To visualize my knowledge, they can read my academic papers at Academia.edu (<a href=\"https://uga.academia.edu/PaulPrae\">https://uga.academia.edu/PaulPrae</a>) or my professional blog (<a href=\"http://blog.paulprae.com/\">http://blog.paulprae.com/</a>).</p>\n<p>For a live demonstration, I am comfortable scripting out data processing solutions in languages I am familiar with at anytime. I could also architect a basic infrastructure for managing their data and even collecting it if it is web-based data. If the problems we were solving were in marketing, I could layout the foundation for a digital marketing strategy and emphasize any solutions particularly beneficial to them. If we were in the human resource domain, I could ask the right questions to see how they could boost morale or productivity. If the company was unsure of the emotional state of their workforce, I could develop a strategy and a system for surveying their colleagues.</p>\n<h4 id=\"4chooseacompanytoanalyze\">4. Choose a company to analyze:</h4>\n<p>Red Ventures is a data-driven company utilizing many proprietary sales and marketing technologies. We are always forward-thinking, looking to aggressively fill any skill gaps and exploit any existing strengths as fast as we can. Our data science team only emerged recently. They are currently focusing on learning and exploration.</p>\n<h4 id=\"4alistthedataskillsyouthinkareneeded\">4. (a) List the data skills you think are needed:</h4>\n<p>It is a combination of gaining skills and applying skills to reach certain end results that may be needed. They are potentially missing or acquiring:</p>\n<ul>\n<li>A dedicated software engineer.</li>\n<li>A central system for advanced predictive analytics.</li>\n<li>A cohesive data management lifecycle.</li>\n<li>Confidence and understanding of the best tools to get data science work done.</li>\n</ul>\n<h4 id=\"4blisttheskillsthecompanyhas\">4. (b) List the skills the company has:</h4>\n<p>Red Ventures has incredible domain expertise in direct sales and digital marketing. Our business analysts tend to be fairly technical with a good grasp on the advanced features of Excel and enough SQL skills to make them dangerous. Our engineers are fantastic at building and supporting advanced web applications. Most of our software engineers are full-stack developers. Any engineer is capable of collecting, processing, storing, and presenting data from across our sales and marketing activities. Our IT operations staff is excellent at automating work and scaling out systems. We also have the foundation for enterprise-level business intelligence in place.</p>\n<h4 id=\"5ahowmanydatascientistsarethereintheus\">5. (a) How many data scientists are there in the US?</h4>\n<p>This is a really hard question because there is no definitive, collectively agreed upon definition for a data scientist (e.g. like one who performs data science as I described it above). Besides this, there are many potentially synonymous titles or job roles that have heavily overlapping skill and knowledge requirements. Some titles in contention are analyst, statistician, and research scientist. We may be able to apply some natural language processing to job or career sites, like Indeed.com or LinkedIn.com, to figure these points out.</p>\n<p>In general, calculating this number would take a few challenging steps. After strictly defining the role of a data scientist, we would have to come up with some measure for how closely related other job roles and their title are to this definition. Then we would accept other titles based on a certain amount of accuracy or closeness to our definition of a data scientist. Finally, we would have to find a database of the titles of people living in the US, say using LinkedIn. Assuming this is a sound sample of the US population, we could then take a count of all data scientist titles and increase it proportionately to the total population of the US.</p>\n<p>I looked for a study that attempted to run through a process like this. Some of the basic steps were performed by someone at NC State a few years ago for a few related titles (<a href=\"http://analytics.ncsu.edu/?page_id=4025\">http://analytics.ncsu.edu/?page_id=4025</a>). At that point in October of 2011, 394 results came back for the exact title “data scientist”. I repeated the same thing now, in January of 2015, and I am getting 14,171 results. If I limit my search to the United States, I get 7,872 results. That is a huge increase that could be influenced in part by the increased popularity of data science (so people are switching to that title for personal branding reasons) or from other things such as improved search relevancy by LinkedIn. Either way, there are still a lot of other titles that could potentially be included plus those individuals that are not publicly searchable on LinkedIn.</p>\n<h4 id=\"5bhowmanyjobopeningsaretherefordatascience\">5. (b) How many job openings are there for data science?</h4>\n<p>This has many of the same issues as the last question. There may be more data points to work with, though, since companies are aggressively recruiting people in this field (shown through the amount of recruiters that message me). It looks like about 2% of all job postings on Indeed.com are related to data science according to the first graph below. I searched for all jobs in the United States without any keywords and found 2,351,687 results. That could mean there are about 47,033 jobs related to data science available. Of course, a lot of this depends on many things like how Indeed.com performs these queries and on where they get this information.</p>\n<div style=\"width:540px\">\n<a href=\"http://www.indeed.com/jobtrends?q=data+science\" title=\"data science Job Trends\">\n<img width=\"540\" height=\"300\" src=\"http://www.indeed.com/trendgraph/jobgraph.png?q=data+science\" border=\"0\" alt=\"data science Job Trends graph\">\n</a>\n<table width=\"100%\" cellpadding=\"6\" cellspacing=\"0\" border=\"0\" style=\"font-size:80%\"><tr>\n<td><a href=\"http://www.indeed.com/jobtrends?q=data+science\">data science Job Trends</a></td>\n<td align=\"right\"><a href=\"http://www.indeed.com/jobs?q=Data+Science\">Data Science jobs</a></td>\n</tr></table>\n</div>\n<div style=\"width:540px\">\n<a href=\"http://www.indeed.com/jobtrends?q=%22data+scientist%22\" title=\"&#034;data scientist&#034; Job Trends\">\n<img width=\"540\" height=\"300\" src=\"http://www.indeed.com/trendgraph/jobgraph.png?q=%22data+scientist%22\" border=\"0\" alt=\"&#034;data scientist&#034; Job Trends graph\">\n</a>\n<table width=\"100%\" cellpadding=\"6\" cellspacing=\"0\" border=\"0\" style=\"font-size:80%\"><tr>\n<td><a href=\"http://www.indeed.com/jobtrends?q=%22data+scientist%22\">&#034;data scientist&#034; Job Trends</a></td>\n<td align=\"right\"><a href=\"http://www.indeed.com/jobs?q=%22data+Scientist%22\">&#034;data Scientist&#034; jobs</a></td>\n</tr></table>\n</div>\n<p>Here are some other example query results:</p>\n<ul>\n<li><a href=\"http://www.indeed.com/jobtrends/statistician.html\">http://www.indeed.com/jobtrends/statistician.html</a></li>\n<li><a href=\"http://www.indeed.com/jobtrends/analyst.html\">http://www.indeed.com/jobtrends/analyst.html</a></li>\n<li><a href=\"http://www.indeed.com/jobtrends/Hadoop.html\">http://www.indeed.com/jobtrends/Hadoop.html</a></li>\n<li><a href=\"http://www.indeed.com/jobtrends/apache+pig.html\">http://www.indeed.com/jobtrends/apache+pig.html</a></li>\n<li><a href=\"http://www.indeed.com/jobtrends/scala.html\">http://www.indeed.com/jobtrends/scala.html</a></li>\n<li><a href=\"http://www.indeed.com/jobtrends/apache+spark.html\">http://www.indeed.com/jobtrends/apache+spark.html</a></li>\n</ul>\n<h4 id=\"5cwhatarethetrends\">5. (c) What are the trends?</h4>\n<p>As you can see above, depending on my queries, the trends vary. The generic job titles appear to be slowly declining overall. I would guess this could be due to people hiring more specific skill sets or due to jobs actually getting filled. The trends tend to be up and to the right for specific big data technologies. Some of these results increase rapidly after 2010, then have a small drop around 2014, and then a spike upwards again. It is interesting that big data technologies and “data scientist” all share a similar curve.</p>\n<p>Some of the graphs follow an almost exponential curve. This could almost make sense since there are several analysts plotting an exponential curve in the growth of data (e.g. <a href=\"http://blog.thomsonreuters.com/index.php/big-data-graphic-of-the-day/\">http://blog.thomsonreuters.com/index.php/big-data-graphic-of-the-day/</a>, and page 2 of <a href=\"https://www.atkearney.com/documents/10192/698536/Big+Data+and+the+Creative+Destruction+of+Todays+Business+Models.pdf/f05aed38-6c26-431d-8500-d75a2c384919\">https://www.atkearney.com/documents/10192/698536/Big+Data+and+the+Creative+Destruction+of+Todays+Business+Models.pdf/f05aed38-6c26-431d-8500-d75a2c384919</a>). Hopefully, good data science will prevent the need for data scientists to increase proportionately to the growth of data ;P</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "15",
            "plaintext": "During a recent session of UNCC’s “Big Data Analytics for Competitive Advantage”\nwe had a general discussion on the state of the field of data science. Here are\nsome of my responses.\n\n1. Based on your investigation, what is data science?\nAs with all words, descriptive definitions arise from general usage. Much of\nwhat I have read and heard (some of which I shared on Twitter, \nhttps://twitter.com/praeducer) can be broken down into a few main points:\n\n * Data science is an interdisciplinary field that brings together computer\n   science, mathematics, and statistics to solve problems in data-generating\n   domains.\n * It augments the decision-making, often with predictive analytics, of domain\n   experts using data.\n * It is generally performed by teams, or rare individuals, that have a variety\n   of skills and knowledge including: systems engineering, software engineering,\n   database administration, machine learning, data visualization, and business\n   analysis.\n * It can be performed with traditional tools such as spreadsheets (e.g. Excel)\n   and relational database systems (e.g. SQL Server or MySQL) but is most often\n   associated with modern scalable cloud computing systems such SAS and Hadoop.\n   A more interactive exploration of the data in these systems is also common,\n   using visualization tools (like SPSS Modeler and Tableau) or statistical\n   programming languages (like R).\n\n2. Is data science a science?\nBeing a science across many disciplines, it is not just a “hard science” (like\nphysics or chemistry) where a rigorous execution of the scientific method is\nnecessary. It is also a humanity, where ethics and social impact must be\nconsidered. So, yes, it is at least a science. Work in data science should\nproduce testable predictions with the highest degree of accuracy and objectivity\npossible. Even when integrating more heavily with social sciences (which can\nstill undergo the rigor of any natural science), it should rely heavily on\nquantifiable data and mathematical models.\n\nIt could be described as a meta-science (i.e. science about science). While\nthere may be some generalizations specific to the field of data science, many of\nthe concepts and techniques associated with data science can be applied to any\nscience. Research and application in this field could benefit any other field\nthat can act on observable data. Skills, knowledge, and tools developed in data\nscience are useful to, and shared across, many other sciences.\n\n3. (a) What are your strengths as an aspiring data scientist?\nMy background is primarily in software engineering and web systems\nadministration. I will excel at any programming or scripting tasks particularly\nin object-oriented, procedural, or functional languages. My strongest languages\nare JavaScript, PHP, and bash scripting since I use them everyday at Red\nVentures. Academically, I am also experienced in Java and C++ with a basic\nunderstanding of Python and Prolog. My understanding of networking and cloud\ncomputing is solid. I know how to manage Microsoft and Linux-based server\nclusters to support web applications (especially content management systems).\n\nThe problem domains I know best are digital marketing, the music business, and\nmental health. I currently work in the Digital Marketing division of Red\nVentures, tackling tasks in natural search (SEO), paid search (SEM), and\ncustomer experience (web UI/UX optimization). Much of my career to date was\nspent in the music industry as an audio engineer (both in studio and for live\nsound) and as a production manager (organizing teams to produce entertaining\nevents). My work at a mental health clinic and my academics in cognitive science\nalso make me well suited for solving human resource problems involving\nhappiness, purpose, and productivity.\n\n3. (b) How would you demonstrate them to the prospective employer?\nMy love for content marketing gives me a great place to start: my web presence.\nTo visualize my skills, I have code up on GitHub (https://github.com/praeducer)\nand a portfolio available on Behance (https://www.behance.net/paulprae). To\nvisualize my knowledge, they can read my academic papers at Academia.edu (\nhttps://uga.academia.edu/PaulPrae) or my professional blog (\nhttp://blog.paulprae.com/).\n\nFor a live demonstration, I am comfortable scripting out data processing\nsolutions in languages I am familiar with at anytime. I could also architect a\nbasic infrastructure for managing their data and even collecting it if it is\nweb-based data. If the problems we were solving were in marketing, I could\nlayout the foundation for a digital marketing strategy and emphasize any\nsolutions particularly beneficial to them. If we were in the human resource\ndomain, I could ask the right questions to see how they could boost morale or\nproductivity. If the company was unsure of the emotional state of their\nworkforce, I could develop a strategy and a system for surveying their\ncolleagues.\n\n4. Choose a company to analyze:\nRed Ventures is a data-driven company utilizing many proprietary sales and\nmarketing technologies. We are always forward-thinking, looking to aggressively\nfill any skill gaps and exploit any existing strengths as fast as we can. Our\ndata science team only emerged recently. They are currently focusing on learning\nand exploration.\n\n4. (a) List the data skills you think are needed:\nIt is a combination of gaining skills and applying skills to reach certain end\nresults that may be needed. They are potentially missing or acquiring:\n\n * A dedicated software engineer.\n * A central system for advanced predictive analytics.\n * A cohesive data management lifecycle.\n * Confidence and understanding of the best tools to get data science work done.\n\n4. (b) List the skills the company has:\nRed Ventures has incredible domain expertise in direct sales and digital\nmarketing. Our business analysts tend to be fairly technical with a good grasp\non the advanced features of Excel and enough SQL skills to make them dangerous.\nOur engineers are fantastic at building and supporting advanced web\napplications. Most of our software engineers are full-stack developers. Any\nengineer is capable of collecting, processing, storing, and presenting data from\nacross our sales and marketing activities. Our IT operations staff is excellent\nat automating work and scaling out systems. We also have the foundation for\nenterprise-level business intelligence in place.\n\n5. (a) How many data scientists are there in the US?\nThis is a really hard question because there is no definitive, collectively\nagreed upon definition for a data scientist (e.g. like one who performs data\nscience as I described it above). Besides this, there are many potentially\nsynonymous titles or job roles that have heavily overlapping skill and knowledge\nrequirements. Some titles in contention are analyst, statistician, and research\nscientist. We may be able to apply some natural language processing to job or\ncareer sites, like Indeed.com or LinkedIn.com, to figure these points out.\n\nIn general, calculating this number would take a few challenging steps. After\nstrictly defining the role of a data scientist, we would have to come up with\nsome measure for how closely related other job roles and their title are to this\ndefinition. Then we would accept other titles based on a certain amount of\naccuracy or closeness to our definition of a data scientist. Finally, we would\nhave to find a database of the titles of people living in the US, say using\nLinkedIn. Assuming this is a sound sample of the US population, we could then\ntake a count of all data scientist titles and increase it proportionately to the\ntotal population of the US.\n\nI looked for a study that attempted to run through a process like this. Some of\nthe basic steps were performed by someone at NC State a few years ago for a few\nrelated titles (http://analytics.ncsu.edu/?page_id=4025). At that point in\nOctober of 2011, 394 results came back for the exact title “data scientist”. I\nrepeated the same thing now, in January of 2015, and I am getting 14,171\nresults. If I limit my search to the United States, I get 7,872 results. That is\na huge increase that could be influenced in part by the increased popularity of\ndata science (so people are switching to that title for personal branding\nreasons) or from other things such as improved search relevancy by LinkedIn.\nEither way, there are still a lot of other titles that could potentially be\nincluded plus those individuals that are not publicly searchable on LinkedIn.\n\n5. (b) How many job openings are there for data science?\nThis has many of the same issues as the last question. There may be more data\npoints to work with, though, since companies are aggressively recruiting people\nin this field (shown through the amount of recruiters that message me). It looks\nlike about 2% of all job postings on Indeed.com are related to data science\naccording to the first graph below. I searched for all jobs in the United States\nwithout any keywords and found 2,351,687 results. That could mean there are\nabout 47,033 jobs related to data science available. Of course, a lot of this\ndepends on many things like how Indeed.com performs these queries and on where\nthey get this information.\n\n[http://www.indeed.com/jobtrends?q=data+science]data science Job Trends\n[http://www.indeed.com/jobtrends?q=data+science]Data Science jobs\n[http://www.indeed.com/jobs?q=Data+Science] \n[http://www.indeed.com/jobtrends?q=%22data+scientist%22]\"data scientist\" Job\nTrends [http://www.indeed.com/jobtrends?q=%22data+scientist%22]\"data Scientist\"\njobs [http://www.indeed.com/jobs?q=%22data+Scientist%22]Here are some other\nexample query results:\n\n * http://www.indeed.com/jobtrends/statistician.html\n * http://www.indeed.com/jobtrends/analyst.html\n * http://www.indeed.com/jobtrends/Hadoop.html\n * http://www.indeed.com/jobtrends/apache+pig.html\n * http://www.indeed.com/jobtrends/scala.html\n * http://www.indeed.com/jobtrends/apache+spark.html\n\n5. (c) What are the trends?\nAs you can see above, depending on my queries, the trends vary. The generic job\ntitles appear to be slowly declining overall. I would guess this could be due to\npeople hiring more specific skill sets or due to jobs actually getting filled.\nThe trends tend to be up and to the right for specific big data technologies.\nSome of these results increase rapidly after 2010, then have a small drop around\n2014, and then a spike upwards again. It is interesting that big data\ntechnologies and “data scientist” all share a similar curve.\n\nSome of the graphs follow an almost exponential curve. This could almost make\nsense since there are several analysts plotting an exponential curve in the\ngrowth of data (e.g. \nhttp://blog.thomsonreuters.com/index.php/big-data-graphic-of-the-day/, and page\n2 of \nhttps://www.atkearney.com/documents/10192/698536/Big+Data+and+the+Creative+Destruction+of+Todays+Business+Models.pdf/f05aed38-6c26-431d-8500-d75a2c384919\n). Hopefully, good data science will prevent the need for data scientists to\nincrease proportionately to the growth of data ;P",
            "feature_image": null,
            "featured": 1,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2015-01-26T00:19:54.000Z",
            "updated_at": "2021-03-09T16:48:59.000Z",
            "published_at": "2015-01-26T00:33:38.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e88",
            "uuid": "2a37de03-60a9-4a62-b31f-cebec247aa0f",
            "title": "The Delusions vs the Effectiveness of Big Data",
            "slug": "the-delusions-vs-the-effectiveness-of-big-data",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Recently, I came across two articles. One argued how the value of big data is overhyped. The other showed how you can exploit large amounts of data to make amazing discoveries. I found their contrasting viewpoints interesting. Below are some highlights from each.\\n\\n#### Summary of “The Delusions of Big Data”\\nOriginal article: http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts \\n\\n+ If you make many hypotheses using exponentially complex data without ensuring that statistical rigor is maintained, some of your inferences will likely be false. A database may have thousands of people in it. Each person may have millions of features. If you start looking at the exponential combinations of these features, there is bound to be a spurious combination that falsely predicts what you are looking for.\\n+ To prevent acting on bad predictions, we must understand the quality of our inferences. Errors must be quantified; error bars and rates are necessary. We cannot simply explore the data and make decisions without considering the quality of the experiment (noisy data, sampling patterns, heterogeneity etc).\\n+ Data science involves hard engineering and mathematics. It will take time to get things right. There is too much hype about what can be gained from big data. We will make steady progress over decades but no major leap in understanding will happen quickly.\\n\\n#### Summary of “The Unreasonable Effectiveness of Data”\\nOriginal article: http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35179.pdf \\n\\n+ There are many more unstructured, unlabeled data resources than there are structured, labeled resources. Take the time to develop intelligent, unsupervised learning that exploits big data rather than using smaller, curated corpora.\\n+ Due to the large complexity of data and its relationships, use nonparametric models to represent your data. This will maintain a high resolution of the details of your data. It will allow the model to expand proportionately with the data’s complexity and size.\\n+ When processing natural language, make use of the data’s context to find established concepts. There are already many existing relationships to help you understand how data should be labeled and categorized. Use what is there instead of inventing new methods or concepts\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>Recently, I came across two articles. One argued how the value of big data is overhyped. The other showed how you can exploit large amounts of data to make amazing discoveries. I found their contrasting viewpoints interesting. Below are some highlights from each.</p>\n<h4 id=\"summaryofthedelusionsofbigdata\">Summary of “The Delusions of Big Data”</h4>\n<p>Original article: <a href=\"http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts\">http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts</a></p>\n<ul>\n<li>If you make many hypotheses using exponentially complex data without ensuring that statistical rigor is maintained, some of your inferences will likely be false. A database may have thousands of people in it. Each person may have millions of features. If you start looking at the exponential combinations of these features, there is bound to be a spurious combination that falsely predicts what you are looking for.</li>\n<li>To prevent acting on bad predictions, we must understand the quality of our inferences. Errors must be quantified; error bars and rates are necessary. We cannot simply explore the data and make decisions without considering the quality of the experiment (noisy data, sampling patterns, heterogeneity etc).</li>\n<li>Data science involves hard engineering and mathematics. It will take time to get things right. There is too much hype about what can be gained from big data. We will make steady progress over decades but no major leap in understanding will happen quickly.</li>\n</ul>\n<h4 id=\"summaryoftheunreasonableeffectivenessofdata\">Summary of “The Unreasonable Effectiveness of Data”</h4>\n<p>Original article: <a href=\"http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35179.pdf\">http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35179.pdf</a></p>\n<ul>\n<li>There are many more unstructured, unlabeled data resources than there are structured, labeled resources. Take the time to develop intelligent, unsupervised learning that exploits big data rather than using smaller, curated corpora.</li>\n<li>Due to the large complexity of data and its relationships, use nonparametric models to represent your data. This will maintain a high resolution of the details of your data. It will allow the model to expand proportionately with the data’s complexity and size.</li>\n<li>When processing natural language, make use of the data’s context to find established concepts. There are already many existing relationships to help you understand how data should be labeled and categorized. Use what is there instead of inventing new methods or concepts</li>\n</ul>\n<!--kg-card-end: markdown-->",
            "comment_id": "16",
            "plaintext": "Recently, I came across two articles. One argued how the value of big data is\noverhyped. The other showed how you can exploit large amounts of data to make\namazing discoveries. I found their contrasting viewpoints interesting. Below are\nsome highlights from each.\n\nSummary of “The Delusions of Big Data”\nOriginal article: \nhttp://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts\n\n * If you make many hypotheses using exponentially complex data without ensuring\n   that statistical rigor is maintained, some of your inferences will likely be\n   false. A database may have thousands of people in it. Each person may have\n   millions of features. If you start looking at the exponential combinations of\n   these features, there is bound to be a spurious combination that falsely\n   predicts what you are looking for.\n * To prevent acting on bad predictions, we must understand the quality of our\n   inferences. Errors must be quantified; error bars and rates are necessary. We\n   cannot simply explore the data and make decisions without considering the\n   quality of the experiment (noisy data, sampling patterns, heterogeneity etc).\n * Data science involves hard engineering and mathematics. It will take time to\n   get things right. There is too much hype about what can be gained from big\n   data. We will make steady progress over decades but no major leap in\n   understanding will happen quickly.\n\nSummary of “The Unreasonable Effectiveness of Data”\nOriginal article: \nhttp://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35179.pdf\n\n * There are many more unstructured, unlabeled data resources than there are\n   structured, labeled resources. Take the time to develop intelligent,\n   unsupervised learning that exploits big data rather than using smaller,\n   curated corpora.\n * Due to the large complexity of data and its relationships, use nonparametric\n   models to represent your data. This will maintain a high resolution of the\n   details of your data. It will allow the model to expand proportionately with\n   the data’s complexity and size.\n * When processing natural language, make use of the data’s context to find\n   established concepts. There are already many existing relationships to help\n   you understand how data should be labeled and categorized. Use what is there\n   instead of inventing new methods or concepts",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2015-01-26T11:26:41.000Z",
            "updated_at": "2021-03-09T16:46:44.000Z",
            "published_at": "2015-01-26T11:30:04.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e89",
            "uuid": "0a34b1ca-aac8-4001-8953-90140986eecc",
            "title": "My Annual Development Plan",
            "slug": "my-annual-development-plan",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We recently wrapped up our 360 reviews at Red Ventures. Part of this process involved assessing ourselves and deciding where we want to take our careers. The “Annual Development” questionaire started off asking me how I felt about my current job role as a software engineer on the digital marketing team (you can learn more about this role on my LinkedIn profile, https://www.linkedin.com/in/paulprae). Here is the option I chose, followed by my other responses:\\n\\n> “If the right opportunity is open now, I am ready for a move, but would also be satisfied remaining in my current role for the near-term future.”\\n\\n###### If none of the statements above accurately describe your feelings, please describe your thoughts here:\\nI am happy on my current team but would prefer using machine learning techniques and technologies full-time. I especially enjoy analyzing social data using powerful data science tools, such as Apache Spark or Mahout, and robust languages like Java/Scala or C++/F#. Second to that, staying in my current role with a focus on new product development with modern technologies would be fantastic. Working to design and build full-stack web applications, with technologies like Node.js, Go, elasticsearch, RabbitMQ, AWS etc, would make me feel more fulfilled.\\n\\n###### What opportunities or roles are you interested in exploring over the next 1-2 years?\\nThe only potential teams that could compete with my current team would be the “Data Science” team or the “Business Intelligence” team. I would fit well as a software engineer on either team. No matter the team, any work in machine learning or data science makes complete sense for me based on my background and career interests. I focused on artificial intelligence during my undergraduate pursuits and I am currently pursuing a graduate degree in data science. In my current job role, I am strengthening the necessary skills to build web services and web applications. Within the next couple of years, I hope to be building out the infrastructure to support machine learning capabilities or creating services to provide machine learning capabilities.\\n\\n###### If you could chart your ideal career path over the next 2-4 years, what would it look like?\\nI ultimately desire to write algorithms and web services for machine learning libraries that are catered to specific social data problems. Specifically, I hope to apply biologically-inspired machine learning techniques, such as evolutionary computation or neural networks, to social media marketing or human resource problems. I want to improve the brand experiences of customers and augment the cognitive abilities of employees. My career is designed to lead into cognitive computing. I want to build adaptive and predictive software systems that help people solve complex social problems.\\n\\n---------------------------------------\\n\\nI think about these topics all of the time. They were fun to write about and share with leadership. I’m excited to see where I can take my career!\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>We recently wrapped up our 360 reviews at Red Ventures. Part of this process involved assessing ourselves and deciding where we want to take our careers. The “Annual Development” questionaire started off asking me how I felt about my current job role as a software engineer on the digital marketing team (you can learn more about this role on my LinkedIn profile, <a href=\"https://www.linkedin.com/in/paulprae\">https://www.linkedin.com/in/paulprae</a>). Here is the option I chose, followed by my other responses:</p>\n<blockquote>\n<p>“If the right opportunity is open now, I am ready for a move, but would also be satisfied remaining in my current role for the near-term future.”</p>\n</blockquote>\n<h6 id=\"ifnoneofthestatementsaboveaccuratelydescribeyourfeelingspleasedescribeyourthoughtshere\">If none of the statements above accurately describe your feelings, please describe your thoughts here:</h6>\n<p>I am happy on my current team but would prefer using machine learning techniques and technologies full-time. I especially enjoy analyzing social data using powerful data science tools, such as Apache Spark or Mahout, and robust languages like Java/Scala or C++/F#. Second to that, staying in my current role with a focus on new product development with modern technologies would be fantastic. Working to design and build full-stack web applications, with technologies like Node.js, Go, elasticsearch, RabbitMQ, AWS etc, would make me feel more fulfilled.</p>\n<h6 id=\"whatopportunitiesorrolesareyouinterestedinexploringoverthenext12years\">What opportunities or roles are you interested in exploring over the next 1-2 years?</h6>\n<p>The only potential teams that could compete with my current team would be the “Data Science” team or the “Business Intelligence” team. I would fit well as a software engineer on either team. No matter the team, any work in machine learning or data science makes complete sense for me based on my background and career interests. I focused on artificial intelligence during my undergraduate pursuits and I am currently pursuing a graduate degree in data science. In my current job role, I am strengthening the necessary skills to build web services and web applications. Within the next couple of years, I hope to be building out the infrastructure to support machine learning capabilities or creating services to provide machine learning capabilities.</p>\n<h6 id=\"ifyoucouldchartyouridealcareerpathoverthenext24yearswhatwoulditlooklike\">If you could chart your ideal career path over the next 2-4 years, what would it look like?</h6>\n<p>I ultimately desire to write algorithms and web services for machine learning libraries that are catered to specific social data problems. Specifically, I hope to apply biologically-inspired machine learning techniques, such as evolutionary computation or neural networks, to social media marketing or human resource problems. I want to improve the brand experiences of customers and augment the cognitive abilities of employees. My career is designed to lead into cognitive computing. I want to build adaptive and predictive software systems that help people solve complex social problems.</p>\n<hr>\n<p>I think about these topics all of the time. They were fun to write about and share with leadership. I’m excited to see where I can take my career!</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "17",
            "plaintext": "We recently wrapped up our 360 reviews at Red Ventures. Part of this process\ninvolved assessing ourselves and deciding where we want to take our careers. The\n“Annual Development” questionaire started off asking me how I felt about my\ncurrent job role as a software engineer on the digital marketing team (you can\nlearn more about this role on my LinkedIn profile, \nhttps://www.linkedin.com/in/paulprae). Here is the option I chose, followed by\nmy other responses:\n\n> “If the right opportunity is open now, I am ready for a move, but would also be\nsatisfied remaining in my current role for the near-term future.”\n\n\nIf none of the statements above accurately describe your feelings, please\ndescribe your thoughts here:\nI am happy on my current team but would prefer using machine learning techniques\nand technologies full-time. I especially enjoy analyzing social data using\npowerful data science tools, such as Apache Spark or Mahout, and robust\nlanguages like Java/Scala or C++/F#. Second to that, staying in my current role\nwith a focus on new product development with modern technologies would be\nfantastic. Working to design and build full-stack web applications, with\ntechnologies like Node.js, Go, elasticsearch, RabbitMQ, AWS etc, would make me\nfeel more fulfilled.\n\nWhat opportunities or roles are you interested in exploring over the next 1-2\nyears?\nThe only potential teams that could compete with my current team would be the\n“Data Science” team or the “Business Intelligence” team. I would fit well as a\nsoftware engineer on either team. No matter the team, any work in machine\nlearning or data science makes complete sense for me based on my background and\ncareer interests. I focused on artificial intelligence during my undergraduate\npursuits and I am currently pursuing a graduate degree in data science. In my\ncurrent job role, I am strengthening the necessary skills to build web services\nand web applications. Within the next couple of years, I hope to be building out\nthe infrastructure to support machine learning capabilities or creating services\nto provide machine learning capabilities.\n\nIf you could chart your ideal career path over the next 2-4 years, what would it\nlook like?\nI ultimately desire to write algorithms and web services for machine learning\nlibraries that are catered to specific social data problems. Specifically, I\nhope to apply biologically-inspired machine learning techniques, such as\nevolutionary computation or neural networks, to social media marketing or human\nresource problems. I want to improve the brand experiences of customers and\naugment the cognitive abilities of employees. My career is designed to lead into\ncognitive computing. I want to build adaptive and predictive software systems\nthat help people solve complex social problems.\n\n\n--------------------------------------------------------------------------------\n\nI think about these topics all of the time. They were fun to write about and\nshare with leadership. I’m excited to see where I can take my career!",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2015-02-08T02:14:44.000Z",
            "updated_at": "2021-03-09T16:46:21.000Z",
            "published_at": "2015-02-08T02:17:41.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e8a",
            "uuid": "cbf87337-405a-4227-95c7-7e788cc5c5ae",
            "title": "My Statement of Purpose",
            "slug": "my-statement-of-purpose",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently applied to Georgia Tech's [Online Master of Science in Computer Science](http://www.omscs.gatech.edu/) and was asked to write a “Statement of Purpose” as part of the process. It allowed me to express my academic and career intentions for the next few years. I think about my mission in life constantly. My purpose is a living, breathing idea. It was a valuable exercise outside of the application process so I thought I’d share the essay. Let me know what you think!\\n\\n**Statement of Purpose:** *Please give a Statement of Purpose detailing your academic and research goals as well as career plans. Include your reasons for choosing the College of Computing as opposed to other programs and/or other universities.*\\n\\nCognitive computing gives people super powers. Augmenting our intelligence with machine intelligence gives us the ability to predict the future and make the best decisions possible. As the field of machine intelligence progresses, more and more super-human abilities become available to us. I am out to bring these super powers to the masses through cognitive computing products and services. Attending Georgia Tech would be an excellent catalyst to help me achieve this.\\n\\nI currently architect and develop web services and full-stack applications. My goal is to start working full-time on interactive applications that exhibit human-like intelligence. One way to make these applications intelligent is to have them consume machine learning and cognitive computing web services. These services could be custom built by me or provided by SaaS APIs like [Microsoft Cognitive Services](https://www.microsoft.com/cognitive-services) or [IBM Watson Services](http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/services-catalog.html). To apply the best cognitive services to a given problem, I need a deep understanding of their underlying algorithms and techniques. I also need to learn how to architect applications and services for scale.\\n\\nThe front-end development skills I have learned so far are targeted at building typical web app and native app user interfaces. Moving forward, I plan to focus more on natural language and brain-computer interfaces. With the power of machine learning, natural language processing, and data, I could create user experiences that are contextually, socially, and even emotionally-aware.\\n\\nGeorgia Tech's Online Master of Science in Computer Science provides courses that would teach me how to best integrate machine intelligence into my applications. The [Knowledge-Based Artificial Intelligence](http://www.omscs.gatech.edu/cs-7637-knowledge-based-artificial-intelligence-cognitive-systems) course is a perfect overview of how to create cognitive systems. The course in [Machine Learning](http://www.omscs.gatech.edu/cs-7641-machine-learning) would teach me how to apply the right algorithms and models to the right problems. The multiple courses on Algorithms, [Complexity](http://www.omscs.gatech.edu/cs-6505-computability-algorithms), and [High-Performance Computing](http://www.omscs.gatech.edu/cs-6290-high-performance-computer-architecture) would teach me how to optimize and scale my applications. Each of these courses covers an essential component to implementing intelligent web services in production applications.\\n\\nThe [Specialization in Interactive Intelligence](http://www.omscs.gatech.edu/specialization-interactive-intelligence) completely aligns with my desire to become a cognitive computing expert. There are several courses in this specialty that would help me create user experiences that exhibit human-level intelligence. The course on Natural Language would help me incorporate natural language understanding features into my applications. The special topic in Computational Creativity would help my applications be more expressive and adaptive. When I first saw this specialty, I immediately knew it was a great fit.\\n\\nThe Georgia Tech Online Masters in Computer Science is my first choice for many reasons. Besides the awesome courses that are available at Georgia Tech, there is a ton of [groundbreaking research](http://www.research.gatech.edu/) being performed by world renowned professors there. I would love to work with Professor Charles Isbell and the rest of the pfunk research group in [The Laboratory for Interactive Artificial Intelligence](http://www.cc.gatech.edu/~isbell/iai/). Their research spans many areas I am interested in including interactive entertainment, deep learning, and computational neuroscience. In addition to these points, the program hit the top of my list because it is affordable, online, and produced by one of the world’s best ranking universities.\\n\\nDriven by the many challenging problems society is facing, I want to use my skills and knowledge to help people live amazing lives. I strive to make the most positive impact on existence as possible. I plan to use machine learning and natural language processing to improve the health, performance, and productivity of people. Leveraging data, I want to create software to help people live the most impactful lives possible. I want to bring unprecedented capabilities to people by extending their minds into cognitive computing systems. This graduate program would be a core part of my strategy to meet these goals.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>I recently applied to Georgia Tech's <a href=\"http://www.omscs.gatech.edu/\">Online Master of Science in Computer Science</a> and was asked to write a “Statement of Purpose” as part of the process. It allowed me to express my academic and career intentions for the next few years. I think about my mission in life constantly. My purpose is a living, breathing idea. It was a valuable exercise outside of the application process so I thought I’d share the essay. Let me know what you think!</p>\n<p><strong>Statement of Purpose:</strong> <em>Please give a Statement of Purpose detailing your academic and research goals as well as career plans. Include your reasons for choosing the College of Computing as opposed to other programs and/or other universities.</em></p>\n<p>Cognitive computing gives people super powers. Augmenting our intelligence with machine intelligence gives us the ability to predict the future and make the best decisions possible. As the field of machine intelligence progresses, more and more super-human abilities become available to us. I am out to bring these super powers to the masses through cognitive computing products and services. Attending Georgia Tech would be an excellent catalyst to help me achieve this.</p>\n<p>I currently architect and develop web services and full-stack applications. My goal is to start working full-time on interactive applications that exhibit human-like intelligence. One way to make these applications intelligent is to have them consume machine learning and cognitive computing web services. These services could be custom built by me or provided by SaaS APIs like <a href=\"https://www.microsoft.com/cognitive-services\">Microsoft Cognitive Services</a> or <a href=\"http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/services-catalog.html\">IBM Watson Services</a>. To apply the best cognitive services to a given problem, I need a deep understanding of their underlying algorithms and techniques. I also need to learn how to architect applications and services for scale.</p>\n<p>The front-end development skills I have learned so far are targeted at building typical web app and native app user interfaces. Moving forward, I plan to focus more on natural language and brain-computer interfaces. With the power of machine learning, natural language processing, and data, I could create user experiences that are contextually, socially, and even emotionally-aware.</p>\n<p>Georgia Tech's Online Master of Science in Computer Science provides courses that would teach me how to best integrate machine intelligence into my applications. The <a href=\"http://www.omscs.gatech.edu/cs-7637-knowledge-based-artificial-intelligence-cognitive-systems\">Knowledge-Based Artificial Intelligence</a> course is a perfect overview of how to create cognitive systems. The course in <a href=\"http://www.omscs.gatech.edu/cs-7641-machine-learning\">Machine Learning</a> would teach me how to apply the right algorithms and models to the right problems. The multiple courses on Algorithms, <a href=\"http://www.omscs.gatech.edu/cs-6505-computability-algorithms\">Complexity</a>, and <a href=\"http://www.omscs.gatech.edu/cs-6290-high-performance-computer-architecture\">High-Performance Computing</a> would teach me how to optimize and scale my applications. Each of these courses covers an essential component to implementing intelligent web services in production applications.</p>\n<p>The <a href=\"http://www.omscs.gatech.edu/specialization-interactive-intelligence\">Specialization in Interactive Intelligence</a> completely aligns with my desire to become a cognitive computing expert. There are several courses in this specialty that would help me create user experiences that exhibit human-level intelligence. The course on Natural Language would help me incorporate natural language understanding features into my applications. The special topic in Computational Creativity would help my applications be more expressive and adaptive. When I first saw this specialty, I immediately knew it was a great fit.</p>\n<p>The Georgia Tech Online Masters in Computer Science is my first choice for many reasons. Besides the awesome courses that are available at Georgia Tech, there is a ton of <a href=\"http://www.research.gatech.edu/\">groundbreaking research</a> being performed by world renowned professors there. I would love to work with Professor Charles Isbell and the rest of the pfunk research group in <a href=\"http://www.cc.gatech.edu/~isbell/iai/\">The Laboratory for Interactive Artificial Intelligence</a>. Their research spans many areas I am interested in including interactive entertainment, deep learning, and computational neuroscience. In addition to these points, the program hit the top of my list because it is affordable, online, and produced by one of the world’s best ranking universities.</p>\n<p>Driven by the many challenging problems society is facing, I want to use my skills and knowledge to help people live amazing lives. I strive to make the most positive impact on existence as possible. I plan to use machine learning and natural language processing to improve the health, performance, and productivity of people. Leveraging data, I want to create software to help people live the most impactful lives possible. I want to bring unprecedented capabilities to people by extending their minds into cognitive computing systems. This graduate program would be a core part of my strategy to meet these goals.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "19",
            "plaintext": "I recently applied to Georgia Tech's Online Master of Science in Computer\nScience [http://www.omscs.gatech.edu/] and was asked to write a “Statement of\nPurpose” as part of the process. It allowed me to express my academic and career\nintentions for the next few years. I think about my mission in life constantly.\nMy purpose is a living, breathing idea. It was a valuable exercise outside of\nthe application process so I thought I’d share the essay. Let me know what you\nthink!\n\nStatement of Purpose: Please give a Statement of Purpose detailing your academic\nand research goals as well as career plans. Include your reasons for choosing\nthe College of Computing as opposed to other programs and/or other universities.\n\nCognitive computing gives people super powers. Augmenting our intelligence with\nmachine intelligence gives us the ability to predict the future and make the\nbest decisions possible. As the field of machine intelligence progresses, more\nand more super-human abilities become available to us. I am out to bring these\nsuper powers to the masses through cognitive computing products and services.\nAttending Georgia Tech would be an excellent catalyst to help me achieve this.\n\nI currently architect and develop web services and full-stack applications. My\ngoal is to start working full-time on interactive applications that exhibit\nhuman-like intelligence. One way to make these applications intelligent is to\nhave them consume machine learning and cognitive computing web services. These\nservices could be custom built by me or provided by SaaS APIs like Microsoft\nCognitive Services [https://www.microsoft.com/cognitive-services] or IBM Watson\nServices\n[http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/services-catalog.html]\n. To apply the best cognitive services to a given problem, I need a deep\nunderstanding of their underlying algorithms and techniques. I also need to\nlearn how to architect applications and services for scale.\n\nThe front-end development skills I have learned so far are targeted at building\ntypical web app and native app user interfaces. Moving forward, I plan to focus\nmore on natural language and brain-computer interfaces. With the power of\nmachine learning, natural language processing, and data, I could create user\nexperiences that are contextually, socially, and even emotionally-aware.\n\nGeorgia Tech's Online Master of Science in Computer Science provides courses\nthat would teach me how to best integrate machine intelligence into my\napplications. The Knowledge-Based Artificial Intelligence\n[http://www.omscs.gatech.edu/cs-7637-knowledge-based-artificial-intelligence-cognitive-systems] \ncourse is a perfect overview of how to create cognitive systems. The course in \nMachine Learning [http://www.omscs.gatech.edu/cs-7641-machine-learning] would\nteach me how to apply the right algorithms and models to the right problems. The\nmultiple courses on Algorithms, Complexity\n[http://www.omscs.gatech.edu/cs-6505-computability-algorithms], and \nHigh-Performance Computing\n[http://www.omscs.gatech.edu/cs-6290-high-performance-computer-architecture] \nwould teach me how to optimize and scale my applications. Each of these courses\ncovers an essential component to implementing intelligent web services in\nproduction applications.\n\nThe Specialization in Interactive Intelligence\n[http://www.omscs.gatech.edu/specialization-interactive-intelligence] completely\naligns with my desire to become a cognitive computing expert. There are several\ncourses in this specialty that would help me create user experiences that\nexhibit human-level intelligence. The course on Natural Language would help me\nincorporate natural language understanding features into my applications. The\nspecial topic in Computational Creativity would help my applications be more\nexpressive and adaptive. When I first saw this specialty, I immediately knew it\nwas a great fit.\n\nThe Georgia Tech Online Masters in Computer Science is my first choice for many\nreasons. Besides the awesome courses that are available at Georgia Tech, there\nis a ton of groundbreaking research [http://www.research.gatech.edu/] being\nperformed by world renowned professors there. I would love to work with\nProfessor Charles Isbell and the rest of the pfunk research group in The\nLaboratory for Interactive Artificial Intelligence\n[http://www.cc.gatech.edu/~isbell/iai/]. Their research spans many areas I am\ninterested in including interactive entertainment, deep learning, and\ncomputational neuroscience. In addition to these points, the program hit the top\nof my list because it is affordable, online, and produced by one of the world’s\nbest ranking universities.\n\nDriven by the many challenging problems society is facing, I want to use my\nskills and knowledge to help people live amazing lives. I strive to make the\nmost positive impact on existence as possible. I plan to use machine learning\nand natural language processing to improve the health, performance, and\nproductivity of people. Leveraging data, I want to create software to help\npeople live the most impactful lives possible. I want to bring unprecedented\ncapabilities to people by extending their minds into cognitive computing\nsystems. This graduate program would be a core part of my strategy to meet these\ngoals.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2016-04-28T23:30:09.000Z",
            "updated_at": "2021-03-09T16:46:00.000Z",
            "published_at": "2016-04-28T23:56:20.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e8b",
            "uuid": "cb6209f0-1745-40ee-80d0-79efe203de8c",
            "title": "An Intelligent Agent to Help Developers Architect Web Apps",
            "slug": "an-intelligent-agent-to-help-developers-architect-web-apps",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"#### The Problem \\nWhen first developing web applications, choosing a tech stack can be overwhelming. Even choosing your first language to learn can feel like a monstrous task let alone deciding on all of the components of a production application and how to make all of the components talk together. In this post, I will propose an intelligent agent to help developers architect basic web applications. The ultimate system would take in business requirements and recommend architectural design patterns and specific technologies to implement. Though a worthwhile problem, the complexity of such a system would be in for a combinatorial explosion. For this initial proof-of-concept, we will assume the developer is out to build a web application that follows a straightforward [Model-View-Controller](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) (MVC) design pattern. It will also recommend a simplified version of all of the components involved. It will recommend a client-side language, a client-side framework, a back-end language, a back-end framework, and a database system. It will also describe how those components will interact and communicate. \\n#### A Solution \\nOverall, the agent will need to learn about all of the individual components in a web application, learn how all of the components relate, store these learned concepts and relationships for retrieval, gather requirements and constraints from the human user, and reason through its knowledge-base to come up with a solution. These major steps will be broken out into three subsystems and described below. This post will focus on some reasoning techniques I’ve studied recently, namely [common sense reasoning](https://en.wikipedia.org/wiki/Commonsense_reasoning) and [scripts](https://en.wikipedia.org/wiki/Script_theory). \\n###### Knowledge Acquisition and Storage System \\nThe [knowledge-base](https://en.wikipedia.org/wiki/Knowledge_base) (KB) would be a [semantic network](https://en.wikipedia.org/wiki/Semantic_network) of software development concepts defined as [frames](https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)). This structured knowledge representation will provide the foundation for understanding. The initial state of the KB would be populated and heavily curated by expert software architects. This will give it a strong set of base cases for the searching and reasoning that will take place later. Through an automated [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) and web mining system, the agent would acquire more knowledge from online Q&A communities like Stack Overflow and Quora. It would also gather cases from technical documentation like might be found in README’s on GitHub.  \\n\\nEach of the major components to a web application will be represented by an action frame. By having a thematic role associated with each component, it will allow the agent to understand the relationships between the components and how they depend on each other. This will be especially useful when reasoning through the flow of data in the web application. This flow of data is how the components communicate. This flow of data and the functional relationships between the components will provide the basic building blocks for the scripts involved in the reasoning process. \\n \\nIn the particular domain of web application development, we can define a domain specific set of primitive actions. They could be derived from those predefined by the HTTP verbs (GET, POST, PUT, DELETE and so on) and standard database CRUD functions (create, read, update, and delete). \\n\\nSome example action frames: \\n![Action Frame 1](http://cdn.paulprae.com/images/blog/action-frame-1.PNG)\\n![Action Frame 2](http://cdn.paulprae.com/images/blog/action-frame-2.PNG)\\n \\nWith these two frames, we can see how the data makes it from the database system to the view that the user will interact with. These frames can be used to define a script that enforces domain logic. Here, the view cannot talk directly with the model without going through the controller. The logic for this script is defined by the MVC design pattern. These frames, and the script that can be built with them, will aid the reasoning process. Once the agent has its initial semantic network built out, it can search the web for other examples that follow these same patterns, building on what it already knows. \\n\\nThere will also be an associated concept database with more traditional frames that define the different components of a web application in detail. This concept space, will contain languages and specific technologies. For example, it may contain ‘JavaScript’ which would be an instance of a ‘Programming Language’. Programming languages would have special relationships with other technologies like development frameworks. ‘jQuery’ could be an instance of a ‘Front-end Framework’ that is written in the ‘Programming Language’ ‘JavaScript’. This will aid the agent in understanding and reasoning through which components of the web application stack are compatible. \\n###### Interactive Conversation System \\nWhen a user comes looking for help building a web application, the agent will need to gather the information it needs from the user. This can be done through a natural language interface. If it were to be done through an online form, it could be cumbersome for the user because the form could be quite large and very sparse. The user will likely only provide a small amount of information about the system the user wants to build. From that small amount of information, the agent will need to reason through a proposed architecture. \\n\\nThe goal of the interactive agent will be to gather requirements from the user. These requirements will be used as constraints. These constraints will be used to satisfy the problem and also limit the search space. \\n\\nExample questions the user could ask the agent: \\n> “I’d like to use JavaScript as much as possible. Is there a tech stack where that is the only language I’d need to know? If so, what is it?” \\n>\\n>“Can I used Python with MongoDB? How do I query MongoDB from Python?” \\n\\nIn the first example, the agent could start building out a semantic network of action frames for the proposed architecture. It will at least be able to set the front-end language and back-end language to ‘JavaScript’. It could then infer that all of the frameworks it suggests must support ‘JavaScript’. \\n###### Reasoning System \\nTo reason through and recommend architectures from existing cases, the agent will use common sense reasoning and scripts. To come up with more unique and original architectures, the agent could use a combination of explanation-based learning and analogical reasoning (though this will not be discussed in this paper). Even though the agent would work just fine by using learning by recording cases and case-based reasoning for suggesting solutions based on existing examples, I would like to experiment with scripts as a way to infer what action frames are missing and what their values need to be. \\n\\n__Common Sense Reasoning__\\n![Common Sense Reasoning](http://cdn.paulprae.com/images/blog/common-sense-reasoning2.png)\\n(Ashok Goel, 2016, https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409) \\n\\nWe’ve already talked through the frames and the foundations for understanding the agent will need. The next step is common sense reasoning. In the context of this agent, universal knowledge is unnecessary. It will just need to know and be able to reason through common web application scenarios. The principles of common sense reasoning still apply though. Reasoning will need to take place at several different moments during the agent’s journey. The first of which is when it learns from conversations on the web. The second is when it is trying to understand what the user is asking for. The third is when it decides on a solution to propose to the user. \\n\\nIf searching through Stack Overflow, it will need to identify concepts it already has stored and make inferences on new relationships it’s discovering. It may discover a post talking about JavaScript and front-end frameworks, two-concepts it is already familiar with. Someone may suggest a new framework, e.g. “you should check out React”. It can then identify ‘React’ as a new concept and store it as an instance of a ‘Front-end Framework’. In this case, even the knowledge acquisition step must apply logic and reasoning so the agent can learn. All of these concepts and their relationships exist to make the reasoning steps easier. \\n\\n__Scripts__\\nThe most critical part of this system is the agent’s ability to recommend architectures given a set of constraints. As I alluded to earlier, the concept of scripts could be exploited to do just this. \\n![Scripts](http://cdn.paulprae.com/images/blog/definition-of-scripts-medium.PNG)\\n(Ashok Goel, 2016, https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409) \\n\\nIn the context of this agent, the ‘events’ are the actions that each component of the web application stack is responsible for. As was obvious in the action frames, each component is responsible for performing some action on data. It could be receiving data from some other component or maybe displaying data to the user. If a user of a web application performs an action, such as clicking a button, it will set off a series of events that may go from the view to the controller to the model and all the way back around again (in fact, the term ‘events’ is used by the web development community as well). To complete such a process, the data passes between all components of the web application’s tech stack. Since these actions cascade throughout all the components, it shows they are ‘causally’ connected. Because this is a real-world scenario, the web application acts as a scene in the world. \\n\\nLooking at each proposed web application architecture as a script is a useful abstraction. For each valid architecture, such as MVC, a script would have to be created. It would need to be abstract enough to capture all common sequences of events that occur in a web application. So, if an instance of a script is causally coherent and all of the relationships of the underlying semantic network are satisfied, the web application architecture is valid. Said another way, if the data can flow from the user to the model and back again successfully, the proposed architecture would function properly making it safe to recommend as a solution. \\n###### Conclusion \\nTo truly be useful, the agent would need to handle much more complex scenarios. There are many other architectural patterns that would need to be covered. There are also many more components to a web application than I discussed (you can get quite granular if you wanted to). What would be really exciting, but much more difficult to implement, is if this agent could architect an application given business requirements. The other components not discussed is how the agent would need to get creative if it does not have any existing cases that satisfy the needs of the user. Analogical reasoning could be helpful here as a way to come up with novel architectures. In general, it would be good to test the design proposed in this paper against one that uses [case-based reasoning](https://en.wikipedia.org/wiki/Case-based_reasoning), [explanation-based learning](https://en.wikipedia.org/wiki/Explanation-based_learning), and [analogical reasoning](https://plato.stanford.edu/entries/reasoning-analogy/). Regardless, if such a tool was built and worked well I know many young professionals would find it useful. \\n \\n\\n#### Works Cited \\n \\n+ \\\"Commonsense reasoning.\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 19 August 2016. Web. 6 November 2016.  \\n+ \\\"Create, read, update and delete.\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 27 October 2016. Web. 6 November 2016.  \\n+ \\\"Frame (artificial intelligence).\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 7 September 2016. Web. 2 October 2016.  \\n+ Goel, Ashok and David Joyner. Knowledge-Based AI: Cognitive Systems by Georgia Tech [Videos]. Udacity. https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409 Accessed 6 November 2016. \\n+ Norvig, Peter and Stuart Russell. Artificial Intelligence: A Modern Approach. 3rd ed., Pearson, 2010.   \\n+ Poole, David and Alan Mackworth. Case-Based Reasoning. Cambridge University Press. artint.info/html/ArtInt_190.html. Accessed 2 October 2016.  \\n+ \\\"Representational state transfer.\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 31 October 2016. Web. 6 November 2016.  \\n+ Winston, Patrick. Artificial Intelligence. 3rd ed., Addison-Wesley Publishing Company, 1993.  \\n \"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><h4 id=\"theproblem\">The Problem</h4>\n<p>When first developing web applications, choosing a tech stack can be overwhelming. Even choosing your first language to learn can feel like a monstrous task let alone deciding on all of the components of a production application and how to make all of the components talk together. In this post, I will propose an intelligent agent to help developers architect basic web applications. The ultimate system would take in business requirements and recommend architectural design patterns and specific technologies to implement. Though a worthwhile problem, the complexity of such a system would be in for a combinatorial explosion. For this initial proof-of-concept, we will assume the developer is out to build a web application that follows a straightforward <a href=\"https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller\">Model-View-Controller</a> (MVC) design pattern. It will also recommend a simplified version of all of the components involved. It will recommend a client-side language, a client-side framework, a back-end language, a back-end framework, and a database system. It will also describe how those components will interact and communicate.</p>\n<h4 id=\"asolution\">A Solution</h4>\n<p>Overall, the agent will need to learn about all of the individual components in a web application, learn how all of the components relate, store these learned concepts and relationships for retrieval, gather requirements and constraints from the human user, and reason through its knowledge-base to come up with a solution. These major steps will be broken out into three subsystems and described below. This post will focus on some reasoning techniques I’ve studied recently, namely <a href=\"https://en.wikipedia.org/wiki/Commonsense_reasoning\">common sense reasoning</a> and <a href=\"https://en.wikipedia.org/wiki/Script_theory\">scripts</a>.</p>\n<h6 id=\"knowledgeacquisitionandstoragesystem\">Knowledge Acquisition and Storage System</h6>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Knowledge_base\">knowledge-base</a> (KB) would be a <a href=\"https://en.wikipedia.org/wiki/Semantic_network\">semantic network</a> of software development concepts defined as <a href=\"https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)\">frames</a>. This structured knowledge representation will provide the foundation for understanding. The initial state of the KB would be populated and heavily curated by expert software architects. This will give it a strong set of base cases for the searching and reasoning that will take place later. Through an automated <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">natural language processing</a> and web mining system, the agent would acquire more knowledge from online Q&amp;A communities like Stack Overflow and Quora. It would also gather cases from technical documentation like might be found in README’s on GitHub.</p>\n<p>Each of the major components to a web application will be represented by an action frame. By having a thematic role associated with each component, it will allow the agent to understand the relationships between the components and how they depend on each other. This will be especially useful when reasoning through the flow of data in the web application. This flow of data is how the components communicate. This flow of data and the functional relationships between the components will provide the basic building blocks for the scripts involved in the reasoning process.</p>\n<p>In the particular domain of web application development, we can define a domain specific set of primitive actions. They could be derived from those predefined by the HTTP verbs (GET, POST, PUT, DELETE and so on) and standard database CRUD functions (create, read, update, and delete).</p>\n<p>Some example action frames:<br>\n<img src=\"http://cdn.paulprae.com/images/blog/action-frame-1.PNG\" alt=\"Action Frame 1\" loading=\"lazy\"><br>\n<img src=\"http://cdn.paulprae.com/images/blog/action-frame-2.PNG\" alt=\"Action Frame 2\" loading=\"lazy\"></p>\n<p>With these two frames, we can see how the data makes it from the database system to the view that the user will interact with. These frames can be used to define a script that enforces domain logic. Here, the view cannot talk directly with the model without going through the controller. The logic for this script is defined by the MVC design pattern. These frames, and the script that can be built with them, will aid the reasoning process. Once the agent has its initial semantic network built out, it can search the web for other examples that follow these same patterns, building on what it already knows.</p>\n<p>There will also be an associated concept database with more traditional frames that define the different components of a web application in detail. This concept space, will contain languages and specific technologies. For example, it may contain ‘JavaScript’ which would be an instance of a ‘Programming Language’. Programming languages would have special relationships with other technologies like development frameworks. ‘jQuery’ could be an instance of a ‘Front-end Framework’ that is written in the ‘Programming Language’ ‘JavaScript’. This will aid the agent in understanding and reasoning through which components of the web application stack are compatible.</p>\n<h6 id=\"interactiveconversationsystem\">Interactive Conversation System</h6>\n<p>When a user comes looking for help building a web application, the agent will need to gather the information it needs from the user. This can be done through a natural language interface. If it were to be done through an online form, it could be cumbersome for the user because the form could be quite large and very sparse. The user will likely only provide a small amount of information about the system the user wants to build. From that small amount of information, the agent will need to reason through a proposed architecture.</p>\n<p>The goal of the interactive agent will be to gather requirements from the user. These requirements will be used as constraints. These constraints will be used to satisfy the problem and also limit the search space.</p>\n<p>Example questions the user could ask the agent:</p>\n<blockquote>\n<p>“I’d like to use JavaScript as much as possible. Is there a tech stack where that is the only language I’d need to know? If so, what is it?”</p>\n<p>“Can I used Python with MongoDB? How do I query MongoDB from Python?”</p>\n</blockquote>\n<p>In the first example, the agent could start building out a semantic network of action frames for the proposed architecture. It will at least be able to set the front-end language and back-end language to ‘JavaScript’. It could then infer that all of the frameworks it suggests must support ‘JavaScript’.</p>\n<h6 id=\"reasoningsystem\">Reasoning System</h6>\n<p>To reason through and recommend architectures from existing cases, the agent will use common sense reasoning and scripts. To come up with more unique and original architectures, the agent could use a combination of explanation-based learning and analogical reasoning (though this will not be discussed in this paper). Even though the agent would work just fine by using learning by recording cases and case-based reasoning for suggesting solutions based on existing examples, I would like to experiment with scripts as a way to infer what action frames are missing and what their values need to be.</p>\n<p><strong>Common Sense Reasoning</strong><br>\n<img src=\"http://cdn.paulprae.com/images/blog/common-sense-reasoning2.png\" alt=\"Common Sense Reasoning\" loading=\"lazy\"><br>\n(Ashok Goel, 2016, <a href=\"https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409\">https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409</a>)</p>\n<p>We’ve already talked through the frames and the foundations for understanding the agent will need. The next step is common sense reasoning. In the context of this agent, universal knowledge is unnecessary. It will just need to know and be able to reason through common web application scenarios. The principles of common sense reasoning still apply though. Reasoning will need to take place at several different moments during the agent’s journey. The first of which is when it learns from conversations on the web. The second is when it is trying to understand what the user is asking for. The third is when it decides on a solution to propose to the user.</p>\n<p>If searching through Stack Overflow, it will need to identify concepts it already has stored and make inferences on new relationships it’s discovering. It may discover a post talking about JavaScript and front-end frameworks, two-concepts it is already familiar with. Someone may suggest a new framework, e.g. “you should check out React”. It can then identify ‘React’ as a new concept and store it as an instance of a ‘Front-end Framework’. In this case, even the knowledge acquisition step must apply logic and reasoning so the agent can learn. All of these concepts and their relationships exist to make the reasoning steps easier.</p>\n<p><strong>Scripts</strong><br>\nThe most critical part of this system is the agent’s ability to recommend architectures given a set of constraints. As I alluded to earlier, the concept of scripts could be exploited to do just this.<br>\n<img src=\"http://cdn.paulprae.com/images/blog/definition-of-scripts-medium.PNG\" alt=\"Scripts\" loading=\"lazy\"><br>\n(Ashok Goel, 2016, <a href=\"https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409\">https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409</a>)</p>\n<p>In the context of this agent, the ‘events’ are the actions that each component of the web application stack is responsible for. As was obvious in the action frames, each component is responsible for performing some action on data. It could be receiving data from some other component or maybe displaying data to the user. If a user of a web application performs an action, such as clicking a button, it will set off a series of events that may go from the view to the controller to the model and all the way back around again (in fact, the term ‘events’ is used by the web development community as well). To complete such a process, the data passes between all components of the web application’s tech stack. Since these actions cascade throughout all the components, it shows they are ‘causally’ connected. Because this is a real-world scenario, the web application acts as a scene in the world.</p>\n<p>Looking at each proposed web application architecture as a script is a useful abstraction. For each valid architecture, such as MVC, a script would have to be created. It would need to be abstract enough to capture all common sequences of events that occur in a web application. So, if an instance of a script is causally coherent and all of the relationships of the underlying semantic network are satisfied, the web application architecture is valid. Said another way, if the data can flow from the user to the model and back again successfully, the proposed architecture would function properly making it safe to recommend as a solution.</p>\n<h6 id=\"conclusion\">Conclusion</h6>\n<p>To truly be useful, the agent would need to handle much more complex scenarios. There are many other architectural patterns that would need to be covered. There are also many more components to a web application than I discussed (you can get quite granular if you wanted to). What would be really exciting, but much more difficult to implement, is if this agent could architect an application given business requirements. The other components not discussed is how the agent would need to get creative if it does not have any existing cases that satisfy the needs of the user. Analogical reasoning could be helpful here as a way to come up with novel architectures. In general, it would be good to test the design proposed in this paper against one that uses <a href=\"https://en.wikipedia.org/wiki/Case-based_reasoning\">case-based reasoning</a>, <a href=\"https://en.wikipedia.org/wiki/Explanation-based_learning\">explanation-based learning</a>, and <a href=\"https://plato.stanford.edu/entries/reasoning-analogy/\">analogical reasoning</a>. Regardless, if such a tool was built and worked well I know many young professionals would find it useful.</p>\n<h4 id=\"workscited\">Works Cited</h4>\n<ul>\n<li>&quot;Commonsense reasoning.&quot; Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 19 August 2016. Web. 6 November 2016.</li>\n<li>&quot;Create, read, update and delete.&quot; Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 27 October 2016. Web. 6 November 2016.</li>\n<li>&quot;Frame (artificial intelligence).&quot; Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 7 September 2016. Web. 2 October 2016.</li>\n<li>Goel, Ashok and David Joyner. Knowledge-Based AI: Cognitive Systems by Georgia Tech [Videos]. Udacity. <a href=\"https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409\">https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409</a> Accessed 6 November 2016.</li>\n<li>Norvig, Peter and Stuart Russell. Artificial Intelligence: A Modern Approach. 3rd ed., Pearson, 2010.</li>\n<li>Poole, David and Alan Mackworth. Case-Based Reasoning. Cambridge University Press. artint.info/html/ArtInt_190.html. Accessed 2 October 2016.</li>\n<li>&quot;Representational state transfer.&quot; Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 31 October 2016. Web. 6 November 2016.</li>\n<li>Winston, Patrick. Artificial Intelligence. 3rd ed., Addison-Wesley Publishing Company, 1993.</li>\n</ul>\n<!--kg-card-end: markdown-->",
            "comment_id": "20",
            "plaintext": "The Problem\nWhen first developing web applications, choosing a tech stack can be\noverwhelming. Even choosing your first language to learn can feel like a\nmonstrous task let alone deciding on all of the components of a production\napplication and how to make all of the components talk together. In this post, I\nwill propose an intelligent agent to help developers architect basic web\napplications. The ultimate system would take in business requirements and\nrecommend architectural design patterns and specific technologies to implement.\nThough a worthwhile problem, the complexity of such a system would be in for a\ncombinatorial explosion. For this initial proof-of-concept, we will assume the\ndeveloper is out to build a web application that follows a straightforward \nModel-View-Controller\n[https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller] (MVC)\ndesign pattern. It will also recommend a simplified version of all of the\ncomponents involved. It will recommend a client-side language, a client-side\nframework, a back-end language, a back-end framework, and a database system. It\nwill also describe how those components will interact and communicate.\n\nA Solution\nOverall, the agent will need to learn about all of the individual components in\na web application, learn how all of the components relate, store these learned\nconcepts and relationships for retrieval, gather requirements and constraints\nfrom the human user, and reason through its knowledge-base to come up with a\nsolution. These major steps will be broken out into three subsystems and\ndescribed below. This post will focus on some reasoning techniques I’ve studied\nrecently, namely common sense reasoning\n[https://en.wikipedia.org/wiki/Commonsense_reasoning] and scripts\n[https://en.wikipedia.org/wiki/Script_theory].\n\nKnowledge Acquisition and Storage System\nThe knowledge-base [https://en.wikipedia.org/wiki/Knowledge_base] (KB) would be\na semantic network [https://en.wikipedia.org/wiki/Semantic_network] of software\ndevelopment concepts defined as frames\n[https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)]. This structured\nknowledge representation will provide the foundation for understanding. The\ninitial state of the KB would be populated and heavily curated by expert\nsoftware architects. This will give it a strong set of base cases for the\nsearching and reasoning that will take place later. Through an automated \nnatural\nlanguage processing [https://en.wikipedia.org/wiki/Natural_language_processing] \nand web mining system, the agent would acquire more knowledge from online Q&A\ncommunities like Stack Overflow and Quora. It would also gather cases from\ntechnical documentation like might be found in README’s on GitHub.\n\nEach of the major components to a web application will be represented by an\naction frame. By having a thematic role associated with each component, it will\nallow the agent to understand the relationships between the components and how\nthey depend on each other. This will be especially useful when reasoning through\nthe flow of data in the web application. This flow of data is how the components\ncommunicate. This flow of data and the functional relationships between the\ncomponents will provide the basic building blocks for the scripts involved in\nthe reasoning process.\n\nIn the particular domain of web application development, we can define a domain\nspecific set of primitive actions. They could be derived from those predefined\nby the HTTP verbs (GET, POST, PUT, DELETE and so on) and standard database CRUD\nfunctions (create, read, update, and delete).\n\nSome example action frames:\n\n\n\nWith these two frames, we can see how the data makes it from the database system\nto the view that the user will interact with. These frames can be used to define\na script that enforces domain logic. Here, the view cannot talk directly with\nthe model without going through the controller. The logic for this script is\ndefined by the MVC design pattern. These frames, and the script that can be\nbuilt with them, will aid the reasoning process. Once the agent has its initial\nsemantic network built out, it can search the web for other examples that follow\nthese same patterns, building on what it already knows.\n\nThere will also be an associated concept database with more traditional frames\nthat define the different components of a web application in detail. This\nconcept space, will contain languages and specific technologies. For example, it\nmay contain ‘JavaScript’ which would be an instance of a ‘Programming Language’.\nProgramming languages would have special relationships with other technologies\nlike development frameworks. ‘jQuery’ could be an instance of a ‘Front-end\nFramework’ that is written in the ‘Programming Language’ ‘JavaScript’. This will\naid the agent in understanding and reasoning through which components of the web\napplication stack are compatible.\n\nInteractive Conversation System\nWhen a user comes looking for help building a web application, the agent will\nneed to gather the information it needs from the user. This can be done through\na natural language interface. If it were to be done through an online form, it\ncould be cumbersome for the user because the form could be quite large and very\nsparse. The user will likely only provide a small amount of information about\nthe system the user wants to build. From that small amount of information, the\nagent will need to reason through a proposed architecture.\n\nThe goal of the interactive agent will be to gather requirements from the user.\nThese requirements will be used as constraints. These constraints will be used\nto satisfy the problem and also limit the search space.\n\nExample questions the user could ask the agent:\n\n> “I’d like to use JavaScript as much as possible. Is there a tech stack where\nthat is the only language I’d need to know? If so, what is it?”\n\n“Can I used Python with MongoDB? How do I query MongoDB from Python?”\n\n\nIn the first example, the agent could start building out a semantic network of\naction frames for the proposed architecture. It will at least be able to set the\nfront-end language and back-end language to ‘JavaScript’. It could then infer\nthat all of the frameworks it suggests must support ‘JavaScript’.\n\nReasoning System\nTo reason through and recommend architectures from existing cases, the agent\nwill use common sense reasoning and scripts. To come up with more unique and\noriginal architectures, the agent could use a combination of explanation-based\nlearning and analogical reasoning (though this will not be discussed in this\npaper). Even though the agent would work just fine by using learning by\nrecording cases and case-based reasoning for suggesting solutions based on\nexisting examples, I would like to experiment with scripts as a way to infer\nwhat action frames are missing and what their values need to be.\n\nCommon Sense Reasoning\n\n(Ashok Goel, 2016, \nhttps://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409)\n\nWe’ve already talked through the frames and the foundations for understanding\nthe agent will need. The next step is common sense reasoning. In the context of\nthis agent, universal knowledge is unnecessary. It will just need to know and be\nable to reason through common web application scenarios. The principles of\ncommon sense reasoning still apply though. Reasoning will need to take place at\nseveral different moments during the agent’s journey. The first of which is when\nit learns from conversations on the web. The second is when it is trying to\nunderstand what the user is asking for. The third is when it decides on a\nsolution to propose to the user.\n\nIf searching through Stack Overflow, it will need to identify concepts it\nalready has stored and make inferences on new relationships it’s discovering. It\nmay discover a post talking about JavaScript and front-end frameworks,\ntwo-concepts it is already familiar with. Someone may suggest a new framework,\ne.g. “you should check out React”. It can then identify ‘React’ as a new concept\nand store it as an instance of a ‘Front-end Framework’. In this case, even the\nknowledge acquisition step must apply logic and reasoning so the agent can\nlearn. All of these concepts and their relationships exist to make the reasoning\nsteps easier.\n\nScripts\nThe most critical part of this system is the agent’s ability to recommend\narchitectures given a set of constraints. As I alluded to earlier, the concept\nof scripts could be exploited to do just this.\n\n(Ashok Goel, 2016, \nhttps://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409)\n\nIn the context of this agent, the ‘events’ are the actions that each component\nof the web application stack is responsible for. As was obvious in the action\nframes, each component is responsible for performing some action on data. It\ncould be receiving data from some other component or maybe displaying data to\nthe user. If a user of a web application performs an action, such as clicking a\nbutton, it will set off a series of events that may go from the view to the\ncontroller to the model and all the way back around again (in fact, the term\n‘events’ is used by the web development community as well). To complete such a\nprocess, the data passes between all components of the web application’s tech\nstack. Since these actions cascade throughout all the components, it shows they\nare ‘causally’ connected. Because this is a real-world scenario, the web\napplication acts as a scene in the world.\n\nLooking at each proposed web application architecture as a script is a useful\nabstraction. For each valid architecture, such as MVC, a script would have to be\ncreated. It would need to be abstract enough to capture all common sequences of\nevents that occur in a web application. So, if an instance of a script is\ncausally coherent and all of the relationships of the underlying semantic\nnetwork are satisfied, the web application architecture is valid. Said another\nway, if the data can flow from the user to the model and back again\nsuccessfully, the proposed architecture would function properly making it safe\nto recommend as a solution.\n\nConclusion\nTo truly be useful, the agent would need to handle much more complex scenarios.\nThere are many other architectural patterns that would need to be covered. There\nare also many more components to a web application than I discussed (you can get\nquite granular if you wanted to). What would be really exciting, but much more\ndifficult to implement, is if this agent could architect an application given\nbusiness requirements. The other components not discussed is how the agent would\nneed to get creative if it does not have any existing cases that satisfy the\nneeds of the user. Analogical reasoning could be helpful here as a way to come\nup with novel architectures. In general, it would be good to test the design\nproposed in this paper against one that uses case-based reasoning\n[https://en.wikipedia.org/wiki/Case-based_reasoning], explanation-based learning\n[https://en.wikipedia.org/wiki/Explanation-based_learning], and analogical\nreasoning [https://plato.stanford.edu/entries/reasoning-analogy/]. Regardless,\nif such a tool was built and worked well I know many young professionals would\nfind it useful.\n\nWorks Cited\n * \"Commonsense reasoning.\" Wikipedia: The Free Encyclopedia. Wikimedia\n   Foundation, Inc. 19 August 2016. Web. 6 November 2016.\n * \"Create, read, update and delete.\" Wikipedia: The Free Encyclopedia.\n   Wikimedia Foundation, Inc. 27 October 2016. Web. 6 November 2016.\n * \"Frame (artificial intelligence).\" Wikipedia: The Free Encyclopedia.\n   Wikimedia Foundation, Inc. 7 September 2016. Web. 2 October 2016.\n * Goel, Ashok and David Joyner. Knowledge-Based AI: Cognitive Systems by\n   Georgia Tech [Videos]. Udacity. \n   https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409 \n   Accessed 6 November 2016.\n * Norvig, Peter and Stuart Russell. Artificial Intelligence: A Modern Approach.\n   3rd ed., Pearson, 2010.\n * Poole, David and Alan Mackworth. Case-Based Reasoning. Cambridge University\n   Press. artint.info/html/ArtInt_190.html. Accessed 2 October 2016.\n * \"Representational state transfer.\" Wikipedia: The Free Encyclopedia.\n   Wikimedia Foundation, Inc. 31 October 2016. Web. 6 November 2016.\n * Winston, Patrick. Artificial Intelligence. 3rd ed., Addison-Wesley Publishing\n   Company, 1993.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2017-01-10T01:49:47.000Z",
            "updated_at": "2021-03-09T16:45:38.000Z",
            "published_at": "2017-01-10T03:15:21.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e8c",
            "uuid": "3f91f961-b513-4e06-864d-c38e79a631c1",
            "title": "Neona: A Conversational Agent That Teaches AI",
            "slug": "neona-a-conversational-agent-that-teaches-ai-2",
            "mobiledoc": null,
            "html": "<p>Towards the end of last year, I began working on a chat bot to help people understand artificial intelligence. She's named Neona. People interacting with her will learn about the topics students of AI study. In the future, she will be used by students to find courses and jobs in the field of AI. From the perspective of universities and employers, she will be used as a teaching assistant and a recruiting tool. In this post, I'll describe her technical architecture and the artificial intelligence concepts used in her design. <a href=\"http://www.neona.chat/\">An early demo is now available</a> if you'd like to chat with her.</p>\n<h2 id=\"high-level-design\">High-Level Design</h2>\n<p>There are two primary systems that compose the agent. The first is a knowledge system composed of subsystems that acquire, store, and retrieve knowledge. The second is the conversational system that allows the agent to interact with people and reason over its knowledge. There are two technologies that these systems share that allowed the agent to remain cohesive: <a href=\"https://azure.microsoft.com/\">Microsoft Azure</a>, an “open, flexible, enterprise-grade cloud computing platform”, and <a href=\"https://nodejs.org/en/\">Node.js</a>, an “event-driven I/O server-side JavaScript environment”. These were chosen because of the potential of the <a href=\"https://dev.botframework.com/\">Microsoft Bot Framework</a>, which was the main driver behind this architecture.</p>\n<h3 id=\"the-knowledge-base\">The Knowledge-Base</h3>\n<p>The current knowledge-base (KB) consists of AI concepts pulled from Wikipedia and stored in a document store. In the future, the KB will also consist of users that interact with the agent, courses from online programs, and jobs from career websites.</p>\n<p>The overall process was to define the data the agent needed, extract it from Wikipedia, and store it in the document store where the agent can retrieve it.</p>\n<p>Here are the technologies that were implemented to put this KB into production:</p>\n<ul>\n<li><a href=\"https://azure.microsoft.com/en-us/services/documentdb/\">DocumentDB</a>: “A distributed database service for managing JSON documents at Internet scale.” It is a highly-flexible key-value store that integrates closely with other Microsoft systems and interfaces well with Node.js.</li>\n<li><a href=\"https://azure.microsoft.com/en-us/services/functions/\">Azure Functions</a>: “Process events with a serverless code architecture.” This service allowed several small Node.js modules to run in the cloud. These modules were responsible for extracting data from Wikipedia, processing it into the desired form, and storing it in DocumentDB.</li>\n<li><a href=\"https://www.mediawiki.org/wiki/API:Main_page\">MediaWiki action API</a>: “A web service that provides convenient access to wiki features, data, and meta-data over HTTP.” Using Node.js’ <a href=\"https://www.npmjs.com/package/request\">request</a> module and JavaScript <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise\">Promises</a>, the API is called with carefully constructed queries that return just the data the agent would need.</li>\n<li><a href=\"https://azure.microsoft.com/en-us/services/search/\">Azure Search</a>: “A fully managed search-as-a-service in the cloud.” It indexed the contents of the KB and provided an HTTP endpoint the agent can hit to query the KB. It natively supports DocumentDB.</li>\n</ul>\n<h4 id=\"the-knowledge-source\">The Knowledge Source</h4>\n<p>All data was extracted from Wikipedia, starting at the node <a href=\"https://en.wikipedia.org/wiki/Category:Artificial_intelligence\">https://en.wikipedia.org/wiki/Category:Artificial_intelligence</a>. The agent made an attempt to learn concepts from every page and several subcategories from that root category. There is a one-to-one relationship between a Wikipedia page and a concept in the KB. If any issues were encountered while scraping a given page, such as missing data, the page was skipped.</p>\n<p>Here are some examples of the final queries developed:</p>\n<ul>\n<li>Get as many as 500 page ids from a given category: <a href=\"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;list=categorymembers&amp;cmlimit=500&amp;cmtype=page&amp;cmprop=ids&amp;cmtitle=Category:Artificial_intelligence\">https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;list=categorymembers&amp;cmlimit=500&amp;cmtype=page&amp;cmprop=ids&amp;cmtitle=Category:Artificial_intelligence</a></li>\n<li>Get up to 20 pages worth of data given a list of page ids: <a href=\"https://en.wikipedia.org/w/api.php?format=json&amp;action=query&amp;prop=extracts&amp;exlimit=20&amp;exintro=&amp;explaintext=&amp;indexpageids=&amp;pageids=51023476%7C1164%7C17188\">https://en.wikipedia.org/w/api.php?format=json&amp;action=query&amp;prop=extracts&amp;exlimit=20&amp;exintro=&amp;explaintext=&amp;indexpageids=&amp;pageids=51023476|1164|17188</a></li>\n</ul>\n<p>Feel free to try either of those out in a browser to see the results. They were built by referencing the WikiMedia module <a href=\"https://www.mediawiki.org/wiki/API:Categorymembers\">Categorymembers</a> and the extension <a href=\"https://www.mediawiki.org/wiki/Extension:TextExtracts\">Extracts</a>, respectively.</p>\n<h4 id=\"knowledge-acquisition\">Knowledge Acquisition</h4>\n<p>The WikipediaCategoryToAIConcepts Azure Function takes in a Wikipedia category and attempts to transform all of its child pages into JSON documents that represent concepts in the KB. The InsertAIConcept Azure Function takes in a single JSON document and inserts it into the KB as long as the concept it represents is not already stored in the KB. These functions are built in a way that they could be ran on a schedule and update the KB as the Wikipedia category and pages are updated. The code for this part of the project can be found on GitHub at <a href=\"https://github.com/praeducer/conversational-agent-functions\">https://github.com/praeducer/conversational-agent-functions</a>. This repo is continuously integrated with the production agent.</p>\n<h4 id=\"knowledge-representation\">Knowledge Representation</h4>\n<p>The concepts are stored in DocumentDB as JSON objects.</p>\n<p>Here is an example document:<br>\n<code>{    \"source\": {      \"name\": \"wikipedia\",      \"accessedDate\": 1479691542482,      \"scriptVersion\": \"0.0.1\",      \"pageid\": 10136    },    \"active\": true,    \"title\": \"Expert system\",    \"extract\": \"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\",    \"id\": \"a958db8e-7160-42f4-b3a8-388d2726ba0e\"  }</code></p>\n<p>The source is designed so that the agent can learn concepts from more than just Wikipedia. Because this is built in DocumentDB, this schema can change easily if new sources provide new information.</p>\n<h4 id=\"knowledge-retrieval\">Knowledge Retrieval</h4>\n<p>The agent uses Azure Search to efficiently retrieve concepts from the KB. The KB is indexed nightly so any documents requested are up-to-date. So far, 811 AI concepts are ready for retrieval:<br>\n<img src=\"http://cdn.paulprae.com/images/neona/azure-search-usage.png\" alt=\"Azure Search Usage\" loading=\"lazy\"></p>\n<p>The ‘title’ and the ‘extract’ of each concept is indexed, making them “searchable”:<br>\n<img src=\"http://cdn.paulprae.com/images/neona/azure-search-fields.png\" alt=\"Azure Search Field\" loading=\"lazy\"></p>\n<p>Results are queried using URLs like: <a href=\"https://contoso.search.windows.net/indexes/aiconcept/docs?api-version=2015-02-28&amp;search=expert%20system\">https://contoso.search.windows.net/indexes/aiconcept/docs?api-version=2015-02-28&amp;search=expert system</a></p>\n<p>Which can return a ranked list of results like:<br>\n<code>[          {              \"@search.score\": 4.4751096,              \"title\": \"Expert system\",              \"extract\": \"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\",              \"id\": \"a958db8e-7160-42f4-b3a8-388d2726ba0e\"          },          {              \"@search.score\": 3.9047346,              \"title\": \"Legal expert system\",              \"extract\": \"A legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law. Legal expert systems employ a rule base or knowledge base and an inference engine to accumulate, reference and produce expert knowledge on specific subjects within the legal domain.\\n\\n\",              \"id\": \"d1f14975-32cd-4243-bdc1-b00d82273092\"          }  ]</code></p>\n<h3 id=\"the-conversational-agent\">The Conversational Agent</h3>\n<p>The agent’s primary purpose and most robust feature is its ability to search for AI concepts from the KB. It is also able to handle simple conversational components such as ‘Hello’, ‘Goodbye’, ‘How are you?’, ‘Who are you?’, ‘Thank You’, ‘You’re Welcome’, and can even tell some jokes. It is currently tested on Skype but theoretically would work as-is on several other channels including Facebook Messenger and a web client. Connecting to a variety of channels from a single code-base is a key benefit of the Microsoft Bot Framework.</p>\n<p>Here are the technologies used to put this conversational agent into production:</p>\n<ul>\n<li><a href=\"https://dev.botframework.com/\">Microsoft Bot Framework</a>: This framework provided an impressive amount of functionality and was the inspiration for this project. For this agent, it provided the connection to Skype, managed incoming and outgoing messages, and helped orchestrate the dialogue flow. It also integrated naturally with LUIS.</li>\n<li><a href=\"https://www.luis.ai/\">LUIS</a>: “A fast and effective way of adding language understanding to applications.” After some manual training, LUIS was used to detect the intent of human messages. Then, after some logical routing, the agent could return a reasonable response.</li>\n<li><a href=\"https://azure.microsoft.com/en-us/services/bot-service/\">Azure Bot Service</a>: “Intelligent, serverless bot service that scales on demand.” After setting up continuous integration, this service turned the development repository into a production application. It acted as the DevOps team.</li>\n</ul>\n<p>The code for these components lives on GitHub at <a href=\"https://github.com/praeducer/conversational-agent\">https://github.com/praeducer/conversational-agent</a>.</p>\n<h4 id=\"detecting-and-acting-upon-intent\">Detecting And Acting Upon Intent</h4>\n<p>The agent has one main or root dialogue which can route the user to several smaller sub-dialogues. The root dialogue is very wide, handling many different kinds of incoming messages. The sub-dialogues are more narrow, doing very specific things but sometimes going a little deeper than the root dialogue would go. When the human first engages with the agent, it introduces itself. After the initial introduction, any incoming messages from the human are routed to LUIS. LUIS then recognizes what the human is trying to say and maps it to an intent. LUIS was manually trained to handle sixteen different intents. Each intent has a set of functionality associated with it that can range from a simple text response to constructing UI elements that are displayed in the chat window. Some intents even start new dialogues and ask the user questions in order to gather more information.</p>\n<p>Custom intents are defined in LUIS and then referenced in code. LUIS uses a classifier to map utterances, i.e. things a human may say, to intents. To seed the system, utterances were entered in manually and tagged with intents. Once LUIS had a solid set of initial data, the model was trained. As humans use the system, an administrator must go in and manually label any utterances LUIS had difficulties classifying and retrain the model.</p>\n<p>Here are some examples of utterances, how LUIS labeled them, and LUIS’s confidence score:<br>\n<img src=\"http://cdn.paulprae.com/images/neona/LUIS-utterances.png\" alt=\"LUIS Utterances\" loading=\"lazy\"></p>\n<p>As you can see, it was very confident when labeling the SearchConcept intent. LUIS’s Thanks label was correct as well, even though its confidence score was low. It won because its confidence score is proportionately much higher than the rest (note that a drop down can be used to correct the label if  needed):<br>\n<img src=\"http://cdn.paulprae.com/images/neona/LUIS-labels.jpg\" alt=\"LUIS Labels\" loading=\"lazy\"></p>\n<h4 id=\"the-dialogue\">The Dialogue</h4>\n<p>Each intent the agent is programmed to handle leads to a new dialogue. From that dialogue, the human can say things that take it deeper into another child dialogue or take it back into the parent dialogue.</p>\n<p>Here are the more important intents the agent handles and what they mean:</p>\n<ul>\n<li>Hello: Responds to things like ‘Hey’ or ‘Hi’ with a welcoming message and some instructions.</li>\n<li>SearchConcept: Looks up AI concepts for the user. The agent can detect things like ‘find’ or ‘search’ and extract the concept it needs to look up.</li>\n<li>More: Displays more search results.</li>\n<li>List: Displays any concepts the user has saved.</li>\n<li>HowAreYou?: Responds to questions like “How are you?” with a friendly response.</li>\n<li>Sorry: If the human gets frustrated, sometimes the agent notices and can apologize.</li>\n<li>Help: Provides instructions to the user if they seem confused or explicitly asks for help.</li>\n<li>Joke: Tells the human a random joke from its KB.</li>\n</ul>\n<p>When trying to understand the human, LUIS allows the agent to handle ambiguity. The more interactions the agent has, the better the agent will get at understanding a variety of possibly equivalent inputs (as long as any new cases are labeled and the model is retrained periodically).</p>\n<p>The agent has a variety of responses it can give. By varying responses, the agent has a more natural feel and engages the user longer.</p>\n<h2 id=\"kbai-concepts\">KBAI Concepts</h2>\n<p>The agent was designed using concepts primarily from the field of study known as <a href=\"http://www.mkbergman.com/1816/knowledge-based-artificial-intelligence/\">Knowledge-Based Artificial Intelligence</a> (KBAI). Many artificial intelligence concepts are used in the design of the agent, with more on the way.</p>\n<h3 id=\"current-design\">Current Design</h3>\n<h4 id=\"learning-by-recording-cases\">Learning By Recording Cases</h4>\n<p>There are two types of knowledge the agent currently has. The first comes from the knowledge-base (KB) of AI concepts. The second comes from the KB of utterances. Both of these collections of data become cases the agent has learned. Since the AI concepts are indexed, we can consider each search phrase and AI concept pair to be a single case. The process for recording a case includes both extracting the concept name and definition from Wikipedia and then indexing it. Utterances become cases once labeled with an intent. This process is crowdsourced as new utterances are collected from user interactions and then labeled by an administrator.</p>\n<h4 id=\"classification\">Classification</h4>\n<p>Right now the concepts the agent understands are all part of a single class, AI. In future iterations there will be different forms of concepts such as courses and jobs. As of now, the more interesting set of classes is the intents (learn more <a href=\"https://www.luis.ai/Help\">about LUIS</a> to understand the purpose of intents). Each intent is a class. Each utterance is in a unique intent class. If you chat with the agent enough, you will notice that certain words are strongly associated with certain intents. That is because LUIS has found those words to be features of those intents. Since these words are not guaranteed to map to any specific intent, intents can be considered prototypical concepts. That is why intents are assigned to utterances with a particular probability. Though the underlying system is likely a <a href=\"https://en.wikipedia.org/wiki/Statistical_classification\">machine learning system</a>, it shares many similarities with a KBAI system. If LUIS was a KBAI system though, it would be able to explain how it labels each utterance. Instead, it is a black box.</p>\n<h4 id=\"case-based-reasoning\">Case-based Reasoning</h4>\n<p>This agent is an ideal candidate to be a <a href=\"https://en.wikipedia.org/wiki/Case-based_reasoning\">case-based reasoning</a> system. Case-Based Reasoning flows as follows:<br>\n<img src=\"http://cdn.paulprae.com/images/neona/case-based-reasoning.png\" alt=\"Case-based Reasoning\" loading=\"lazy\"><br>\n(Ashok Goel, 2016, <a href=\"https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409\">https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409</a>)</p>\n<p>We will discuss the two different types of cases the agent initially stored when it Learned By Recording Cases.</p>\n<h5 id=\"retrieval\">Retrieval</h5>\n<p>The AI concepts are indexed, retrieved, and ranked by Azure Search as described in the Knowledge Retrieval section of the High-Level Design. Azure Search retrieves data based on exact keywords only. This does not handle ambiguity well but does make for efficient retrieval. For the search mechanism, we can consider the keywords to be the constraints of the problem the agent is trying to solve and the search results to be potential solutions that meet those constraints (i.e. they contain the keywords).</p>\n<h5 id=\"adaptation\">Adaptation</h5>\n<p>Compared to a human learner, the agent is not very good at finding related concepts. The agent needs to learn how to abstract concepts in order to adapt new solutions. Implementing a class hierarchy would help with this. Unlike with searching for AI concepts, the intent detection built into LUIS is highly adaptable. The more interactions it has, the better it gets. It is good, sometimes aggressively so, at matching new utterances it has never seen before with existing intents. This is due to the underlying classification system described above.</p>\n<h5 id=\"evaluation\">Evaluation</h5>\n<p>Evaluation will mostly happen in future phases. For AI concepts, there is a mechanism in place for the user to evaluate whether or not a search result was useful: the save button. A user saving an item to their list could be considered a success. This keyword set and AI concept pair can then be considered a more fit solution. For intents, future enhancements where the agent learns by interaction and making mistakes will be valuable. As it works now, an administrator must observe the conversation the agent had post-mortem and evaluate the utterance labels for accuracy.</p>\n<h5 id=\"storage\">Storage</h5>\n<p>As of now, storage is limited to the same process described in the Learning By Recording Cases section above. Once learning is in place, there will be additional mechanisms for storing new cases. We will see this in the Future Design section.</p>\n<h3 id=\"future-design\">Future Design</h3>\n<p>The full vision of the agent will require more advanced KBAI concepts, particularly in how the agent will learn new concepts and learn the needs of the users. Future features include helping students find appropriate courses for the kind of work they want to do and helping those students find jobs if they are ready. From the perspective of universities and employers, this agent could become a useful recruiting tool.</p>\n<h4 id=\"semantic-networks-and-frames\">Semantic Networks And Frames</h4>\n<p>The purpose of the KB is to store concepts as frames in a <a href=\"https://en.wikipedia.org/wiki/Semantic_network\">semantic network</a>. The current KB is missing the most powerful component of a semantic network: relationships. The concepts do not relate to each other in any explicit way. The KB is no more than a collection of a single class of concepts, namely AI concepts. These AI concepts would benefit from developing a class hierarchy and relating the concepts together in specific ways. For example, you can search for ‘bot’ and find ‘Niki.ai’ but the agent is unaware that ‘Niki.ai’ ‘is-a’ ‘intelligent agent’. Relationships like these would allow the agent to answer questions like ‘What is an example of an intelligent agent?’. It would also greatly improve any classification system in place by formalizing class hierarchies.</p>\n<p>Besides strengthening the relationships between existing concepts, new data sources would allow for new types of relationships that would provide the agent a more robust set of features. Once frames for courses and jobs are in the KB, semantic relationships like “course CS7637 teaches intelligent agents” and “studying intelligent agents is required for a job with Microsoft Research” would be possible.</p>\n<h4 id=\"constraint-propagation\">Constraint Propagation</h4>\n<p>If we approach finding a job for a student as a <a href=\"https://en.wikipedia.org/wiki/Constraint_satisfaction_problem\">constraint satisfaction problem</a>, we limit the search space significantly. Many job descriptions have a ‘requirements’ section. We can consider these the combination of values that satisfy the problem of obtaining a job. The agent can approach the conversation with the student from two ways. One way would be for the agent to discover what the user is capable of and recommend a job. A second way would be to ask the student what job they want and recommend what courses to take to get that job. In the first approach, the agent can eliminate potential jobs by propagating the constraints of the job’s requirements. In the second, the job’s requirements would narrow down the courses the student would need to take to get that job.</p>\n<h4 id=\"learning-by-interaction\">Learning by Interaction</h4>\n<p>No matter how constraints are propagated, the agent will need to learn about the user to provide good recommendations. A user may also be a good source for learning about new concepts, courses, or jobs. For example, if a student is about to graduate and already qualifies for many jobs, this student may be able to teach the agent new concepts or improve existing ones. These use cases lend themselves to an application of Incremental Concept Learning.</p>\n<h5 id=\"incremental-concept-learning\">Incremental Concept Learning</h5>\n<p>The current design of the agent can build off of the simple Learning By Recording Cases method used to populate the AI concepts KB and progress to the more robust Incremental Concept Learning method:<br>\n<img src=\"http://cdn.paulprae.com/images/neona/learning-by-recording-cases.png\" alt=\"Incremental Concept Learning\" loading=\"lazy\"><br>\n(Ashok Goel, 2016, <a href=\"https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409\">https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409</a>)</p>\n<p>Case-based reasoning cooperates advantageously well with incremental concept learning (<a href=\"http://link.springer.com/chapter/10.1007/3-540-60654-8_25\">Bichindaritz 105</a>). Incremental concept learning is able to introduce new cases into the KB that can then be retrieved, adapted, and evaluated. Bichindaritz expresses the integration of these two approaches in the following flowchart:<br>\n<img src=\"http://cdn.paulprae.com/images/neona/functional-architecture-of-the-system.jpg\" alt=\"Functional Flow Chart\" loading=\"lazy\"><br>\nThe main flow that goes down the left-hand side of the diagram maps directly to the case-based reasoning algorithm described earlier. The key new additions are the experimental memory and learning nodes. The students the agent identifies as domain experts can act as teachers for the agent. The agent can extract data it needs through conversation, memorize it, validate it, and store it as a new case.</p>\n<p>The following algorithm details the “memory updating/learning” node from the flowchart above:</p>\n<ol>\n<li>Keep an index of concepts it needs to learn. These can be gathered through searches that led to no results or by hearing from a user that it used a concept incorrectly.</li>\n<li>Identify a user as a domain expert. This can be through the same test the user may go through to see if they qualify for a job.</li>\n<li>Present a concept to the expert and ask for a definition if the user is able and willing. The agent may also ask for new relationships between existing concepts.</li>\n<li>Store the new concept or relationship in experimental memory.</li>\n<li>Identify other domain experts and have them validate this new concept or relationship.</li>\n<li>Once a validation threshold is met, permanently store the concept or relationship as a new case.</li>\n</ol>\n<p>By implementing learning algorithms like the one above, the agent gets better over time without the need for developers to program in new information. It is able to learn from its experience just like a human might. If you'd like to learn more, check out the resources below or check out my presentation titled, \"<a href=\"http://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture\">Azure as a Chatbot Service: From Purpose To Production With A Cloud Bot Architecture</a>\".</p>\n<h2 id=\"works-cited\">Works Cited</h2>\n<ul>\n<li>Bessiere, Christian. Constraint Propagation. Technical Report LIRMM 06020 CNRS. University of Montpellier. March 2006.</li>\n<li>Bichindaritz, Isabelle. Incremental Concept Learning and Case-Based Reasoning: For a Co-Operative Approach. LIAP-5, U.F.R. de Mathematiques et Informatique, Paris, France. 1995. pp 91-106.</li>\n<li>\"Case-based reasoning.\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 9 September 2016. Web. 6 December 2016.</li>\n<li>\"Constraint satisfaction.\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 16 October 2016. Web. 8 December 2016.</li>\n<li>\"Frame (artificial intelligence).\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 7 September 2016. Web. 6 December 2016.</li>\n<li>Goel, Ashok and David Joyner. Knowledge-Based AI: Cognitive Systems by Georgia Tech [Videos]. Udacity. <a href=\"https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409\">https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409</a> Accessed 6 November 2016.</li>\n<li>Norvig, Peter and Stuart Russell. Artificial Intelligence: A Modern Approach. 3rd ed., Pearson, 2010.</li>\n<li>Poole, David and Alan Mackworth. Case-Based Reasoning. Cambridge University Press. artint.info/html/ArtInt_190.html. Accessed 6 December 2016.</li>\n<li>Winston, Patrick. Artificial Intelligence. 3rd ed., Addison-Wesley Publishing Company, 1993.</li>\n</ul>\n<h2 id=\"development-resources\">Development Resources</h2>\n<h3 id=\"tutorials\">Tutorials</h3>\n<ul>\n<li>“Azure Functions NodeJS developer reference.” <a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-node\">https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-node</a></li>\n<li>“Build a Node.js web application using DocumentDB.” <a href=\"https://docs.microsoft.com/en-us/azure/documentdb/documentdb-nodejs-application\">https://docs.microsoft.com/en-us/azure/documentdb/documentdb-nodejs-application</a></li>\n<li>“Get started with Azure Search in the portal.” <a href=\"https://docs.microsoft.com/en-us/azure/search/search-get-started-portal\">https://docs.microsoft.com/en-us/azure/search/search-get-started-portal</a></li>\n<li>“Bot Framework: UniveralBot.” <a href=\"https://docs.botframework.com/en-us/node/builder/chat/UniversalBot/#navtitle\">https://docs.botframework.com/en-us/node/builder/chat/UniversalBot/#navtitle</a></li>\n<li>“Bot Framework: Understanding Natural Language.” <a href=\"https://docs.botframework.com/en-us/node/builder/guides/understanding-natural-language/\">https://docs.botframework.com/en-us/node/builder/guides/understanding-natural-language/</a></li>\n</ul>\n<h3 id=\"code\">Code</h3>\n<ul>\n<li>“Bot Builder for Node.js examples are organized into groups and designed to illustrate the techniques needed to build great bots.” <a href=\"https://docs.botframework.com/en-us/node/builder/guides/examples/\">https://docs.botframework.com/en-us/node/builder/guides/examples/</a></li>\n<li>“These samples illustrate how to approach dialogs that need to help the user navigate large amounts of content, creating a data-driven exploration experience.” <a href=\"https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/demo-Search\">https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/demo-Search</a></li>\n<li>“A sample bot using IntentDialog to integrate with a LUIS.ai application.” <a href=\"https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/intelligence-LUIS\">https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/intelligence-LUIS</a></li>\n</ul>\n",
            "comment_id": "21",
            "plaintext": "Towards the end of last year, I began working on a chat bot to help people understand artificial intelligence. She's named Neona. People interacting with her will learn about the topics students of AI study. In the future, she will be used by students to find courses and jobs in the field of AI. From the perspective of universities and employers, she will be used as a teaching assistant and a recruiting tool. In this post, I'll describe her technical architecture and the artificial intelligence concepts used in her design. An early demo is now available if you'd like to chat with her.\n\n\n\nHigh-Level Design\n\n\nThere are two primary systems that compose the agent. The first is a knowledge system composed of subsystems that acquire, store, and retrieve knowledge. The second is the conversational system that allows the agent to interact with people and reason over its knowledge. There are two technologies that these systems share that allowed the agent to remain cohesive: Microsoft Azure, an “open, flexible, enterprise-grade cloud computing platform”, and Node.js, an “event-driven I/O server-side JavaScript environment”. These were chosen because of the potential of the Microsoft Bot Framework, which was the main driver behind this architecture.\n\n\n\nThe Knowledge-Base\n\n\nThe current knowledge-base (KB) consists of AI concepts pulled from Wikipedia and stored in a document store. In the future, the KB will also consist of users that interact with the agent, courses from online programs, and jobs from career websites.\n\n\nThe overall process was to define the data the agent needed, extract it from Wikipedia, and store it in the document store where the agent can retrieve it.\n\n\nHere are the technologies that were implemented to put this KB into production:\n\n\n * DocumentDB: “A distributed database service for managing JSON documents at Internet scale.” It is a highly-flexible key-value store that integrates closely with other Microsoft systems and interfaces well with Node.js.\n * Azure Functions: “Process events with a serverless code architecture.” This service allowed several small Node.js modules to run in the cloud. These modules were responsible for extracting data from Wikipedia, processing it into the desired form, and storing it in DocumentDB.\n * MediaWiki action API: “A web service that provides convenient access to wiki features, data, and meta-data over HTTP.” Using Node.js’ request module and JavaScript Promises, the API is called with carefully constructed queries that return just the data the agent would need.\n * Azure Search: “A fully managed search-as-a-service in the cloud.” It indexed the contents of the KB and provided an HTTP endpoint the agent can hit to query the KB. It natively supports DocumentDB.\n\n\nThe Knowledge Source\n\n\nAll data was extracted from Wikipedia, starting at the node https://en.wikipedia.org/wiki/Category:Artificial_intelligence. The agent made an attempt to learn concepts from every page and several subcategories from that root category. There is a one-to-one relationship between a Wikipedia page and a concept in the KB. If any issues were encountered while scraping a given page, such as missing data, the page was skipped.\n\n\nHere are some examples of the final queries developed:\n\n\n * Get as many as 500 page ids from a given category: https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmlimit=500&cmtype=page&cmprop=ids&cmtitle=Category:Artificial_intelligence\n * Get up to 20 pages worth of data given a list of page ids: https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=20&exintro=&explaintext=&indexpageids=&pageids=51023476|1164|17188\n\n\nFeel free to try either of those out in a browser to see the results. They were built by referencing the WikiMedia module Categorymembers and the extension Extracts, respectively.\n\n\nKnowledge Acquisition\n\n\nThe WikipediaCategoryToAIConcepts Azure Function takes in a Wikipedia category and attempts to transform all of its child pages into JSON documents that represent concepts in the KB. The InsertAIConcept Azure Function takes in a single JSON document and inserts it into the KB as long as the concept it represents is not already stored in the KB. These functions are built in a way that they could be ran on a schedule and update the KB as the Wikipedia category and pages are updated. The code for this part of the project can be found on GitHub at https://github.com/praeducer/conversational-agent-functions. This repo is continuously integrated with the production agent.\n\n\nKnowledge Representation\n\n\nThe concepts are stored in DocumentDB as JSON objects.\n\n\nHere is an example document:\n\n{ \"source\": { \"name\": \"wikipedia\", \"accessedDate\": 1479691542482, \"scriptVersion\": \"0.0.1\", \"pageid\": 10136 }, \"active\": true, \"title\": \"Expert system\", \"extract\": \"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\", \"id\": \"a958db8e-7160-42f4-b3a8-388d2726ba0e\" }\n\n\nThe source is designed so that the agent can learn concepts from more than just Wikipedia. Because this is built in DocumentDB, this schema can change easily if new sources provide new information.\n\n\nKnowledge Retrieval\n\n\nThe agent uses Azure Search to efficiently retrieve concepts from the KB. The KB is indexed nightly so any documents requested are up-to-date. So far, 811 AI concepts are ready for retrieval:\n\n\n\n\nThe ‘title’ and the ‘extract’ of each concept is indexed, making them “searchable”:\n\n\n\n\nResults are queried using URLs like: https://contoso.search.windows.net/indexes/aiconcept/docs?api-version=2015-02-28&search=expert system\n\n\nWhich can return a ranked list of results like:\n\n[ { \"@search.score\": 4.4751096, \"title\": \"Expert system\", \"extract\": \"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\", \"id\": \"a958db8e-7160-42f4-b3a8-388d2726ba0e\" }, { \"@search.score\": 3.9047346, \"title\": \"Legal expert system\", \"extract\": \"A legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law. Legal expert systems employ a rule base or knowledge base and an inference engine to accumulate, reference and produce expert knowledge on specific subjects within the legal domain.\\n\\n\", \"id\": \"d1f14975-32cd-4243-bdc1-b00d82273092\" } ]\n\n\n\nThe Conversational Agent\n\n\nThe agent’s primary purpose and most robust feature is its ability to search for AI concepts from the KB. It is also able to handle simple conversational components such as ‘Hello’, ‘Goodbye’, ‘How are you?’, ‘Who are you?’, ‘Thank You’, ‘You’re Welcome’, and can even tell some jokes. It is currently tested on Skype but theoretically would work as-is on several other channels including Facebook Messenger and a web client. Connecting to a variety of channels from a single code-base is a key benefit of the Microsoft Bot Framework.\n\n\nHere are the technologies used to put this conversational agent into production:\n\n\n * Microsoft Bot Framework: This framework provided an impressive amount of functionality and was the inspiration for this project. For this agent, it provided the connection to Skype, managed incoming and outgoing messages, and helped orchestrate the dialogue flow. It also integrated naturally with LUIS.\n * LUIS: “A fast and effective way of adding language understanding to applications.” After some manual training, LUIS was used to detect the intent of human messages. Then, after some logical routing, the agent could return a reasonable response.\n * Azure Bot Service: “Intelligent, serverless bot service that scales on demand.” After setting up continuous integration, this service turned the development repository into a production application. It acted as the DevOps team.\n\n\nThe code for these components lives on GitHub at https://github.com/praeducer/conversational-agent.\n\n\nDetecting And Acting Upon Intent\n\n\nThe agent has one main or root dialogue which can route the user to several smaller sub-dialogues. The root dialogue is very wide, handling many different kinds of incoming messages. The sub-dialogues are more narrow, doing very specific things but sometimes going a little deeper than the root dialogue would go. When the human first engages with the agent, it introduces itself. After the initial introduction, any incoming messages from the human are routed to LUIS. LUIS then recognizes what the human is trying to say and maps it to an intent. LUIS was manually trained to handle sixteen different intents. Each intent has a set of functionality associated with it that can range from a simple text response to constructing UI elements that are displayed in the chat window. Some intents even start new dialogues and ask the user questions in order to gather more information.\n\n\nCustom intents are defined in LUIS and then referenced in code. LUIS uses a classifier to map utterances, i.e. things a human may say, to intents. To seed the system, utterances were entered in manually and tagged with intents. Once LUIS had a solid set of initial data, the model was trained. As humans use the system, an administrator must go in and manually label any utterances LUIS had difficulties classifying and retrain the model.\n\n\nHere are some examples of utterances, how LUIS labeled them, and LUIS’s confidence score:\n\n\n\n\nAs you can see, it was very confident when labeling the SearchConcept intent. LUIS’s Thanks label was correct as well, even though its confidence score was low. It won because its confidence score is proportionately much higher than the rest (note that a drop down can be used to correct the label if needed):\n\n\n\n\nThe Dialogue\n\n\nEach intent the agent is programmed to handle leads to a new dialogue. From that dialogue, the human can say things that take it deeper into another child dialogue or take it back into the parent dialogue.\n\n\nHere are the more important intents the agent handles and what they mean:\n\n\n * Hello: Responds to things like ‘Hey’ or ‘Hi’ with a welcoming message and some instructions.\n * SearchConcept: Looks up AI concepts for the user. The agent can detect things like ‘find’ or ‘search’ and extract the concept it needs to look up.\n * More: Displays more search results.\n * List: Displays any concepts the user has saved.\n * HowAreYou?: Responds to questions like “How are you?” with a friendly response.\n * Sorry: If the human gets frustrated, sometimes the agent notices and can apologize.\n * Help: Provides instructions to the user if they seem confused or explicitly asks for help.\n * Joke: Tells the human a random joke from its KB.\n\n\nWhen trying to understand the human, LUIS allows the agent to handle ambiguity. The more interactions the agent has, the better the agent will get at understanding a variety of possibly equivalent inputs (as long as any new cases are labeled and the model is retrained periodically).\n\n\nThe agent has a variety of responses it can give. By varying responses, the agent has a more natural feel and engages the user longer.\n\n\n\nKBAI Concepts\n\n\nThe agent was designed using concepts primarily from the field of study known as Knowledge-Based Artificial Intelligence (KBAI). Many artificial intelligence concepts are used in the design of the agent, with more on the way.\n\n\n\nCurrent Design\n\n\nLearning By Recording Cases\n\n\nThere are two types of knowledge the agent currently has. The first comes from the knowledge-base (KB) of AI concepts. The second comes from the KB of utterances. Both of these collections of data become cases the agent has learned. Since the AI concepts are indexed, we can consider each search phrase and AI concept pair to be a single case. The process for recording a case includes both extracting the concept name and definition from Wikipedia and then indexing it. Utterances become cases once labeled with an intent. This process is crowdsourced as new utterances are collected from user interactions and then labeled by an administrator.\n\n\nClassification\n\n\nRight now the concepts the agent understands are all part of a single class, AI. In future iterations there will be different forms of concepts such as courses and jobs. As of now, the more interesting set of classes is the intents (learn more about LUIS to understand the purpose of intents). Each intent is a class. Each utterance is in a unique intent class. If you chat with the agent enough, you will notice that certain words are strongly associated with certain intents. That is because LUIS has found those words to be features of those intents. Since these words are not guaranteed to map to any specific intent, intents can be considered prototypical concepts. That is why intents are assigned to utterances with a particular probability. Though the underlying system is likely a machine learning system, it shares many similarities with a KBAI system. If LUIS was a KBAI system though, it would be able to explain how it labels each utterance. Instead, it is a black box.\n\n\nCase-based Reasoning\n\n\nThis agent is an ideal candidate to be a case-based reasoning system. Case-Based Reasoning flows as follows:\n\n\n\n(Ashok Goel, 2016, https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409)\n\n\nWe will discuss the two different types of cases the agent initially stored when it Learned By Recording Cases.\n\n\nRetrieval\n\n\nThe AI concepts are indexed, retrieved, and ranked by Azure Search as described in the Knowledge Retrieval section of the High-Level Design. Azure Search retrieves data based on exact keywords only. This does not handle ambiguity well but does make for efficient retrieval. For the search mechanism, we can consider the keywords to be the constraints of the problem the agent is trying to solve and the search results to be potential solutions that meet those constraints (i.e. they contain the keywords).\n\n\nAdaptation\n\n\nCompared to a human learner, the agent is not very good at finding related concepts. The agent needs to learn how to abstract concepts in order to adapt new solutions. Implementing a class hierarchy would help with this. Unlike with searching for AI concepts, the intent detection built into LUIS is highly adaptable. The more interactions it has, the better it gets. It is good, sometimes aggressively so, at matching new utterances it has never seen before with existing intents. This is due to the underlying classification system described above.\n\n\nEvaluation\n\n\nEvaluation will mostly happen in future phases. For AI concepts, there is a mechanism in place for the user to evaluate whether or not a search result was useful: the save button. A user saving an item to their list could be considered a success. This keyword set and AI concept pair can then be considered a more fit solution. For intents, future enhancements where the agent learns by interaction and making mistakes will be valuable. As it works now, an administrator must observe the conversation the agent had post-mortem and evaluate the utterance labels for accuracy.\n\n\nStorage\n\n\nAs of now, storage is limited to the same process described in the Learning By Recording Cases section above. Once learning is in place, there will be additional mechanisms for storing new cases. We will see this in the Future Design section.\n\n\n\nFuture Design\n\n\nThe full vision of the agent will require more advanced KBAI concepts, particularly in how the agent will learn new concepts and learn the needs of the users. Future features include helping students find appropriate courses for the kind of work they want to do and helping those students find jobs if they are ready. From the perspective of universities and employers, this agent could become a useful recruiting tool.\n\n\nSemantic Networks And Frames\n\n\nThe purpose of the KB is to store concepts as frames in a semantic network. The current KB is missing the most powerful component of a semantic network: relationships. The concepts do not relate to each other in any explicit way. The KB is no more than a collection of a single class of concepts, namely AI concepts. These AI concepts would benefit from developing a class hierarchy and relating the concepts together in specific ways. For example, you can search for ‘bot’ and find ‘Niki.ai’ but the agent is unaware that ‘Niki.ai’ ‘is-a’ ‘intelligent agent’. Relationships like these would allow the agent to answer questions like ‘What is an example of an intelligent agent?’. It would also greatly improve any classification system in place by formalizing class hierarchies.\n\n\nBesides strengthening the relationships between existing concepts, new data sources would allow for new types of relationships that would provide the agent a more robust set of features. Once frames for courses and jobs are in the KB, semantic relationships like “course CS7637 teaches intelligent agents” and “studying intelligent agents is required for a job with Microsoft Research” would be possible.\n\n\nConstraint Propagation\n\n\nIf we approach finding a job for a student as a constraint satisfaction problem, we limit the search space significantly. Many job descriptions have a ‘requirements’ section. We can consider these the combination of values that satisfy the problem of obtaining a job. The agent can approach the conversation with the student from two ways. One way would be for the agent to discover what the user is capable of and recommend a job. A second way would be to ask the student what job they want and recommend what courses to take to get that job. In the first approach, the agent can eliminate potential jobs by propagating the constraints of the job’s requirements. In the second, the job’s requirements would narrow down the courses the student would need to take to get that job.\n\n\nLearning by Interaction\n\n\nNo matter how constraints are propagated, the agent will need to learn about the user to provide good recommendations. A user may also be a good source for learning about new concepts, courses, or jobs. For example, if a student is about to graduate and already qualifies for many jobs, this student may be able to teach the agent new concepts or improve existing ones. These use cases lend themselves to an application of Incremental Concept Learning.\n\n\nIncremental Concept Learning\n\n\nThe current design of the agent can build off of the simple Learning By Recording Cases method used to populate the AI concepts KB and progress to the more robust Incremental Concept Learning method:\n\n\n\n(Ashok Goel, 2016, https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409)\n\n\nCase-based reasoning cooperates advantageously well with incremental concept learning (Bichindaritz 105). Incremental concept learning is able to introduce new cases into the KB that can then be retrieved, adapted, and evaluated. Bichindaritz expresses the integration of these two approaches in the following flowchart:\n\n\n\nThe main flow that goes down the left-hand side of the diagram maps directly to the case-based reasoning algorithm described earlier. The key new additions are the experimental memory and learning nodes. The students the agent identifies as domain experts can act as teachers for the agent. The agent can extract data it needs through conversation, memorize it, validate it, and store it as a new case.\n\n\nThe following algorithm details the “memory updating/learning” node from the flowchart above:\n\n\n 1. Keep an index of concepts it needs to learn. These can be gathered through searches that led to no results or by hearing from a user that it used a concept incorrectly.\n 2. Identify a user as a domain expert. This can be through the same test the user may go through to see if they qualify for a job.\n 3. Present a concept to the expert and ask for a definition if the user is able and willing. The agent may also ask for new relationships between existing concepts.\n 4. Store the new concept or relationship in experimental memory.\n 5. Identify other domain experts and have them validate this new concept or relationship.\n 6. Once a validation threshold is met, permanently store the concept or relationship as a new case.\n\n\nBy implementing learning algorithms like the one above, the agent gets better over time without the need for developers to program in new information. It is able to learn from its experience just like a human might. If you'd like to learn more, check out the resources below or check out my presentation titled, \"Azure as a Chatbot Service: From Purpose To Production With A Cloud Bot Architecture\".\n\n\n\nWorks Cited\n\n\n * Bessiere, Christian. Constraint Propagation. Technical Report LIRMM 06020 CNRS. University of Montpellier. March 2006.\n * Bichindaritz, Isabelle. Incremental Concept Learning and Case-Based Reasoning: For a Co-Operative Approach. LIAP-5, U.F.R. de Mathematiques et Informatique, Paris, France. 1995. pp 91-106.\n * \"Case-based reasoning.\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 9 September 2016. Web. 6 December 2016.\n * \"Constraint satisfaction.\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 16 October 2016. Web. 8 December 2016.\n * \"Frame (artificial intelligence).\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 7 September 2016. Web. 6 December 2016.\n * Goel, Ashok and David Joyner. Knowledge-Based AI: Cognitive Systems by Georgia Tech [Videos]. Udacity. https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409 Accessed 6 November 2016.\n * Norvig, Peter and Stuart Russell. Artificial Intelligence: A Modern Approach. 3rd ed., Pearson, 2010.\n * Poole, David and Alan Mackworth. Case-Based Reasoning. Cambridge University Press. artint.info/html/ArtInt_190.html. Accessed 6 December 2016.\n * Winston, Patrick. Artificial Intelligence. 3rd ed., Addison-Wesley Publishing Company, 1993.\n\n\n\nDevelopment Resources\n\n\n\nTutorials\n\n\n * “Azure Functions NodeJS developer reference.” https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-node\n * “Build a Node.js web application using DocumentDB.” https://docs.microsoft.com/en-us/azure/documentdb/documentdb-nodejs-application\n * “Get started with Azure Search in the portal.” https://docs.microsoft.com/en-us/azure/search/search-get-started-portal\n * “Bot Framework: UniveralBot.” https://docs.botframework.com/en-us/node/builder/chat/UniversalBot/#navtitle\n * “Bot Framework: Understanding Natural Language.” https://docs.botframework.com/en-us/node/builder/guides/understanding-natural-language/\n\n\n\nCode\n\n\n * “Bot Builder for Node.js examples are organized into groups and designed to illustrate the techniques needed to build great bots.” https://docs.botframework.com/en-us/node/builder/guides/examples/\n * “These samples illustrate how to approach dialogs that need to help the user navigate large amounts of content, creating a data-driven exploration experience.” https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/demo-Search\n * “A sample bot using IntentDialog to integrate with a LUIS.ai application.” https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/intelligence-LUIS\n",
            "feature_image": "__GHOST_URL__/content/images/2018/03/neona_face.jpg",
            "featured": 1,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2017-01-15T01:42:38.000Z",
            "updated_at": "2025-08-20T19:57:15.000Z",
            "published_at": "2017-01-15T03:40:48.000Z",
            "custom_excerpt": "Towards the end of last year, I began working on a chat bot to help people understand artificial intelligence. By studying her design and architecture, you can learn how to build conversational agents.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": "6270fc10d5472a0031f5f827",
            "lexical": "{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"Towards the end of last year, I began working on a chat bot to help people understand artificial intelligence. She's named Neona. People interacting with her will learn about the topics students of AI study. In the future, she will be used by students to find courses and jobs in the field of AI. From the perspective of universities and employers, she will be used as a teaching assistant and a recruiting tool. In this post, I'll describe her technical architecture and the artificial intelligence concepts used in her design. [An early demo is now available](http://www.neona.chat/) if you'd like to chat with her.\\n\\n## High-Level Design\\nThere are two primary systems that compose the agent. The first is a knowledge system composed of subsystems that acquire, store, and retrieve knowledge. The second is the conversational system that allows the agent to interact with people and reason over its knowledge. There are two technologies that these systems share that allowed the agent to remain cohesive: [Microsoft Azure](https://azure.microsoft.com/), an “open, flexible, enterprise-grade cloud computing platform”, and [Node.js](https://nodejs.org/en/), an “event-driven I/O server-side JavaScript environment”. These were chosen because of the potential of the [Microsoft Bot Framework](https://dev.botframework.com/), which was the main driver behind this architecture. \\n\\n### The Knowledge-Base \\nThe current knowledge-base (KB) consists of AI concepts pulled from Wikipedia and stored in a document store. In the future, the KB will also consist of users that interact with the agent, courses from online programs, and jobs from career websites.\\n\\nThe overall process was to define the data the agent needed, extract it from Wikipedia, and store it in the document store where the agent can retrieve it.\\n\\nHere are the technologies that were implemented to put this KB into production:\\n\\n* [DocumentDB](https://azure.microsoft.com/en-us/services/documentdb/): “A distributed database service for managing JSON documents at Internet scale.” It is a highly-flexible key-value store that integrates closely with other Microsoft systems and interfaces well with Node.js. \\n* [Azure Functions](https://azure.microsoft.com/en-us/services/functions/): “Process events with a serverless code architecture.” This service allowed several small Node.js modules to run in the cloud. These modules were responsible for extracting data from Wikipedia, processing it into the desired form, and storing it in DocumentDB. \\n* [MediaWiki action API](https://www.mediawiki.org/wiki/API:Main_page): “A web service that provides convenient access to wiki features, data, and meta-data over HTTP.” Using Node.js’ [request](https://www.npmjs.com/package/request) module and JavaScript [Promises](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise), the API is called with carefully constructed queries that return just the data the agent would need. \\n* [Azure Search](https://azure.microsoft.com/en-us/services/search/): “A fully managed search-as-a-service in the cloud.” It indexed the contents of the KB and provided an HTTP endpoint the agent can hit to query the KB. It natively supports DocumentDB.\\n\\n#### The Knowledge Source \\nAll data was extracted from Wikipedia, starting at the node https://en.wikipedia.org/wiki/Category:Artificial_intelligence. The agent made an attempt to learn concepts from every page and several subcategories from that root category. There is a one-to-one relationship between a Wikipedia page and a concept in the KB. If any issues were encountered while scraping a given page, such as missing data, the page was skipped. \\n\\nHere are some examples of the final queries developed: \\n\\n* Get as many as 500 page ids from a given category: https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmlimit=500&cmtype=page&cmprop=ids&cmtitle=Category:Artificial_intelligence \\n* Get up to 20 pages worth of data given a list of page ids: https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=20&exintro=&explaintext=&indexpageids=&pageids=51023476|1164|17188\\n\\nFeel free to try either of those out in a browser to see the results. They were built by referencing the WikiMedia module [Categorymembers](https://www.mediawiki.org/wiki/API:Categorymembers) and the extension [Extracts](https://www.mediawiki.org/wiki/Extension:TextExtracts), respectively.\\n#### Knowledge Acquisition \\nThe WikipediaCategoryToAIConcepts Azure Function takes in a Wikipedia category and attempts to transform all of its child pages into JSON documents that represent concepts in the KB. The InsertAIConcept Azure Function takes in a single JSON document and inserts it into the KB as long as the concept it represents is not already stored in the KB. These functions are built in a way that they could be ran on a schedule and update the KB as the Wikipedia category and pages are updated. The code for this part of the project can be found on GitHub at https://github.com/praeducer/conversational-agent-functions. This repo is continuously integrated with the production agent.\\n#### Knowledge Representation \\nThe concepts are stored in DocumentDB as JSON objects.\\n\\nHere is an example document:\\n`{ \\n  \\\"source\\\": { \\n    \\\"name\\\": \\\"wikipedia\\\", \\n    \\\"accessedDate\\\": 1479691542482, \\n    \\\"scriptVersion\\\": \\\"0.0.1\\\", \\n    \\\"pageid\\\": 10136 \\n  }, \\n  \\\"active\\\": true, \\n  \\\"title\\\": \\\"Expert system\\\", \\n  \\\"extract\\\": \\\"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\\\\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\\\", \\n  \\\"id\\\": \\\"a958db8e-7160-42f4-b3a8-388d2726ba0e\\\" \\n}`\\n\\nThe source is designed so that the agent can learn concepts from more than just Wikipedia. Because this is built in DocumentDB, this schema can change easily if new sources provide new information.\\n#### Knowledge Retrieval \\nThe agent uses Azure Search to efficiently retrieve concepts from the KB. The KB is indexed nightly so any documents requested are up-to-date. So far, 811 AI concepts are ready for retrieval:\\n![Azure Search Usage](http://cdn.paulprae.com/images/neona/azure-search-usage.png)\\n\\nThe ‘title’ and the ‘extract’ of each concept is indexed, making them “searchable”:\\n![Azure Search Field](http://cdn.paulprae.com/images/neona/azure-search-fields.png)\\n\\nResults are queried using URLs like: https://contoso.search.windows.net/indexes/aiconcept/docs?api-version=2015-02-28&search=expert%20system \\n\\nWhich can return a ranked list of results like:\\n`[ \\n        { \\n            \\\"@search.score\\\": 4.4751096, \\n            \\\"title\\\": \\\"Expert system\\\", \\n            \\\"extract\\\": \\\"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\\\\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\\\", \\n            \\\"id\\\": \\\"a958db8e-7160-42f4-b3a8-388d2726ba0e\\\" \\n        }, \\n        { \\n            \\\"@search.score\\\": 3.9047346, \\n            \\\"title\\\": \\\"Legal expert system\\\", \\n            \\\"extract\\\": \\\"A legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law. Legal expert systems employ a rule base or knowledge base and an inference engine to accumulate, reference and produce expert knowledge on specific subjects within the legal domain.\\\\n\\\\n\\\", \\n            \\\"id\\\": \\\"d1f14975-32cd-4243-bdc1-b00d82273092\\\" \\n        } \\n]`\\n### The Conversational Agent \\nThe agent’s primary purpose and most robust feature is its ability to search for AI concepts from the KB. It is also able to handle simple conversational components such as ‘Hello’, ‘Goodbye’, ‘How are you?’, ‘Who are you?’, ‘Thank You’, ‘You’re Welcome’, and can even tell some jokes. It is currently tested on Skype but theoretically would work as-is on several other channels including Facebook Messenger and a web client. Connecting to a variety of channels from a single code-base is a key benefit of the Microsoft Bot Framework.\\n\\nHere are the technologies used to put this conversational agent into production: \\n\\n* [Microsoft Bot Framework](https://dev.botframework.com/): This framework provided an impressive amount of functionality and was the inspiration for this project. For this agent, it provided the connection to Skype, managed incoming and outgoing messages, and helped orchestrate the dialogue flow. It also integrated naturally with LUIS. \\n* [LUIS](https://www.luis.ai/): “A fast and effective way of adding language understanding to applications.” After some manual training, LUIS was used to detect the intent of human messages. Then, after some logical routing, the agent could return a reasonable response. \\n* [Azure Bot Service](https://azure.microsoft.com/en-us/services/bot-service/): “Intelligent, serverless bot service that scales on demand.” After setting up continuous integration, this service turned the development repository into a production application. It acted as the DevOps team.\\n\\nThe code for these components lives on GitHub at https://github.com/praeducer/conversational-agent.\\n#### Detecting And Acting Upon Intent \\nThe agent has one main or root dialogue which can route the user to several smaller sub-dialogues. The root dialogue is very wide, handling many different kinds of incoming messages. The sub-dialogues are more narrow, doing very specific things but sometimes going a little deeper than the root dialogue would go. When the human first engages with the agent, it introduces itself. After the initial introduction, any incoming messages from the human are routed to LUIS. LUIS then recognizes what the human is trying to say and maps it to an intent. LUIS was manually trained to handle sixteen different intents. Each intent has a set of functionality associated with it that can range from a simple text response to constructing UI elements that are displayed in the chat window. Some intents even start new dialogues and ask the user questions in order to gather more information. \\n\\nCustom intents are defined in LUIS and then referenced in code. LUIS uses a classifier to map utterances, i.e. things a human may say, to intents. To seed the system, utterances were entered in manually and tagged with intents. Once LUIS had a solid set of initial data, the model was trained. As humans use the system, an administrator must go in and manually label any utterances LUIS had difficulties classifying and retrain the model.\\n\\nHere are some examples of utterances, how LUIS labeled them, and LUIS’s confidence score:\\n![LUIS Utterances](http://cdn.paulprae.com/images/neona/LUIS-utterances.png)\\n\\nAs you can see, it was very confident when labeling the SearchConcept intent. LUIS’s Thanks label was correct as well, even though its confidence score was low. It won because its confidence score is proportionately much higher than the rest (note that a drop down can be used to correct the label if  needed):\\n![LUIS Labels](http://cdn.paulprae.com/images/neona/LUIS-labels.jpg)\\n#### The Dialogue \\nEach intent the agent is programmed to handle leads to a new dialogue. From that dialogue, the human can say things that take it deeper into another child dialogue or take it back into the parent dialogue.\\n\\nHere are the more important intents the agent handles and what they mean:\\n\\n* Hello: Responds to things like ‘Hey’ or ‘Hi’ with a welcoming message and some instructions. \\n* SearchConcept: Looks up AI concepts for the user. The agent can detect things like ‘find’ or ‘search’ and extract the concept it needs to look up. \\n* More: Displays more search results. \\n* List: Displays any concepts the user has saved. \\n* HowAreYou?: Responds to questions like “How are you?” with a friendly response. \\n* Sorry: If the human gets frustrated, sometimes the agent notices and can apologize.  \\n* Help: Provides instructions to the user if they seem confused or explicitly asks for help. \\n* Joke: Tells the human a random joke from its KB.\\n\\nWhen trying to understand the human, LUIS allows the agent to handle ambiguity. The more interactions the agent has, the better the agent will get at understanding a variety of possibly equivalent inputs (as long as any new cases are labeled and the model is retrained periodically).\\n \\nThe agent has a variety of responses it can give. By varying responses, the agent has a more natural feel and engages the user longer.\\n## KBAI Concepts \\nThe agent was designed using concepts primarily from the field of study known as [Knowledge-Based Artificial Intelligence](http://www.mkbergman.com/1816/knowledge-based-artificial-intelligence/) (KBAI). Many artificial intelligence concepts are used in the design of the agent, with more on the way.\\n### Current Design\\n#### Learning By Recording Cases \\nThere are two types of knowledge the agent currently has. The first comes from the knowledge-base (KB) of AI concepts. The second comes from the KB of utterances. Both of these collections of data become cases the agent has learned. Since the AI concepts are indexed, we can consider each search phrase and AI concept pair to be a single case. The process for recording a case includes both extracting the concept name and definition from Wikipedia and then indexing it. Utterances become cases once labeled with an intent. This process is crowdsourced as new utterances are collected from user interactions and then labeled by an administrator.\\n#### Classification \\nRight now the concepts the agent understands are all part of a single class, AI. In future iterations there will be different forms of concepts such as courses and jobs. As of now, the more interesting set of classes is the intents (learn more [about LUIS](https://www.luis.ai/Help) to understand the purpose of intents). Each intent is a class. Each utterance is in a unique intent class. If you chat with the agent enough, you will notice that certain words are strongly associated with certain intents. That is because LUIS has found those words to be features of those intents. Since these words are not guaranteed to map to any specific intent, intents can be considered prototypical concepts. That is why intents are assigned to utterances with a particular probability. Though the underlying system is likely a [machine learning system](https://en.wikipedia.org/wiki/Statistical_classification), it shares many similarities with a KBAI system. If LUIS was a KBAI system though, it would be able to explain how it labels each utterance. Instead, it is a black box.\\n#### Case-based Reasoning \\nThis agent is an ideal candidate to be a [case-based reasoning](https://en.wikipedia.org/wiki/Case-based_reasoning) system. Case-Based Reasoning flows as follows:\\n![Case-based Reasoning](http://cdn.paulprae.com/images/neona/case-based-reasoning.png)\\n(Ashok Goel, 2016, https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409)  \\n\\nWe will discuss the two different types of cases the agent initially stored when it Learned By Recording Cases.\\n##### Retrieval \\nThe AI concepts are indexed, retrieved, and ranked by Azure Search as described in the Knowledge Retrieval section of the High-Level Design. Azure Search retrieves data based on exact keywords only. This does not handle ambiguity well but does make for efficient retrieval. For the search mechanism, we can consider the keywords to be the constraints of the problem the agent is trying to solve and the search results to be potential solutions that meet those constraints (i.e. they contain the keywords).\\n##### Adaptation \\nCompared to a human learner, the agent is not very good at finding related concepts. The agent needs to learn how to abstract concepts in order to adapt new solutions. Implementing a class hierarchy would help with this. Unlike with searching for AI concepts, the intent detection built into LUIS is highly adaptable. The more interactions it has, the better it gets. It is good, sometimes aggressively so, at matching new utterances it has never seen before with existing intents. This is due to the underlying classification system described above.\\n##### Evaluation \\nEvaluation will mostly happen in future phases. For AI concepts, there is a mechanism in place for the user to evaluate whether or not a search result was useful: the save button. A user saving an item to their list could be considered a success. This keyword set and AI concept pair can then be considered a more fit solution. For intents, future enhancements where the agent learns by interaction and making mistakes will be valuable. As it works now, an administrator must observe the conversation the agent had post-mortem and evaluate the utterance labels for accuracy.\\n##### Storage \\nAs of now, storage is limited to the same process described in the Learning By Recording Cases section above. Once learning is in place, there will be additional mechanisms for storing new cases. We will see this in the Future Design section.\\n### Future Design \\nThe full vision of the agent will require more advanced KBAI concepts, particularly in how the agent will learn new concepts and learn the needs of the users. Future features include helping students find appropriate courses for the kind of work they want to do and helping those students find jobs if they are ready. From the perspective of universities and employers, this agent could become a useful recruiting tool.\\n#### Semantic Networks And Frames \\nThe purpose of the KB is to store concepts as frames in a [semantic network](https://en.wikipedia.org/wiki/Semantic_network). The current KB is missing the most powerful component of a semantic network: relationships. The concepts do not relate to each other in any explicit way. The KB is no more than a collection of a single class of concepts, namely AI concepts. These AI concepts would benefit from developing a class hierarchy and relating the concepts together in specific ways. For example, you can search for ‘bot’ and find ‘Niki.ai’ but the agent is unaware that ‘Niki.ai’ ‘is-a’ ‘intelligent agent’. Relationships like these would allow the agent to answer questions like ‘What is an example of an intelligent agent?’. It would also greatly improve any classification system in place by formalizing class hierarchies. \\n \\nBesides strengthening the relationships between existing concepts, new data sources would allow for new types of relationships that would provide the agent a more robust set of features. Once frames for courses and jobs are in the KB, semantic relationships like “course CS7637 teaches intelligent agents” and “studying intelligent agents is required for a job with Microsoft Research” would be possible.\\n#### Constraint Propagation \\nIf we approach finding a job for a student as a [constraint satisfaction problem](https://en.wikipedia.org/wiki/Constraint_satisfaction_problem), we limit the search space significantly. Many job descriptions have a ‘requirements’ section. We can consider these the combination of values that satisfy the problem of obtaining a job. The agent can approach the conversation with the student from two ways. One way would be for the agent to discover what the user is capable of and recommend a job. A second way would be to ask the student what job they want and recommend what courses to take to get that job. In the first approach, the agent can eliminate potential jobs by propagating the constraints of the job’s requirements. In the second, the job’s requirements would narrow down the courses the student would need to take to get that job.\\n#### Learning by Interaction  \\nNo matter how constraints are propagated, the agent will need to learn about the user to provide good recommendations. A user may also be a good source for learning about new concepts, courses, or jobs. For example, if a student is about to graduate and already qualifies for many jobs, this student may be able to teach the agent new concepts or improve existing ones. These use cases lend themselves to an application of Incremental Concept Learning.\\n##### Incremental Concept Learning\\nThe current design of the agent can build off of the simple Learning By Recording Cases method used to populate the AI concepts KB and progress to the more robust Incremental Concept Learning method:\\n![Incremental Concept Learning](http://cdn.paulprae.com/images/neona/learning-by-recording-cases.png)\\n(Ashok Goel, 2016, https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409)  \\n\\nCase-based reasoning cooperates advantageously well with incremental concept learning ([Bichindaritz 105](http://link.springer.com/chapter/10.1007/3-540-60654-8_25)). Incremental concept learning is able to introduce new cases into the KB that can then be retrieved, adapted, and evaluated. Bichindaritz expresses the integration of these two approaches in the following flowchart:\\n![Functional Flow Chart](http://cdn.paulprae.com/images/neona/functional-architecture-of-the-system.jpg)\\nThe main flow that goes down the left-hand side of the diagram maps directly to the case-based reasoning algorithm described earlier. The key new additions are the experimental memory and learning nodes. The students the agent identifies as domain experts can act as teachers for the agent. The agent can extract data it needs through conversation, memorize it, validate it, and store it as a new case.\\n\\nThe following algorithm details the “memory updating/learning” node from the flowchart above:\\n1. Keep an index of concepts it needs to learn. These can be gathered through searches that led to no results or by hearing from a user that it used a concept incorrectly. \\n2. Identify a user as a domain expert. This can be through the same test the user may go through to see if they qualify for a job. \\n3. Present a concept to the expert and ask for a definition if the user is able and willing. The agent may also ask for new relationships between existing concepts. \\n4. Store the new concept or relationship in experimental memory. \\n5. Identify other domain experts and have them validate this new concept or relationship. \\n6. Once a validation threshold is met, permanently store the concept or relationship as a new case.\\n\\nBy implementing learning algorithms like the one above, the agent gets better over time without the need for developers to program in new information. It is able to learn from its experience just like a human might. If you'd like to learn more, check out the resources below or check out my presentation titled, \\\"[Azure as a Chatbot Service: From Purpose To Production With A Cloud Bot Architecture](http://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture)\\\".\\n\\n## Works Cited\\n* Bessiere, Christian. Constraint Propagation. Technical Report LIRMM 06020 CNRS. University of Montpellier. March 2006. \\n* Bichindaritz, Isabelle. Incremental Concept Learning and Case-Based Reasoning: For a Co-Operative Approach. LIAP-5, U.F.R. de Mathematiques et Informatique, Paris, France. 1995. pp 91-106. \\n* \\\"Case-based reasoning.\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 9 September 2016. Web. 6 December 2016.  \\n* \\\"Constraint satisfaction.\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 16 October 2016. Web. 8 December 2016.  \\n* \\\"Frame (artificial intelligence).\\\" Wikipedia: The Free Encyclopedia. Wikimedia Foundation, Inc. 7 September 2016. Web. 6 December 2016.  \\n* Goel, Ashok and David Joyner. Knowledge-Based AI: Cognitive Systems by Georgia Tech [Videos]. Udacity. https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409 Accessed 6 November 2016. \\n* Norvig, Peter and Stuart Russell. Artificial Intelligence: A Modern Approach. 3rd ed., Pearson, 2010.   \\n* Poole, David and Alan Mackworth. Case-Based Reasoning. Cambridge University Press. artint.info/html/ArtInt_190.html. Accessed 6 December 2016.  \\n* Winston, Patrick. Artificial Intelligence. 3rd ed., Addison-Wesley Publishing Company, 1993. \\n## Development Resources\\n### Tutorials \\n* “Azure Functions NodeJS developer reference.” https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-node  \\n* “Build a Node.js web application using DocumentDB.” https://docs.microsoft.com/en-us/azure/documentdb/documentdb-nodejs-application  \\n* “Get started with Azure Search in the portal.” https://docs.microsoft.com/en-us/azure/search/search-get-started-portal \\n* “Bot Framework: UniveralBot.” https://docs.botframework.com/en-us/node/builder/chat/UniversalBot/#navtitle  \\n* “Bot Framework: Understanding Natural Language.” https://docs.botframework.com/en-us/node/builder/guides/understanding-natural-language/ \\n### Code \\n* “Bot Builder for Node.js examples are organized into groups and designed to illustrate the techniques needed to build great bots.” https://docs.botframework.com/en-us/node/builder/guides/examples/ \\n* “These samples illustrate how to approach dialogs that need to help the user navigate large amounts of content, creating a data-driven exploration experience.” https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/demo-Search  \\n* “A sample bot using IntentDialog to integrate with a LUIS.ai application.” https://github.com/Microsoft/BotBuilder-Samples/tree/master/Node/intelligence-LUIS\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}",
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e8d",
            "uuid": "3419702b-e876-46e5-b728-04d86936043d",
            "title": "Conversational UI Best Practices",
            "slug": "conversational-ui-best-practices",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"> Editor's Note:\\n\\n> This post was written by my good friend and collaborator, [Thomas Bailey](https://github.com/noise-machines). We met years ago during under grad in an introductory course on natural language processing. We immediately connected for many reasons, particularly around music and entrepreneurship. Our collaboration continues to this day as [we produce music](https://www.youtube.com/watch?v=fxC2-HZxMbc) and build chatbots together.\\n\\n> I hope you enjoy the article.\\n\\n> Paul\\n\\nFor the past few months, [Paul](__GHOST_URL__/about/) has been leading [our team](http://neona.chat/) in upgrading Neona, a chatbot that introduces users to A.I. concepts. Along with some other things, we're re-working her conversational UI. In this post, I want to synthesize some of what we learned in the process. As a quick note - our interface improvements aren't live yet, but we'll update this post when they are!\\n\\nThe very short version:\\n\\n- Be concise.\\n- Make fallbacks helpful.\\n- Avoid false options. Only present commands that the bot can respond to meaningfully.\\n- Find the right voice. Make sure it lines up with the bot's visuals.\\n- Avoid meaningless variation, especially in commands.\\n\\nAll of these ideas have analogues in visual UI design, but you have to approach them a little differently for conversational interfaces.\\n\\n### A little background\\n\\nNeona has two purposes: teach users about artificial intelligence concepts, and serve as an interactive chatbot demo (e.g., for [this talk](https://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture) of Paul's).\\n\\nIf you're curious, you can talk to her [here](http://neona.chat/webchat.html), and [this post](http://blog.paulprae.com/neona-a-conversational-agent-that-teaches-ai-2/) goes in-depth into her architecture and algorithms. Here's a demo of interacting with her:\\n\\n![](__GHOST_URL__/content/images/2017/10/neona-demo-1.gif)\\n\\n### Be concise\\n\\nConciseness has two major benefits: it makes your writing easier to understand, and it takes up less screen space.\\n\\nIn computational terms, processing time for an n-word sentence is proportional to at least n^3. So if you cut a sentence in half, you make it eight times easier to understand. A warning, though: like in programming, being too terse can make make something harder to read. The goal isn't to play [code golf](https://meta.stackexchange.com/questions/20736/what-is-code-golf-on-stack-overflow) with English.\\n\\nShorter sentences are also better for small screens. An ad hoc test shows that the Messages app on an iPhone 6S Plus (a 5.5 inch screen) can display about 130 words of lorem ipsum at once. For reference, most phone screens range from from 3.5 inches (iPhone 4) to 6.2 inches (Galaxy S8+). And even on large screens, 130 words feels like a huge wall of text.\\n\\n![130 words of lorem ipsum on an iPhone 6S Plus](__GHOST_URL__/content/images/2017/10/130-words-of-lorem-ipsum.PNG)\\n> 130 words of lorem ipsum on an iPhone 6S Plus\\n\\nFor our Neona redesign, we aimed to keep her messages below 30 words. When more text was necessary, we split it up into multiple messages. And even then, we limited ourselves to two messages between user interactions. These rules functioned similarly to Sandi Metz's [rules for developers](https://robots.thoughtbot.com/sandi-metz-rules-for-developers), helping us narrow the scope of each interaction.\\n\\n### Make fallbacks helpful\\n\\nWhen your bot doesn't understand a command, there are a lot of strategies you can try before giving up (\\\"Sorry, I don't understand\\\"):\\n\\n- Check if the unknown command is a typo.\\n- Check if the unknown command is a synonym. This is actually pretty tricky -- see below.\\n- Use a fallback strategy specific to your bot's domain.\\n\\nFor typo correction, it's often enough to use a simple metric like [Levenshtein Distance](https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm). For example, using the [meant](https://www.npmjs.com/package/meant) package on npm:\\n\\n```js\\nconst meant = require('meant')\\nconst userInput = 'udpate'\\nconst commands = ['update', 'prognosticate', 'validate']\\nconst result = meant(userInput, commands)\\n// => 'update'\\n```\\n\\nFinding synonyms is harder, mostly because general thesauruses don't work well for this application. Only one of the three I tried returned a connection between *find* and *search*, even though a user could easily type `find restaurants` instead of `search restaurants`.\\n\\nFor now, I recommend actually looking at user sessions to identify common synonyms. Down the line, techniques from NLP and corpus linguistics could almost certainly be used to develop a more general solution.\\n\\nUsing domain-specific fallback strategies boils down to making an informed guess about the user's intention. When Neona doesn't recognize a command, she does a look-up in her knowledge base. If there are articles relating to the unknown command, she offers to show them to the user. Since we're aiming for Neona to be an expert in the A.I. domain, it's reasonable for users to expect her to recognize and respond intelligently to A.I. terms.\\n\\n### Avoid false options\\n\\nFalse options are options that the bot can't respond to meaningfully. In the first version of Neona's interface, she would always remind the user to type `more` to see more search results, even if no additional search results were available. That occasionally led to situations like this:\\n\\n![](__GHOST_URL__/content/images/2017/10/neona-more-results.gif)\\n\\nWhen there aren't more search results, Neona's `more` command is a false option.\\n\\nSimilarly, Neona has the ability to save articles for a user to read later. The `list` command allows a user to see the articles they've saved. But if they haven't saved any articles, `list` is a false option.\\n\\nThis is related to [progressive disclosure](https://en.wikipedia.org/wiki/Progressive_disclosure). By only mentioning commands when they're relevant and the bot can act on them, you reduce the user's cognitive workload.\\n\\n### Avoid meaningless variation\\n\\nWhen communicating your bot's functionality to a user, be consistent in your word choice. This is especially important for commands. Using a variety of words (*save*, *add to your list*, *remember*) to refer to the same functionality can confuse a user's memory of what the command actually is. Pick a command and stick with it. Previous iterations of Neona's interface occasionally made this mistake:\\n\\n> User: Search machine learning\\n\\n> Neona: Here are a few good options I found:\\n\\n> Neona: [Machine Learning, Active Learning, Logic Learning Machine]\\n\\n> Neona: You can select one or more to add to your list to study later, *list* what you’ve selected so far, see *more* results, or *search* again.\\n\\n> User: Add machine learning to my list\\n\\n> Neona: Sorry, I did not understand ‘Add machine learning to my list’. Type ‘help’ if you need assistance.\\n\\n> User: But you just told me I could add it to my list 🤔\\n\\n### Find the right voice\\n\\nThings to think about when building your chatbot's voice:\\n\\n- Audience\\n- Social context\\n- The problem your bot is solving\\n- The brand the bot represents\\n\\nBe aware that the way you write your conversational interface has consequences for all of them. For Neona:\\n\\n<table>\\n<tr>\\n<td>Audience</td>\\n<td>Tech folks</td>\\n</tr>\\n<tr>\\n<td>Social context</td>\\n<td>Women under-represented in technology, often portrayed as subservient</td>\\n</tr>\\n<tr>\\n<td>Problem</td>\\n<td>Retrieve information on A.I. concepts</td>\\n</tr>\\n</table>\\n\\nSince we decided Neona would be female, and since her primary purpose was to look up information for users, it was important to us to avoid stereotypes about female subservience. To that end, we worked to balance humor, self-assurance, and clear communication in her personality.\\n\\nThis was also a task where working closely with our artist, John, was invaluable. Especially in the early phases of the design process, John and I regularly traded ideas with each other and the rest of the team to make sure her voice and appearance created a unified personality.\\n\\n### Final thoughts\\n\\nWriting a good conversational interface is really hard. [Michael Covington](http://www.covingtoninnovations.com/mc/), one of my favorite professors, once said \\\"writing is almost too complicated for human beings to do.\\\" Hopefully this post helps you overcome some of that difficulty.\\n\\nFor even more help, these slides of his are well worth looking through:\\n\\n> [How To Write More Clearly, Think More Clearly, and Learn Complex Material More Easily](http://www.covingtoninnovations.com/mc/WriteThinkLearn.pdf)\\n\\nKristi Colleran over at Sciensio also has some really good articles on conversational UI:\\n\\n- [Writing Chatbot Conversations: What You Need To Know - Part I](https://blog.sciens.io/writing-chatbot-conversation-what-you-need-to-know-part-i-eabb0ef428e1) talks about platforms, visual affordances, and meeting your users where they're at. I especially like her reminder that some users have trouble with spelling and literacy, which makes the fallback strategies above even more important.\\n- [Writing Chatbot Conversations: What You Need To Know - Part II](https://blog.sciens.io/writing-chatbot-conversations-what-you-need-to-know-part-ii-82cc31650874) dives deeper into crafting the personality and identity of your bot. This is something I wish I'd seen before we revised Neona's personality. I think we did a great job, but I really like the systematic framework this article lays out.\\n\\n### Acknowledgements\\n\\nMany thanks to Paul for assembling and leading neona.chat's world class team, and for inviting me to be a part of it. Unlike many of the team leads I've worked with, Paul has the ability to find great people, align their intentions, then leave them free to do their best work.\\n\\nAlso thanks to John Roberson, our artist, for communicating Neona's character so vibrantly through her appearance, and Paul, Austin, and John for working with me to co-architect her personality.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><blockquote>\n<p>Editor's Note:</p>\n</blockquote>\n<blockquote>\n<p>This post was written by my good friend and collaborator, <a href=\"https://github.com/noise-machines\">Thomas Bailey</a>. We met years ago during under grad in an introductory course on natural language processing. We immediately connected for many reasons, particularly around music and entrepreneurship. Our collaboration continues to this day as <a href=\"https://www.youtube.com/watch?v=fxC2-HZxMbc\">we produce music</a> and build chatbots together.</p>\n</blockquote>\n<blockquote>\n<p>I hope you enjoy the article.</p>\n</blockquote>\n<blockquote>\n<p>Paul</p>\n</blockquote>\n<p>For the past few months, <a href=\"__GHOST_URL__/about/\">Paul</a> has been leading <a href=\"http://neona.chat/\">our team</a> in upgrading Neona, a chatbot that introduces users to A.I. concepts. Along with some other things, we're re-working her conversational UI. In this post, I want to synthesize some of what we learned in the process. As a quick note - our interface improvements aren't live yet, but we'll update this post when they are!</p>\n<p>The very short version:</p>\n<ul>\n<li>Be concise.</li>\n<li>Make fallbacks helpful.</li>\n<li>Avoid false options. Only present commands that the bot can respond to meaningfully.</li>\n<li>Find the right voice. Make sure it lines up with the bot's visuals.</li>\n<li>Avoid meaningless variation, especially in commands.</li>\n</ul>\n<p>All of these ideas have analogues in visual UI design, but you have to approach them a little differently for conversational interfaces.</p>\n<h3 id=\"alittlebackground\">A little background</h3>\n<p>Neona has two purposes: teach users about artificial intelligence concepts, and serve as an interactive chatbot demo (e.g., for <a href=\"https://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture\">this talk</a> of Paul's).</p>\n<p>If you're curious, you can talk to her <a href=\"http://neona.chat/webchat.html\">here</a>, and <a href=\"http://blog.paulprae.com/neona-a-conversational-agent-that-teaches-ai-2/\">this post</a> goes in-depth into her architecture and algorithms. Here's a demo of interacting with her:</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/neona-demo-1.gif\" alt=\"\" loading=\"lazy\"></p>\n<h3 id=\"beconcise\">Be concise</h3>\n<p>Conciseness has two major benefits: it makes your writing easier to understand, and it takes up less screen space.</p>\n<p>In computational terms, processing time for an n-word sentence is proportional to at least n^3. So if you cut a sentence in half, you make it eight times easier to understand. A warning, though: like in programming, being too terse can make make something harder to read. The goal isn't to play <a href=\"https://meta.stackexchange.com/questions/20736/what-is-code-golf-on-stack-overflow\">code golf</a> with English.</p>\n<p>Shorter sentences are also better for small screens. An ad hoc test shows that the Messages app on an iPhone 6S Plus (a 5.5 inch screen) can display about 130 words of lorem ipsum at once. For reference, most phone screens range from from 3.5 inches (iPhone 4) to 6.2 inches (Galaxy S8+). And even on large screens, 130 words feels like a huge wall of text.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/130-words-of-lorem-ipsum.PNG\" alt=\"130 words of lorem ipsum on an iPhone 6S Plus\" loading=\"lazy\"></p>\n<blockquote>\n<p>130 words of lorem ipsum on an iPhone 6S Plus</p>\n</blockquote>\n<p>For our Neona redesign, we aimed to keep her messages below 30 words. When more text was necessary, we split it up into multiple messages. And even then, we limited ourselves to two messages between user interactions. These rules functioned similarly to Sandi Metz's <a href=\"https://robots.thoughtbot.com/sandi-metz-rules-for-developers\">rules for developers</a>, helping us narrow the scope of each interaction.</p>\n<h3 id=\"makefallbackshelpful\">Make fallbacks helpful</h3>\n<p>When your bot doesn't understand a command, there are a lot of strategies you can try before giving up (&quot;Sorry, I don't understand&quot;):</p>\n<ul>\n<li>Check if the unknown command is a typo.</li>\n<li>Check if the unknown command is a synonym. This is actually pretty tricky -- see below.</li>\n<li>Use a fallback strategy specific to your bot's domain.</li>\n</ul>\n<p>For typo correction, it's often enough to use a simple metric like <a href=\"https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm\">Levenshtein Distance</a>. For example, using the <a href=\"https://www.npmjs.com/package/meant\">meant</a> package on npm:</p>\n<pre><code class=\"language-js\">const meant = require('meant')\nconst userInput = 'udpate'\nconst commands = ['update', 'prognosticate', 'validate']\nconst result = meant(userInput, commands)\n// =&gt; 'update'\n</code></pre>\n<p>Finding synonyms is harder, mostly because general thesauruses don't work well for this application. Only one of the three I tried returned a connection between <em>find</em> and <em>search</em>, even though a user could easily type <code>find restaurants</code> instead of <code>search restaurants</code>.</p>\n<p>For now, I recommend actually looking at user sessions to identify common synonyms. Down the line, techniques from NLP and corpus linguistics could almost certainly be used to develop a more general solution.</p>\n<p>Using domain-specific fallback strategies boils down to making an informed guess about the user's intention. When Neona doesn't recognize a command, she does a look-up in her knowledge base. If there are articles relating to the unknown command, she offers to show them to the user. Since we're aiming for Neona to be an expert in the A.I. domain, it's reasonable for users to expect her to recognize and respond intelligently to A.I. terms.</p>\n<h3 id=\"avoidfalseoptions\">Avoid false options</h3>\n<p>False options are options that the bot can't respond to meaningfully. In the first version of Neona's interface, she would always remind the user to type <code>more</code> to see more search results, even if no additional search results were available. That occasionally led to situations like this:</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/neona-more-results.gif\" alt=\"\" loading=\"lazy\"></p>\n<p>When there aren't more search results, Neona's <code>more</code> command is a false option.</p>\n<p>Similarly, Neona has the ability to save articles for a user to read later. The <code>list</code> command allows a user to see the articles they've saved. But if they haven't saved any articles, <code>list</code> is a false option.</p>\n<p>This is related to <a href=\"https://en.wikipedia.org/wiki/Progressive_disclosure\">progressive disclosure</a>. By only mentioning commands when they're relevant and the bot can act on them, you reduce the user's cognitive workload.</p>\n<h3 id=\"avoidmeaninglessvariation\">Avoid meaningless variation</h3>\n<p>When communicating your bot's functionality to a user, be consistent in your word choice. This is especially important for commands. Using a variety of words (<em>save</em>, <em>add to your list</em>, <em>remember</em>) to refer to the same functionality can confuse a user's memory of what the command actually is. Pick a command and stick with it. Previous iterations of Neona's interface occasionally made this mistake:</p>\n<blockquote>\n<p>User: Search machine learning</p>\n</blockquote>\n<blockquote>\n<p>Neona: Here are a few good options I found:</p>\n</blockquote>\n<blockquote>\n<p>Neona: [Machine Learning, Active Learning, Logic Learning Machine]</p>\n</blockquote>\n<blockquote>\n<p>Neona: You can select one or more to add to your list to study later, <em>list</em> what you’ve selected so far, see <em>more</em> results, or <em>search</em> again.</p>\n</blockquote>\n<blockquote>\n<p>User: Add machine learning to my list</p>\n</blockquote>\n<blockquote>\n<p>Neona: Sorry, I did not understand ‘Add machine learning to my list’. Type ‘help’ if you need assistance.</p>\n</blockquote>\n<blockquote>\n<p>User: But you just told me I could add it to my list 🤔</p>\n</blockquote>\n<h3 id=\"findtherightvoice\">Find the right voice</h3>\n<p>Things to think about when building your chatbot's voice:</p>\n<ul>\n<li>Audience</li>\n<li>Social context</li>\n<li>The problem your bot is solving</li>\n<li>The brand the bot represents</li>\n</ul>\n<p>Be aware that the way you write your conversational interface has consequences for all of them. For Neona:</p>\n<table>\n<tr>\n<td>Audience</td>\n<td>Tech folks</td>\n</tr>\n<tr>\n<td>Social context</td>\n<td>Women under-represented in technology, often portrayed as subservient</td>\n</tr>\n<tr>\n<td>Problem</td>\n<td>Retrieve information on A.I. concepts</td>\n</tr>\n</table>\n<p>Since we decided Neona would be female, and since her primary purpose was to look up information for users, it was important to us to avoid stereotypes about female subservience. To that end, we worked to balance humor, self-assurance, and clear communication in her personality.</p>\n<p>This was also a task where working closely with our artist, John, was invaluable. Especially in the early phases of the design process, John and I regularly traded ideas with each other and the rest of the team to make sure her voice and appearance created a unified personality.</p>\n<h3 id=\"finalthoughts\">Final thoughts</h3>\n<p>Writing a good conversational interface is really hard. <a href=\"http://www.covingtoninnovations.com/mc/\">Michael Covington</a>, one of my favorite professors, once said &quot;writing is almost too complicated for human beings to do.&quot; Hopefully this post helps you overcome some of that difficulty.</p>\n<p>For even more help, these slides of his are well worth looking through:</p>\n<blockquote>\n<p><a href=\"http://www.covingtoninnovations.com/mc/WriteThinkLearn.pdf\">How To Write More Clearly, Think More Clearly, and Learn Complex Material More Easily</a></p>\n</blockquote>\n<p>Kristi Colleran over at Sciensio also has some really good articles on conversational UI:</p>\n<ul>\n<li><a href=\"https://blog.sciens.io/writing-chatbot-conversation-what-you-need-to-know-part-i-eabb0ef428e1\">Writing Chatbot Conversations: What You Need To Know - Part I</a> talks about platforms, visual affordances, and meeting your users where they're at. I especially like her reminder that some users have trouble with spelling and literacy, which makes the fallback strategies above even more important.</li>\n<li><a href=\"https://blog.sciens.io/writing-chatbot-conversations-what-you-need-to-know-part-ii-82cc31650874\">Writing Chatbot Conversations: What You Need To Know - Part II</a> dives deeper into crafting the personality and identity of your bot. This is something I wish I'd seen before we revised Neona's personality. I think we did a great job, but I really like the systematic framework this article lays out.</li>\n</ul>\n<h3 id=\"acknowledgements\">Acknowledgements</h3>\n<p>Many thanks to Paul for assembling and leading neona.chat's world class team, and for inviting me to be a part of it. Unlike many of the team leads I've worked with, Paul has the ability to find great people, align their intentions, then leave them free to do their best work.</p>\n<p>Also thanks to John Roberson, our artist, for communicating Neona's character so vibrantly through her appearance, and Paul, Austin, and John for working with me to co-architect her personality.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "22",
            "plaintext": "> Editor's Note:\n\n\n> This post was written by my good friend and collaborator, Thomas Bailey\n[https://github.com/noise-machines]. We met years ago during under grad in an\nintroductory course on natural language processing. We immediately connected for\nmany reasons, particularly around music and entrepreneurship. Our collaboration\ncontinues to this day as we produce music\n[https://www.youtube.com/watch?v=fxC2-HZxMbc] and build chatbots together.\n\n\n> I hope you enjoy the article.\n\n\n> Paul\n\n\nFor the past few months, Paul [__GHOST_URL__/about/] has been leading our team\n[http://neona.chat/] in upgrading Neona, a chatbot that introduces users to A.I.\nconcepts. Along with some other things, we're re-working her conversational UI.\nIn this post, I want to synthesize some of what we learned in the process. As a\nquick note - our interface improvements aren't live yet, but we'll update this\npost when they are!\n\nThe very short version:\n\n * Be concise.\n * Make fallbacks helpful.\n * Avoid false options. Only present commands that the bot can respond to\n   meaningfully.\n * Find the right voice. Make sure it lines up with the bot's visuals.\n * Avoid meaningless variation, especially in commands.\n\nAll of these ideas have analogues in visual UI design, but you have to approach\nthem a little differently for conversational interfaces.\n\nA little background\nNeona has two purposes: teach users about artificial intelligence concepts, and\nserve as an interactive chatbot demo (e.g., for this talk\n[https://www.slideshare.net/PaulPrae/azure-as-a-chatbot-service-from-purpose-to-production-with-a-cloud-bot-architecture] \nof Paul's).\n\nIf you're curious, you can talk to her here [http://neona.chat/webchat.html],\nand this post\n[http://blog.paulprae.com/neona-a-conversational-agent-that-teaches-ai-2/] goes\nin-depth into her architecture and algorithms. Here's a demo of interacting with\nher:\n\n\n\nBe concise\nConciseness has two major benefits: it makes your writing easier to understand,\nand it takes up less screen space.\n\nIn computational terms, processing time for an n-word sentence is proportional\nto at least n^3. So if you cut a sentence in half, you make it eight times\neasier to understand. A warning, though: like in programming, being too terse\ncan make make something harder to read. The goal isn't to play code golf\n[https://meta.stackexchange.com/questions/20736/what-is-code-golf-on-stack-overflow] \nwith English.\n\nShorter sentences are also better for small screens. An ad hoc test shows that\nthe Messages app on an iPhone 6S Plus (a 5.5 inch screen) can display about 130\nwords of lorem ipsum at once. For reference, most phone screens range from from\n3.5 inches (iPhone 4) to 6.2 inches (Galaxy S8+). And even on large screens, 130\nwords feels like a huge wall of text.\n\n\n\n> 130 words of lorem ipsum on an iPhone 6S Plus\n\n\nFor our Neona redesign, we aimed to keep her messages below 30 words. When more\ntext was necessary, we split it up into multiple messages. And even then, we\nlimited ourselves to two messages between user interactions. These rules\nfunctioned similarly to Sandi Metz's rules for developers\n[https://robots.thoughtbot.com/sandi-metz-rules-for-developers], helping us\nnarrow the scope of each interaction.\n\nMake fallbacks helpful\nWhen your bot doesn't understand a command, there are a lot of strategies you\ncan try before giving up (\"Sorry, I don't understand\"):\n\n * Check if the unknown command is a typo.\n * Check if the unknown command is a synonym. This is actually pretty tricky --\n   see below.\n * Use a fallback strategy specific to your bot's domain.\n\nFor typo correction, it's often enough to use a simple metric like Levenshtein\nDistance\n[https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm]\n. For example, using the meant [https://www.npmjs.com/package/meant] package on\nnpm:\n\nconst meant = require('meant')\nconst userInput = 'udpate'\nconst commands = ['update', 'prognosticate', 'validate']\nconst result = meant(userInput, commands)\n// => 'update'\n\n\nFinding synonyms is harder, mostly because general thesauruses don't work well\nfor this application. Only one of the three I tried returned a connection\nbetween find and search, even though a user could easily type find restaurants \ninstead of search restaurants.\n\nFor now, I recommend actually looking at user sessions to identify common\nsynonyms. Down the line, techniques from NLP and corpus linguistics could almost\ncertainly be used to develop a more general solution.\n\nUsing domain-specific fallback strategies boils down to making an informed guess\nabout the user's intention. When Neona doesn't recognize a command, she does a\nlook-up in her knowledge base. If there are articles relating to the unknown\ncommand, she offers to show them to the user. Since we're aiming for Neona to be\nan expert in the A.I. domain, it's reasonable for users to expect her to\nrecognize and respond intelligently to A.I. terms.\n\nAvoid false options\nFalse options are options that the bot can't respond to meaningfully. In the\nfirst version of Neona's interface, she would always remind the user to type \nmore to see more search results, even if no additional search results were\navailable. That occasionally led to situations like this:\n\n\n\nWhen there aren't more search results, Neona's more command is a false option.\n\nSimilarly, Neona has the ability to save articles for a user to read later. The \nlist command allows a user to see the articles they've saved. But if they\nhaven't saved any articles, list is a false option.\n\nThis is related to progressive disclosure\n[https://en.wikipedia.org/wiki/Progressive_disclosure]. By only mentioning\ncommands when they're relevant and the bot can act on them, you reduce the\nuser's cognitive workload.\n\nAvoid meaningless variation\nWhen communicating your bot's functionality to a user, be consistent in your\nword choice. This is especially important for commands. Using a variety of words\n(save, add to your list, remember) to refer to the same functionality can\nconfuse a user's memory of what the command actually is. Pick a command and\nstick with it. Previous iterations of Neona's interface occasionally made this\nmistake:\n\n> User: Search machine learning\n\n\n> Neona: Here are a few good options I found:\n\n\n> Neona: [Machine Learning, Active Learning, Logic Learning Machine]\n\n\n> Neona: You can select one or more to add to your list to study later, list what\nyou’ve selected so far, see more results, or search again.\n\n\n> User: Add machine learning to my list\n\n\n> Neona: Sorry, I did not understand ‘Add machine learning to my list’. Type\n‘help’ if you need assistance.\n\n\n> User: But you just told me I could add it to my list 🤔\n\n\nFind the right voice\nThings to think about when building your chatbot's voice:\n\n * Audience\n * Social context\n * The problem your bot is solving\n * The brand the bot represents\n\nBe aware that the way you write your conversational interface has consequences\nfor all of them. For Neona:\n\nAudienceTech folksSocial contextWomen under-represented in technology, often\nportrayed as subservientProblemRetrieve information on A.I. conceptsSince we\ndecided Neona would be female, and since her primary purpose was to look up\ninformation for users, it was important to us to avoid stereotypes about female\nsubservience. To that end, we worked to balance humor, self-assurance, and clear\ncommunication in her personality.\n\nThis was also a task where working closely with our artist, John, was\ninvaluable. Especially in the early phases of the design process, John and I\nregularly traded ideas with each other and the rest of the team to make sure her\nvoice and appearance created a unified personality.\n\nFinal thoughts\nWriting a good conversational interface is really hard. Michael Covington\n[http://www.covingtoninnovations.com/mc/], one of my favorite professors, once\nsaid \"writing is almost too complicated for human beings to do.\" Hopefully this\npost helps you overcome some of that difficulty.\n\nFor even more help, these slides of his are well worth looking through:\n\n> How To Write More Clearly, Think More Clearly, and Learn Complex Material More\nEasily [http://www.covingtoninnovations.com/mc/WriteThinkLearn.pdf]\n\n\nKristi Colleran over at Sciensio also has some really good articles on\nconversational UI:\n\n * Writing Chatbot Conversations: What You Need To Know - Part I\n   [https://blog.sciens.io/writing-chatbot-conversation-what-you-need-to-know-part-i-eabb0ef428e1] \n   talks about platforms, visual affordances, and meeting your users where\n   they're at. I especially like her reminder that some users have trouble with\n   spelling and literacy, which makes the fallback strategies above even more\n   important.\n * Writing Chatbot Conversations: What You Need To Know - Part II\n   [https://blog.sciens.io/writing-chatbot-conversations-what-you-need-to-know-part-ii-82cc31650874] \n   dives deeper into crafting the personality and identity of your bot. This is\n   something I wish I'd seen before we revised Neona's personality. I think we\n   did a great job, but I really like the systematic framework this article lays\n   out.\n\nAcknowledgements\nMany thanks to Paul for assembling and leading neona.chat's world class team,\nand for inviting me to be a part of it. Unlike many of the team leads I've\nworked with, Paul has the ability to find great people, align their intentions,\nthen leave them free to do their best work.\n\nAlso thanks to John Roberson, our artist, for communicating Neona's character so\nvibrantly through her appearance, and Paul, Austin, and John for working with me\nto co-architect her personality.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2017-10-12T21:43:00.000Z",
            "updated_at": "2021-03-09T16:45:19.000Z",
            "published_at": "2017-10-19T18:15:00.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046a1f0db781400394a7e8e",
            "uuid": "c2fa364e-212f-4419-9935-14b6f3609fac",
            "title": "Bot Prototyping: Right Now, None Of The Tools Are Great",
            "slug": "bot-prototyping-right-now-none-of-the-tools-are-great",
            "mobiledoc": "{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I went through all six prototyping tools on [Bot Stash's list](http://www.botsfloor.com/botstash/products/?category=Prototyping) and was underwhelmed.\\n\\nAcross the board they suffer from poor UI, serious bugs, or a lack of important features.\\n\\nIf you want the most functional option, you'll need to pay. [Botsociety](https://botsociety.io/) is reliable and has the most features, but you're limited to a single export per month unless you upgrade to their $49/month premium plan.\\n\\nAfter reviewing most of the bot prototyping tools available, here are my final recommendations:\\n\\n> Use **[Botsociety](https://botsociety.io/)** if you need the ability to model alternate paths through a conversation and don't mind paying. Supported platforms: Facebook Messenger, Slack, Google Home.\\n\\n> Use **[Walkie](https://walkiebot.co/)** if you're building a Slack bot and don't need paths or gif export.\\n\\n> Use **[Botpreview](https://botpreview.com/)** if you're building a Facebook Messenger bot and don't need paths.\\n\\nAll of the above have decent user interfaces and are pretty reliable. Read on for a deeper look at each one. \\n\\n### Background\\n\\nI recently wrote about the [conversational UI best practices](http://blog.paulprae.com/conversational-ui-best-practices/) that we uncovered while redesigning our chatbot, [Neona](http://neona.chat).  Originally, I had planned to write a follow-up post showing concretely how we applied those best practices to improve Neona's conversations. To make it easy to follow along in that post, I planned to use one of the fancy bot prototyping tools I had run across. Unfortunately, they either didn't meet my requirements or were too buggy to be usable. So instead of walking readers through our redesign process, I figured I'd survey the landscape of bot prototyping tools.\\n\\n(When I first redesigned Neona, I used flowcharts made with [draw.io](https://www.draw.io/) to map out the conversation flow. It wasn't great, but it was functional.)\\n\\n### Requirements\\n\\nMy main goal was to communicate and explain design decisions, which motivated my requirements.\\n\\nFirst, I need to be able to **easily share animated mockups of the conversation.** I prefer animation over static images because it more clearly communicates the feel of interacting with the bot.\\n\\nFurthermore, I want to **organize conversations as a tree, not a line**. This allows you to handle different user responses more naturally. Bonus points if the app lets you share sub-trees of the conversation.\\n\\nFinally, the prototyping tool should **output visuals that mirror one of the platforms Neona runs on.** At the moment, that means Skype or Facebook Messenger, but we're working on Slack integration. You can also talk to Neona [directly in the browser](http://neona.chat/webchat.html), but I prefer designing interfaces within the more visually constrained environment of a messaging app.\\n\\n### [Botmock](https://botmock.com/)\\n\\nBotmock is one of the better tools I tested. It was the only one that let you organize your conversation as a tree, and like I mentioned in my requirements, it gets a bonus for the ability to export sub-trees. It feels like a tool you could use from beginning to end for designing a conversational interface.\\n\\nIt also supports a few different visual styles, including Facebook Messenger.\\n\\nYou create a tree by dragging connections between nodes, similar to mind-mapping or patcher programming languages like Max and Pure Data.\\n\\n![Botmock's editor page](__GHOST_URL__/content/images/2017/10/botmock-editor.png)\\n\\nUnfortunately, it's pretty buggy. Export to gif (admittedly in beta) displays a strange zooming behavior:\\n\\n![Botmock's gif export](__GHOST_URL__/content/images/2017/10/botmock.gif)\\n\\nNodes also occasionally disappear from the editor page. When this happens, previously deleted nodes re-appear, covering up \\\"Get Started.\\\" The vanished nodes still take up space in your conversation tree, but they can't be selected or deleted.\\n\\nThis was the deal breaker for me. I wasn't willing to design our UI in a program that could break at any moment.\\n\\n### [Botsociety](https://botsociety.io/)\\n\\nBotsociety is probably the best tool I found. It takes second place after Botmock in terms of features, but is more reliable. There's only limited support for conversation trees, but it does have Facebook Messenger-style visuals, and you can export to gif and video. \\n\\nMore on conversation trees: Botsociety implements this as a feature called \\\"paths,\\\" that allows you to specify alternative responses. The major shortcoming is that you can't reuse portions of your conversation -- you have to duplicate them.\\n\\n![Botsociety's editor](__GHOST_URL__/content/images/2017/10/botsociety-editor.png)\\n\\nA major limitation to be aware of upfront -- the free plan only allows a single export per month. To get more, you need to invite friends (you each get a free extra export) or upgrade to their $49/month premium plan.\\n\\n![Botsociety's gif export](__GHOST_URL__/content/images/2017/10/botsociety.gif)\\n\\nA few UI critiques:\\n\\n- The editor UI feels really busy.\\n- In the gif export, I'm not a huge fan of how the keyboard pops up and then hides again after every message from the user.\\n\\nDespite those limitations, Botsociety probably has the best combination of features and stability. If my requirements were different, I would've gone with it.\\n\\n### [Botpreview](https://botpreview.com/)\\n\\nBotpreview has a pretty clean user interface, supports unlimited gif and video export, and allows you to preview your bot's interactions in a Facebook Messenger-style interface. Unfortunately, it doesn't offer conversation trees, or even something like Botsociety's paths.\\n\\n![Botpreview's editor](__GHOST_URL__/content/images/2017/10/Screen-Shot-2017-10-13-at-2.59.18-PM.png)\\n\\nIf you don't need the ability to map different routes through a conversation, Botpreview is a decent choice.\\n\\n### [Botframe](https://botframe.com/editor/new)\\n\\nBotframe is a minimal prototyping tool that has a limited scope and a clear UI. It's also pretty reliable. Unfortunately, it's missing key features. Its visualization of the conversation is static, like you're looking back at one that already happened. It only supports still image export and doesn't allow you to model multiple paths through a conversation.\\n\\n![Botframe's editor page](__GHOST_URL__/content/images/2017/10/botframe-editor-.png)\\n\\n### [Walkie](https://walkiebot.co/)\\n\\nWalkie is a prototyping tool for Slack bots. Since chatbots have an advantage in the simplicity of their render target (plain text), I'm curious to see how their platform-specific strategy pans out. Despite their solid design, they meet none of my requirements. Naturally, they only render your conversation in an approximation of Slack's UI. Conversations are modeled as the linear history of a Slack channel, so no tree-based conversations. Curiously, they only export to JSON.\\n\\n![Walkie's editor](__GHOST_URL__/content/images/2017/10/walkie-editor.png)\\n\\n### [Mockabot](http://mockabot.theartofbuildingbots.com/)\\n\\nThis app didn't meet any of my requirements. It only supports linear conversation editing, doesn't have an export feature, and the mockup's UI is half-hearted material design. If Botmock and Botsociety are the closest to getting it right, Mockabot is probably the farthest.\\n\\n![Mockabot's editor](__GHOST_URL__/content/images/2017/10/Screen-Shot-2017-10-13-at-2.29.16-PM.png)\\n\\n### Final Thoughts\\n\\nFor many domains, one or a few apps stand head and shoulders above the rest. Bot prototyping isn't one of them. Botsociety holds the lead currently, but Botmock could jump ahead with the next bugfix release.\\n\\nThe hardest part of building a bot is that [writing is almost too complicated for human beings to do](http://www.covingtoninnovations.com/mc/WriteThinkLearn.pdf). While comparing these tools, I was surprised that none of them even began to tackle that problem.\\n\\nIn the past, I've pitched the idea of building a tool like [Textio](https://textio.com/) for bots to my [brilliant and hardworking teammates](http://neona.chat#team). Having done this deep dive, I feel like that's an even more viable option.\\n\\nP.S. - Bot Stash includes various Sketch UI Kits in their list of prototyping tools. Since I don't know Sketch and my visual design skills are limited, I didn't consider them in my evaluation.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: markdown--><p>I went through all six prototyping tools on <a href=\"http://www.botsfloor.com/botstash/products/?category=Prototyping\">Bot Stash's list</a> and was underwhelmed.</p>\n<p>Across the board they suffer from poor UI, serious bugs, or a lack of important features.</p>\n<p>If you want the most functional option, you'll need to pay. <a href=\"https://botsociety.io/\">Botsociety</a> is reliable and has the most features, but you're limited to a single export per month unless you upgrade to their $49/month premium plan.</p>\n<p>After reviewing most of the bot prototyping tools available, here are my final recommendations:</p>\n<blockquote>\n<p>Use <strong><a href=\"https://botsociety.io/\">Botsociety</a></strong> if you need the ability to model alternate paths through a conversation and don't mind paying. Supported platforms: Facebook Messenger, Slack, Google Home.</p>\n</blockquote>\n<blockquote>\n<p>Use <strong><a href=\"https://walkiebot.co/\">Walkie</a></strong> if you're building a Slack bot and don't need paths or gif export.</p>\n</blockquote>\n<blockquote>\n<p>Use <strong><a href=\"https://botpreview.com/\">Botpreview</a></strong> if you're building a Facebook Messenger bot and don't need paths.</p>\n</blockquote>\n<p>All of the above have decent user interfaces and are pretty reliable. Read on for a deeper look at each one.</p>\n<h3 id=\"background\">Background</h3>\n<p>I recently wrote about the <a href=\"http://blog.paulprae.com/conversational-ui-best-practices/\">conversational UI best practices</a> that we uncovered while redesigning our chatbot, <a href=\"http://neona.chat\">Neona</a>.  Originally, I had planned to write a follow-up post showing concretely how we applied those best practices to improve Neona's conversations. To make it easy to follow along in that post, I planned to use one of the fancy bot prototyping tools I had run across. Unfortunately, they either didn't meet my requirements or were too buggy to be usable. So instead of walking readers through our redesign process, I figured I'd survey the landscape of bot prototyping tools.</p>\n<p>(When I first redesigned Neona, I used flowcharts made with <a href=\"https://www.draw.io/\">draw.io</a> to map out the conversation flow. It wasn't great, but it was functional.)</p>\n<h3 id=\"requirements\">Requirements</h3>\n<p>My main goal was to communicate and explain design decisions, which motivated my requirements.</p>\n<p>First, I need to be able to <strong>easily share animated mockups of the conversation.</strong> I prefer animation over static images because it more clearly communicates the feel of interacting with the bot.</p>\n<p>Furthermore, I want to <strong>organize conversations as a tree, not a line</strong>. This allows you to handle different user responses more naturally. Bonus points if the app lets you share sub-trees of the conversation.</p>\n<p>Finally, the prototyping tool should <strong>output visuals that mirror one of the platforms Neona runs on.</strong> At the moment, that means Skype or Facebook Messenger, but we're working on Slack integration. You can also talk to Neona <a href=\"http://neona.chat/webchat.html\">directly in the browser</a>, but I prefer designing interfaces within the more visually constrained environment of a messaging app.</p>\n<h3 id=\"botmock\"><a href=\"https://botmock.com/\">Botmock</a></h3>\n<p>Botmock is one of the better tools I tested. It was the only one that let you organize your conversation as a tree, and like I mentioned in my requirements, it gets a bonus for the ability to export sub-trees. It feels like a tool you could use from beginning to end for designing a conversational interface.</p>\n<p>It also supports a few different visual styles, including Facebook Messenger.</p>\n<p>You create a tree by dragging connections between nodes, similar to mind-mapping or patcher programming languages like Max and Pure Data.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/botmock-editor.png\" alt=\"Botmock's editor page\" loading=\"lazy\"></p>\n<p>Unfortunately, it's pretty buggy. Export to gif (admittedly in beta) displays a strange zooming behavior:</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/botmock.gif\" alt=\"Botmock's gif export\" loading=\"lazy\"></p>\n<p>Nodes also occasionally disappear from the editor page. When this happens, previously deleted nodes re-appear, covering up &quot;Get Started.&quot; The vanished nodes still take up space in your conversation tree, but they can't be selected or deleted.</p>\n<p>This was the deal breaker for me. I wasn't willing to design our UI in a program that could break at any moment.</p>\n<h3 id=\"botsociety\"><a href=\"https://botsociety.io/\">Botsociety</a></h3>\n<p>Botsociety is probably the best tool I found. It takes second place after Botmock in terms of features, but is more reliable. There's only limited support for conversation trees, but it does have Facebook Messenger-style visuals, and you can export to gif and video.</p>\n<p>More on conversation trees: Botsociety implements this as a feature called &quot;paths,&quot; that allows you to specify alternative responses. The major shortcoming is that you can't reuse portions of your conversation -- you have to duplicate them.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/botsociety-editor.png\" alt=\"Botsociety's editor\" loading=\"lazy\"></p>\n<p>A major limitation to be aware of upfront -- the free plan only allows a single export per month. To get more, you need to invite friends (you each get a free extra export) or upgrade to their $49/month premium plan.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/botsociety.gif\" alt=\"Botsociety's gif export\" loading=\"lazy\"></p>\n<p>A few UI critiques:</p>\n<ul>\n<li>The editor UI feels really busy.</li>\n<li>In the gif export, I'm not a huge fan of how the keyboard pops up and then hides again after every message from the user.</li>\n</ul>\n<p>Despite those limitations, Botsociety probably has the best combination of features and stability. If my requirements were different, I would've gone with it.</p>\n<h3 id=\"botpreview\"><a href=\"https://botpreview.com/\">Botpreview</a></h3>\n<p>Botpreview has a pretty clean user interface, supports unlimited gif and video export, and allows you to preview your bot's interactions in a Facebook Messenger-style interface. Unfortunately, it doesn't offer conversation trees, or even something like Botsociety's paths.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/Screen-Shot-2017-10-13-at-2.59.18-PM.png\" alt=\"Botpreview's editor\" loading=\"lazy\"></p>\n<p>If you don't need the ability to map different routes through a conversation, Botpreview is a decent choice.</p>\n<h3 id=\"botframe\"><a href=\"https://botframe.com/editor/new\">Botframe</a></h3>\n<p>Botframe is a minimal prototyping tool that has a limited scope and a clear UI. It's also pretty reliable. Unfortunately, it's missing key features. Its visualization of the conversation is static, like you're looking back at one that already happened. It only supports still image export and doesn't allow you to model multiple paths through a conversation.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/botframe-editor-.png\" alt=\"Botframe's editor page\" loading=\"lazy\"></p>\n<h3 id=\"walkie\"><a href=\"https://walkiebot.co/\">Walkie</a></h3>\n<p>Walkie is a prototyping tool for Slack bots. Since chatbots have an advantage in the simplicity of their render target (plain text), I'm curious to see how their platform-specific strategy pans out. Despite their solid design, they meet none of my requirements. Naturally, they only render your conversation in an approximation of Slack's UI. Conversations are modeled as the linear history of a Slack channel, so no tree-based conversations. Curiously, they only export to JSON.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/walkie-editor.png\" alt=\"Walkie's editor\" loading=\"lazy\"></p>\n<h3 id=\"mockabot\"><a href=\"http://mockabot.theartofbuildingbots.com/\">Mockabot</a></h3>\n<p>This app didn't meet any of my requirements. It only supports linear conversation editing, doesn't have an export feature, and the mockup's UI is half-hearted material design. If Botmock and Botsociety are the closest to getting it right, Mockabot is probably the farthest.</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/10/Screen-Shot-2017-10-13-at-2.29.16-PM.png\" alt=\"Mockabot's editor\" loading=\"lazy\"></p>\n<h3 id=\"finalthoughts\">Final Thoughts</h3>\n<p>For many domains, one or a few apps stand head and shoulders above the rest. Bot prototyping isn't one of them. Botsociety holds the lead currently, but Botmock could jump ahead with the next bugfix release.</p>\n<p>The hardest part of building a bot is that <a href=\"http://www.covingtoninnovations.com/mc/WriteThinkLearn.pdf\">writing is almost too complicated for human beings to do</a>. While comparing these tools, I was surprised that none of them even began to tackle that problem.</p>\n<p>In the past, I've pitched the idea of building a tool like <a href=\"https://textio.com/\">Textio</a> for bots to my <a href=\"http://neona.chat#team\">brilliant and hardworking teammates</a>. Having done this deep dive, I feel like that's an even more viable option.</p>\n<p>P.S. - Bot Stash includes various Sketch UI Kits in their list of prototyping tools. Since I don't know Sketch and my visual design skills are limited, I didn't consider them in my evaluation.</p>\n<!--kg-card-end: markdown-->",
            "comment_id": "23",
            "plaintext": "I went through all six prototyping tools on Bot Stash's list\n[http://www.botsfloor.com/botstash/products/?category=Prototyping] and was\nunderwhelmed.\n\nAcross the board they suffer from poor UI, serious bugs, or a lack of important\nfeatures.\n\nIf you want the most functional option, you'll need to pay. Botsociety\n[https://botsociety.io/] is reliable and has the most features, but you're\nlimited to a single export per month unless you upgrade to their $49/month\npremium plan.\n\nAfter reviewing most of the bot prototyping tools available, here are my final\nrecommendations:\n\n> Use Botsociety [https://botsociety.io/] if you need the ability to model\nalternate paths through a conversation and don't mind paying. Supported\nplatforms: Facebook Messenger, Slack, Google Home.\n\n\n> Use Walkie [https://walkiebot.co/] if you're building a Slack bot and don't need\npaths or gif export.\n\n\n> Use Botpreview [https://botpreview.com/] if you're building a Facebook Messenger\nbot and don't need paths.\n\n\nAll of the above have decent user interfaces and are pretty reliable. Read on\nfor a deeper look at each one.\n\nBackground\nI recently wrote about the conversational UI best practices\n[http://blog.paulprae.com/conversational-ui-best-practices/] that we uncovered\nwhile redesigning our chatbot, Neona [http://neona.chat]. Originally, I had\nplanned to write a follow-up post showing concretely how we applied those best\npractices to improve Neona's conversations. To make it easy to follow along in\nthat post, I planned to use one of the fancy bot prototyping tools I had run\nacross. Unfortunately, they either didn't meet my requirements or were too buggy\nto be usable. So instead of walking readers through our redesign process, I\nfigured I'd survey the landscape of bot prototyping tools.\n\n(When I first redesigned Neona, I used flowcharts made with draw.io\n[https://www.draw.io/] to map out the conversation flow. It wasn't great, but it\nwas functional.)\n\nRequirements\nMy main goal was to communicate and explain design decisions, which motivated my\nrequirements.\n\nFirst, I need to be able to easily share animated mockups of the conversation. I\nprefer animation over static images because it more clearly communicates the\nfeel of interacting with the bot.\n\nFurthermore, I want to organize conversations as a tree, not a line. This allows\nyou to handle different user responses more naturally. Bonus points if the app\nlets you share sub-trees of the conversation.\n\nFinally, the prototyping tool should output visuals that mirror one of the\nplatforms Neona runs on. At the moment, that means Skype or Facebook Messenger,\nbut we're working on Slack integration. You can also talk to Neona directly in\nthe browser [http://neona.chat/webchat.html], but I prefer designing interfaces\nwithin the more visually constrained environment of a messaging app.\n\nBotmock [https://botmock.com/]\nBotmock is one of the better tools I tested. It was the only one that let you\norganize your conversation as a tree, and like I mentioned in my requirements,\nit gets a bonus for the ability to export sub-trees. It feels like a tool you\ncould use from beginning to end for designing a conversational interface.\n\nIt also supports a few different visual styles, including Facebook Messenger.\n\nYou create a tree by dragging connections between nodes, similar to mind-mapping\nor patcher programming languages like Max and Pure Data.\n\n\n\nUnfortunately, it's pretty buggy. Export to gif (admittedly in beta) displays a\nstrange zooming behavior:\n\n\n\nNodes also occasionally disappear from the editor page. When this happens,\npreviously deleted nodes re-appear, covering up \"Get Started.\" The vanished\nnodes still take up space in your conversation tree, but they can't be selected\nor deleted.\n\nThis was the deal breaker for me. I wasn't willing to design our UI in a program\nthat could break at any moment.\n\nBotsociety [https://botsociety.io/]\nBotsociety is probably the best tool I found. It takes second place after\nBotmock in terms of features, but is more reliable. There's only limited support\nfor conversation trees, but it does have Facebook Messenger-style visuals, and\nyou can export to gif and video.\n\nMore on conversation trees: Botsociety implements this as a feature called\n\"paths,\" that allows you to specify alternative responses. The major shortcoming\nis that you can't reuse portions of your conversation -- you have to duplicate\nthem.\n\n\n\nA major limitation to be aware of upfront -- the free plan only allows a single\nexport per month. To get more, you need to invite friends (you each get a free\nextra export) or upgrade to their $49/month premium plan.\n\n\n\nA few UI critiques:\n\n * The editor UI feels really busy.\n * In the gif export, I'm not a huge fan of how the keyboard pops up and then\n   hides again after every message from the user.\n\nDespite those limitations, Botsociety probably has the best combination of\nfeatures and stability. If my requirements were different, I would've gone with\nit.\n\nBotpreview [https://botpreview.com/]\nBotpreview has a pretty clean user interface, supports unlimited gif and video\nexport, and allows you to preview your bot's interactions in a Facebook\nMessenger-style interface. Unfortunately, it doesn't offer conversation trees,\nor even something like Botsociety's paths.\n\n\n\nIf you don't need the ability to map different routes through a conversation,\nBotpreview is a decent choice.\n\nBotframe [https://botframe.com/editor/new]\nBotframe is a minimal prototyping tool that has a limited scope and a clear UI.\nIt's also pretty reliable. Unfortunately, it's missing key features. Its\nvisualization of the conversation is static, like you're looking back at one\nthat already happened. It only supports still image export and doesn't allow you\nto model multiple paths through a conversation.\n\n\n\nWalkie [https://walkiebot.co/]\nWalkie is a prototyping tool for Slack bots. Since chatbots have an advantage in\nthe simplicity of their render target (plain text), I'm curious to see how their\nplatform-specific strategy pans out. Despite their solid design, they meet none\nof my requirements. Naturally, they only render your conversation in an\napproximation of Slack's UI. Conversations are modeled as the linear history of\na Slack channel, so no tree-based conversations. Curiously, they only export to\nJSON.\n\n\n\nMockabot [http://mockabot.theartofbuildingbots.com/]\nThis app didn't meet any of my requirements. It only supports linear\nconversation editing, doesn't have an export feature, and the mockup's UI is\nhalf-hearted material design. If Botmock and Botsociety are the closest to\ngetting it right, Mockabot is probably the farthest.\n\n\n\nFinal Thoughts\nFor many domains, one or a few apps stand head and shoulders above the rest. Bot\nprototyping isn't one of them. Botsociety holds the lead currently, but Botmock\ncould jump ahead with the next bugfix release.\n\nThe hardest part of building a bot is that writing is almost too complicated\nfor\nhuman beings to do [http://www.covingtoninnovations.com/mc/WriteThinkLearn.pdf].\nWhile comparing these tools, I was surprised that none of them even began to\ntackle that problem.\n\nIn the past, I've pitched the idea of building a tool like Textio\n[https://textio.com/] for bots to my brilliant and hardworking teammates\n[http://neona.chat#team]. Having done this deep dive, I feel like that's an even\nmore viable option.\n\nP.S. - Bot Stash includes various Sketch UI Kits in their list of prototyping\ntools. Since I don't know Sketch and my visual design skills are limited, I\ndidn't consider them in my evaluation.",
            "feature_image": null,
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2017-10-13T20:06:24.000Z",
            "updated_at": "2021-03-08T22:42:05.000Z",
            "published_at": "2017-11-20T00:20:00.000Z",
            "custom_excerpt": "For many domains, one or a few apps stand head and shoulders above the rest. Bot prototyping isn't one of them. Botsociety holds the lead currently, but Botmock could jump ahead with the next bugfix release.",
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "post",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          },
          {
            "id": "6046b215db781400394a7eb7",
            "uuid": "9447e90c-0748-4422-bc92-fb7159359be4",
            "title": "Resume",
            "slug": "resume",
            "mobiledoc": "{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<!DOCTYPE html>\\n<html>\\n<body>\\n\\n<iframe src=\\\"https://docs.google.com/document/d/1x-4VkfyLOmQdW43U6cOUpz9suCQczxGcxtCh3NcsMNs/edit\\\" width=\\\"2550\\\" height=\\\"3300\\\">\\n</iframe>\\n\\n</body>\\n</html>\\n\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}",
            "html": "<!--kg-card-begin: html--><!DOCTYPE html>\n<html>\n<body>\n\n<iframe src=\"https://docs.google.com/document/d/1x-4VkfyLOmQdW43U6cOUpz9suCQczxGcxtCh3NcsMNs/edit\" width=\"2550\" height=\"3300\">\n</iframe>\n\n</body>\n</html>\n<!--kg-card-end: html-->",
            "comment_id": "6046b215db781400394a7eb7",
            "plaintext": "",
            "feature_image": "__GHOST_URL__/content/images/2021/03/pauls-sky.jpg",
            "featured": 0,
            "status": "published",
            "locale": null,
            "visibility": "public",
            "created_at": "2021-03-08T23:24:05.000Z",
            "updated_at": "2021-03-08T23:45:32.000Z",
            "published_at": "2021-03-08T23:29:35.000Z",
            "custom_excerpt": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "custom_template": null,
            "canonical_url": null,
            "type": "page",
            "email_recipient_filter": "all",
            "newsletter_id": null,
            "lexical": null,
            "show_title_and_feature_image": 1
          }
        ],
        "posts_authors": [
          {
            "id": "5ac380e7c6b630001845859c",
            "post_id": "5a846c0742d1b300183a2064",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "5afb75a2435d6900bf1fdad2",
            "post_id": "5afb75a2435d6900bf1fdad1",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "5afb7f78435d6900bf1fdad4",
            "post_id": "5afb7f78435d6900bf1fdad3",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "5b12d98dc338f600bf8b90a8",
            "post_id": "5b12d98dc338f600bf8b90a7",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "5b7596bac573be00bf3bd063",
            "post_id": "5b7596bac573be00bf3bd062",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e8f",
            "post_id": "6046a1f0db781400394a7e7e",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e90",
            "post_id": "6046a1f0db781400394a7e7f",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e91",
            "post_id": "6046a1f0db781400394a7e80",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e92",
            "post_id": "6046a1f0db781400394a7e81",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e93",
            "post_id": "6046a1f0db781400394a7e82",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e94",
            "post_id": "6046a1f0db781400394a7e83",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e95",
            "post_id": "6046a1f0db781400394a7e84",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e96",
            "post_id": "6046a1f0db781400394a7e85",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e97",
            "post_id": "6046a1f0db781400394a7e86",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e98",
            "post_id": "6046a1f0db781400394a7e87",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e99",
            "post_id": "6046a1f0db781400394a7e88",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e9b",
            "post_id": "6046a1f0db781400394a7e89",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f0db781400394a7e9d",
            "post_id": "6046a1f0db781400394a7e8a",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f1db781400394a7e9e",
            "post_id": "6046a1f0db781400394a7e8b",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f1db781400394a7e9f",
            "post_id": "6046a1f0db781400394a7e8c",
            "author_id": "1",
            "sort_order": 0
          },
          {
            "id": "6046a1f1db781400394a7ea0",
            "post_id": "6046a1f0db781400394a7e8d",
            "author_id": "6046a1efdb781400394a7e70",
            "sort_order": 0
          },
          {
            "id": "6046a1f1db781400394a7ea2",
            "post_id": "6046a1f0db781400394a7e8e",
            "author_id": "6046a1efdb781400394a7e70",
            "sort_order": 0
          },
          {
            "id": "6046b39fdb781400394a7edc",
            "post_id": "6046b215db781400394a7eb7",
            "author_id": "1",
            "sort_order": 0
          }
        ],
        "posts_meta": [
          {
            "id": "5daeec86bcb5d7002d42e913",
            "post_id": "5a846c0742d1b300183a2064",
            "og_image": "__GHOST_URL__/content/images/2018/03/paul-prae-1.jpg",
            "og_title": null,
            "og_description": null,
            "twitter_image": "__GHOST_URL__/content/images/2018/03/paul-prae.jpg",
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "Paul Prae - About",
            "meta_description": "I’m a solution architect with a specialization in artificial intelligence. I build interactive systems that augment cognitive abilities and scale impact.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "5daeec86bcb5d7002d42e914",
            "post_id": "5afb75a2435d6900bf1fdad1",
            "og_image": "__GHOST_URL__/content/images/2018/05/neona-flat-2.png",
            "og_title": null,
            "og_description": null,
            "twitter_image": "__GHOST_URL__/content/images/2018/05/neona-flat-1.png",
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "Meet Neona, a chatbot for learning AI",
            "meta_description": "By interacting with her, you can explore concepts from across the field of AI. By studying her design, you can learn how to build conversational agents.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "5daeec86bcb5d7002d42e915",
            "post_id": "5afb7f78435d6900bf1fdad3",
            "og_image": "__GHOST_URL__/content/images/2018/05/Azure-ML-screenshot-3.png",
            "og_title": null,
            "og_description": null,
            "twitter_image": null,
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "Predicting the Future with Azure Machine Learning ",
            "meta_description": "In this presentation, I focus on supervised learning. My demo predicts the outcomes of patients who went through substance abuse treatment.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "5daeec86bcb5d7002d42e916",
            "post_id": "5b12d98dc338f600bf8b90a7",
            "og_image": "__GHOST_URL__/content/images/2018/06/bot-to-human-handoff-3.png",
            "og_title": null,
            "og_description": null,
            "twitter_image": "__GHOST_URL__/content/images/2018/06/bot-to-human-handoff-2.png",
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "Natural Language Processing Systems",
            "meta_description": "Paul Prae has worked on NLP projects throughout the sales and delivery process. He's architected and led the development of many NLP systems. In this post, he shares some designs.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "5daeec86bcb5d7002d42e917",
            "post_id": "5b7596bac573be00bf3bd062",
            "og_image": "__GHOST_URL__/content/images/2018/08/chats-with-positive-sentiment-3.png",
            "og_title": null,
            "og_description": null,
            "twitter_image": "__GHOST_URL__/content/images/2018/08/chats-with-positive-sentiment-1.png",
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "Data Science Prototypes",
            "meta_description": "Back when AI was making a transition to the cloud, I wanted to show the value of cognitive computing. Here, I share some of my favorite hackathon projects.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "6046a1f0db781400394a7e9a",
            "post_id": "6046a1f0db781400394a7e88",
            "og_image": null,
            "og_title": null,
            "og_description": null,
            "twitter_image": null,
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "The Delusions vs the Effectiveness of Big Data",
            "meta_description": "One argued how the value of big data is overhyped. The other showed how you can exploit large amounts of data to make amazing discoveries.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "6046a1f0db781400394a7e9c",
            "post_id": "6046a1f0db781400394a7e89",
            "og_image": null,
            "og_title": null,
            "og_description": null,
            "twitter_image": null,
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "My Annual Development Plan",
            "meta_description": "We recently wrapped up our 360 reviews at Red Ventures. Part of this process involved assessing ourselves and deciding where we want to take our careers.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          },
          {
            "id": "6046a1f1db781400394a7ea1",
            "post_id": "6046a1f0db781400394a7e8d",
            "og_image": null,
            "og_title": null,
            "og_description": null,
            "twitter_image": null,
            "twitter_title": null,
            "twitter_description": null,
            "meta_title": "Conversational UI Best Practices",
            "meta_description": "In this post, Thomas synthesizes what we learned while creating a chatbot. He provides key takeaways to consider when designing user interfaces for bots.",
            "email_subject": null,
            "frontmatter": null,
            "feature_image_alt": null,
            "feature_image_caption": null,
            "email_only": 0
          }
        ],
        "posts_products": [
          {
            "id": "68a6288911fd6f000159ff8d",
            "post_id": "6046a1f0db781400394a7e8c",
            "product_id": "61f7d1a0905a14002f52cf1f",
            "sort_order": 0
          }
        ],
        "posts_tags": [
          {
            "id": "6046a741db781400394a7ea3",
            "post_id": "5b7596bac573be00bf3bd062",
            "tag_id": "5a846c0542d1b300183a1f82",
            "sort_order": 0
          },
          {
            "id": "6046a7c0db781400394a7ea6",
            "post_id": "5b12d98dc338f600bf8b90a7",
            "tag_id": "5a846c0542d1b300183a1f82",
            "sort_order": 0
          },
          {
            "id": "6046a7ecdb781400394a7eaa",
            "post_id": "5afb7f78435d6900bf1fdad3",
            "tag_id": "5a846c0542d1b300183a1f82",
            "sort_order": 0
          },
          {
            "id": "6046a81ddb781400394a7eac",
            "post_id": "5afb75a2435d6900bf1fdad1",
            "tag_id": "5a846c0542d1b300183a1f82",
            "sort_order": 0
          },
          {
            "id": "6047a5efdb781400394a7eee",
            "post_id": "6046a1f0db781400394a7e8e",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a61fdb781400394a7ef1",
            "post_id": "6046a1f0db781400394a7e8d",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a632db781400394a7ef3",
            "post_id": "6046a1f0db781400394a7e8b",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a648db781400394a7ef5",
            "post_id": "6046a1f0db781400394a7e8a",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a65ddb781400394a7ef7",
            "post_id": "6046a1f0db781400394a7e89",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a674db781400394a7ef9",
            "post_id": "6046a1f0db781400394a7e88",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a6fbdb781400394a7efb",
            "post_id": "6046a1f0db781400394a7e87",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a711db781400394a7efd",
            "post_id": "6046a1f0db781400394a7e86",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a728db781400394a7eff",
            "post_id": "6046a1f0db781400394a7e85",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a73ddb781400394a7f01",
            "post_id": "6046a1f0db781400394a7e84",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a750db781400394a7f03",
            "post_id": "6046a1f0db781400394a7e83",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a763db781400394a7f05",
            "post_id": "6046a1f0db781400394a7e82",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a779db781400394a7f07",
            "post_id": "6046a1f0db781400394a7e81",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a78adb781400394a7f09",
            "post_id": "6046a1f0db781400394a7e80",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a79ddb781400394a7f0b",
            "post_id": "6046a1f0db781400394a7e7f",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          },
          {
            "id": "6047a7b0db781400394a7f0d",
            "post_id": "6046a1f0db781400394a7e7e",
            "tag_id": "6047a5efdb781400394a7eed",
            "sort_order": 0
          }
        ],
        "products": [
          {
            "id": "61f7d1a0905a14002f52cf1f",
            "name": "Ghost Subscription",
            "slug": "ghost-subscription",
            "created_at": "2022-01-31T12:10:08.000Z",
            "updated_at": null,
            "description": null,
            "monthly_price_id": null,
            "yearly_price_id": null,
            "type": "paid",
            "active": 1,
            "welcome_page_url": "/",
            "visibility": "public",
            "trial_days": 0,
            "monthly_price": 500,
            "yearly_price": 5000,
            "currency": "usd"
          },
          {
            "id": "61f7d1ab905a14002f52cf44",
            "name": "Free",
            "slug": "61f7d1ab905a14002f52cf44",
            "created_at": "2022-01-31T12:10:19.000Z",
            "updated_at": null,
            "description": null,
            "monthly_price_id": null,
            "yearly_price_id": null,
            "type": "free",
            "active": 1,
            "welcome_page_url": "/",
            "visibility": "public",
            "trial_days": 0,
            "monthly_price": null,
            "yearly_price": null,
            "currency": null
          }
        ],
        "products_benefits": [],
        "roles": [
          {
            "id": "5a846c0542d1b300183a1f87",
            "name": "Administrator",
            "description": "Administrators",
            "created_at": "2018-02-14T17:04:05.000Z",
            "updated_at": "2018-02-14T17:04:05.000Z"
          },
          {
            "id": "5a846c0542d1b300183a1f88",
            "name": "Editor",
            "description": "Editors",
            "created_at": "2018-02-14T17:04:05.000Z",
            "updated_at": "2018-02-14T17:04:05.000Z"
          },
          {
            "id": "5a846c0542d1b300183a1f89",
            "name": "Author",
            "description": "Authors",
            "created_at": "2018-02-14T17:04:05.000Z",
            "updated_at": "2018-02-14T17:04:05.000Z"
          },
          {
            "id": "5a846c0542d1b300183a1f8a",
            "name": "Contributor",
            "description": "Contributors",
            "created_at": "2018-02-14T17:04:05.000Z",
            "updated_at": "2018-02-14T17:04:05.000Z"
          },
          {
            "id": "5a846c0542d1b300183a1f8b",
            "name": "Owner",
            "description": "Blog Owner",
            "created_at": "2018-02-14T17:04:05.000Z",
            "updated_at": "2018-02-14T17:04:05.000Z"
          },
          {
            "id": "5bb78195e38aa100b5ff26d6",
            "name": "Admin Integration",
            "description": "External Apps",
            "created_at": "2018-10-05T15:21:57.000Z",
            "updated_at": "2018-10-05T15:21:57.000Z"
          },
          {
            "id": "5d4aa05290c2ee002d8d68f1",
            "name": "DB Backup Integration",
            "description": "Internal DB Backup Client",
            "created_at": "2019-08-07T09:56:34.000Z",
            "updated_at": "2019-08-07T09:56:34.000Z"
          },
          {
            "id": "5d5d330ab7f25c002d11112e",
            "name": "Scheduler Integration",
            "description": "Internal Scheduler Client",
            "created_at": "2019-08-21T12:03:22.000Z",
            "updated_at": "2019-08-21T12:03:22.000Z"
          },
          {
            "id": "63da4124e0e34f0031cdbea6",
            "name": "Ghost Explore Integration",
            "description": "Internal Integration for the Ghost Explore directory",
            "created_at": "2023-02-01T10:38:28.000Z",
            "updated_at": null
          },
          {
            "id": "6422ce34562842003186509b",
            "name": "Self-Serve Migration Integration",
            "description": "Core Integration for the Ghost Explore directory",
            "created_at": "2023-03-28T12:23:32.000Z",
            "updated_at": null
          },
          {
            "id": "67d8462f49163f0008419ca3",
            "name": "Super Editor",
            "description": "Editor plus member management",
            "created_at": "2025-03-17T15:56:31.000Z",
            "updated_at": null
          }
        ],
        "roles_users": [
          {
            "id": "5a846c0642d1b300183a1fc1",
            "role_id": "5a846c0542d1b300183a1f89",
            "user_id": "5951f5fca366002ebd5dbef7"
          },
          {
            "id": "5a846c0642d1b300183a2043",
            "role_id": "5a846c0542d1b300183a1f8b",
            "user_id": "1"
          },
          {
            "id": "603cf30ddbc8770039db567f",
            "role_id": "5a846c0542d1b300183a1f87",
            "user_id": "603cf30cdbc8770039db567e"
          },
          {
            "id": "6046a1efdb781400394a7e72",
            "role_id": "5a846c0542d1b300183a1f87",
            "user_id": "6046a1efdb781400394a7e6d"
          },
          {
            "id": "6046a1efdb781400394a7e73",
            "role_id": "5a846c0542d1b300183a1f87",
            "user_id": "6046a1efdb781400394a7e6e"
          },
          {
            "id": "6046a1efdb781400394a7e74",
            "role_id": "5a846c0542d1b300183a1f89",
            "user_id": "6046a1efdb781400394a7e6f"
          },
          {
            "id": "6046a1efdb781400394a7e75",
            "role_id": "5a846c0542d1b300183a1f87",
            "user_id": "6046a1efdb781400394a7e70"
          }
        ],
        "settings": [
          {
            "id": "5a846c0742d1b300183a2044",
            "key": "db_hash",
            "value": "67cfb13d-cc99-47f2-a125-fbfead015ad2",
            "type": "string",
            "created_at": "2018-02-14T17:04:07.000Z",
            "updated_at": "2018-02-14T17:04:07.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2045",
            "key": "next_update_check",
            "value": "1755377896",
            "type": "number",
            "created_at": "2018-02-14T17:04:07.000Z",
            "updated_at": "2025-08-15T20:58:16.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2046",
            "key": "notifications",
            "value": "[]",
            "type": "array",
            "created_at": "2018-02-14T17:04:07.000Z",
            "updated_at": "2018-02-14T17:04:07.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2047",
            "key": "title",
            "value": "Paul Prae",
            "type": "string",
            "created_at": "2013-12-23T12:57:46.000Z",
            "updated_at": "2018-02-14T15:56:30.000Z",
            "group": "site",
            "flags": "PUBLIC"
          },
          {
            "id": "5a846c0742d1b300183a2048",
            "key": "description",
            "value": "Chief AI Officer | 17+ Years Building AI-Driven Products | Mental Health and Neuro Tech Specialist | Fighting for Health and Wealth Accessibility",
            "type": "string",
            "created_at": "2013-12-23T12:57:46.000Z",
            "updated_at": "2024-04-27T15:27:37.000Z",
            "group": "site",
            "flags": "PUBLIC"
          },
          {
            "id": "5a846c0742d1b300183a2049",
            "key": "logo",
            "value": null,
            "type": "string",
            "created_at": "2013-12-23T12:57:46.000Z",
            "updated_at": "2018-02-14T15:56:30.000Z",
            "group": "site",
            "flags": "PUBLIC"
          },
          {
            "id": "5a846c0742d1b300183a204a",
            "key": "cover_image",
            "value": "__GHOST_URL__/content/images/2021/03/pauls-sky-2.jpg",
            "type": "string",
            "created_at": "2013-12-23T12:57:46.000Z",
            "updated_at": "2021-03-08T23:49:25.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a204b",
            "key": "icon",
            "value": "__GHOST_URL__/content/images/2024/04/IMG_3758-1.jpg",
            "type": "string",
            "created_at": "2018-02-14T15:56:29.000Z",
            "updated_at": "2024-04-27T15:31:32.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2050",
            "key": "amp",
            "value": "false",
            "type": "boolean",
            "created_at": "2017-01-12T19:03:51.000Z",
            "updated_at": "2021-02-26T22:34:01.000Z",
            "group": "amp",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2053",
            "key": "facebook",
            "value": null,
            "type": "string",
            "created_at": "2016-05-18T11:51:07.000Z",
            "updated_at": "2018-06-02T22:02:48.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2054",
            "key": "twitter",
            "value": "@praeducer",
            "type": "string",
            "created_at": "2016-05-18T11:51:07.000Z",
            "updated_at": "2018-02-14T15:56:30.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2056",
            "key": "navigation",
            "value": "[{\"label\":\"About\",\"url\":\"/about/\"},{\"label\":\"Blog\",\"url\":\"/tag/blog/\"},{\"label\":\"Portfolio\",\"url\":\"/tag/portfolio/\"},{\"label\":\"Resume\",\"url\":\"/resume/\"}]",
            "type": "array",
            "created_at": "2015-02-28T21:01:17.000Z",
            "updated_at": "2021-03-09T16:56:01.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2058",
            "key": "unsplash",
            "value": "true",
            "type": "boolean",
            "created_at": "2018-02-14T15:56:29.000Z",
            "updated_at": "2018-03-27T22:50:29.000Z",
            "group": "unsplash",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a2059",
            "key": "active_theme",
            "value": "themeforest-AKX8OviT-orca-responsive-ghost-theme",
            "type": "string",
            "created_at": "2018-02-14T17:04:07.000Z",
            "updated_at": "2021-03-08T20:01:38.000Z",
            "group": "theme",
            "flags": "RO"
          },
          {
            "id": "5a846c0742d1b300183a205c",
            "key": "is_private",
            "value": "false",
            "type": "boolean",
            "created_at": "2015-07-19T21:10:32.000Z",
            "updated_at": "2018-02-14T17:04:07.000Z",
            "group": "private",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a205d",
            "key": "password",
            "value": null,
            "type": "string",
            "created_at": "2015-07-19T21:10:32.000Z",
            "updated_at": "2018-02-14T17:04:07.000Z",
            "group": "private",
            "flags": null
          },
          {
            "id": "5a846c0742d1b300183a205e",
            "key": "public_hash",
            "value": "32a61edbe15a22e5ac9ac838a12a81",
            "type": "string",
            "created_at": "2018-02-14T15:56:29.000Z",
            "updated_at": "2018-02-14T15:56:29.000Z",
            "group": "private",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f0a",
            "key": "meta_title",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f0b",
            "key": "meta_description",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f0c",
            "key": "og_image",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f0d",
            "key": "og_title",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f0e",
            "key": "og_description",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f0f",
            "key": "twitter_image",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f10",
            "key": "twitter_title",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d384b5f09284d0038ad2f11",
            "key": "twitter_description",
            "value": null,
            "type": "string",
            "created_at": "2019-07-24T12:24:58.000Z",
            "updated_at": "2019-07-24T12:24:58.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5d9de78bac78030038df5640",
            "key": "default_content_visibility",
            "value": "public",
            "type": "string",
            "created_at": "2019-10-09T14:10:43.000Z",
            "updated_at": "2019-10-09T14:10:43.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "5de94d1b35303900389b2456",
            "key": "secondary_navigation",
            "value": "[]",
            "type": "array",
            "created_at": "2019-12-05T18:54:33.000Z",
            "updated_at": "2021-03-09T17:34:40.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fcdf",
            "key": "admin_session_secret",
            "value": "b1c7dec4ed19a757ddf0f3ea8a6a9058176e0de75186c1fa0315df5384516cd0",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fce0",
            "key": "theme_session_secret",
            "value": "06670cdc2931970214912d5daeb857a71f24d8076579acbd76789e8d46387ec4",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fce1",
            "key": "ghost_public_key",
            "value": "-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAIf98B8LgpbhtvbYkOs8+MHW0IMeAwj3KW8ViFD6FaoVbAKzG/LhIK7erluYORym\npBs2KUBn0v5MzYNHWOXqp2Dtnus8x+dx8u7lzsEQrHTNTDlTMnTse7Bzn4Uf20Fs94Ru6sXv\nOVDVJAa7O4Tg9NSAzfjU8PScBQrAz7O6G3uRAgMBAAE=\n-----END RSA PUBLIC KEY-----\n",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fce2",
            "key": "ghost_private_key",
            "value": "-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQCH/fAfC4KW4bb22JDrPPjB1tCDHgMI9ylvFYhQ+hWqFWwCsxvy4SCu3q5b\nmDkcpqQbNilAZ9L+TM2DR1jl6qdg7Z7rPMfncfLu5c7BEKx0zUw5UzJ07Huwc5+FH9tBbPeE\nburF7zlQ1SQGuzuE4PTUgM341PD0nAUKwM+zuht7kQIDAQABAoGAOxJ/n7ysOUZK5+ci7ExL\n8keRw6Lhxp82jF5aHHLFvmAXzwRme5Z3T/7C1l7FUEDCwF9ChAuVh4ltLD1tzFGl6rvF7k69\nSLkNkYoMYOny/hahH+h+mIi89Y3qQiyHBk6vNuaEb01WHaCCVrtaV7IUfojaAVbXVxJs9qPD\noBqBVQECQQDhlj789FPvjXDVmIm3TlQzRNW0wqQ7dD/UZQ3auea22s1lwFGhsxBx5+es5ubV\nDfTQQG9fzPWmrWtgY9Nido/JAkEAmlN44LmTF7ljO6WEcsN0gW6sOmrFJqyYMwgbV1IIUwAj\n8CL0xcKVN79GiTsotTZBKCYji94f5CXwKMFOiTLBiQJAKQQ0d7HFAS1qcqvFfnrTfG7rnG43\nhkfgwzGMj1R1ypGF/xY0wX3ZY5yS1zE0j1B2TtcoaPbPQ3sMpv3/lUuRYQJAXkdiUD9umg66\njcTXPlkjwrK+6s0xdif4ryxjr5vA1BwA2IIycOIn0K+8wrdubg4gDgwGqTrg1rtKlUtK1Ew6\nqQJBAJcNKI4ElD7lp/JphPb5GqYScFNMeCASt3fT8jXkYFnTNiNHv2ogjTaYi8osR82B38DJ\neS2i8oSeaKUmkdXg/C0=\n-----END RSA PRIVATE KEY-----\n",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fce3",
            "key": "members_public_key",
            "value": "-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBANCqAMZXnutCZnmMEaC0H0GEwPtTtR6Zwm/cLdm0TMRkYQ96C+PvY278aYQ8P4d8\nXVUBFACaoFWPdw/eHo2isbFRMxbKAUzhn2+3J5+sq8TRdgpeUISxASOtDSlh6hJVL82DQR9A\nhLaXhn9WWPeppjVTeE/UiLCXB1Qzqkz5JgyRAgMBAAE=\n-----END RSA PUBLIC KEY-----\n",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fce4",
            "key": "members_private_key",
            "value": "-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQDQqgDGV57rQmZ5jBGgtB9BhMD7U7UemcJv3C3ZtEzEZGEPegvj72Nu/GmE\nPD+HfF1VARQAmqBVj3cP3h6NorGxUTMWygFM4Z9vtyefrKvE0XYKXlCEsQEjrQ0pYeoSVS/N\ng0EfQIS2l4Z/Vlj3qaY1U3hP1IiwlwdUM6pM+SYMkQIDAQABAoGBAJNLThy0gVPknV2ziEX+\noAmp4mgHINUnCN/Zduw0n/QKP5GjP3144KHZv8o+lUHYgjGc/zel7wQ2r38d9kdYz0DqpzWB\nIlZipY4X4TmC8V+Gu7efN/7MMjFS9V5fcP39vRFaPphMAJ+Ko523UljyXzKSP2ZbMiRdOX6m\nPs0hjGf5AkEA+WrxKQOzFJJXmSwwmXfy9nm89IzkNuhRRDCWo3zMb+NrYrR8A3OnlKQ+i6ub\niGRZmJzlYc1UGwrObBxoWQToewJBANYruwaZ7jINdJD0PlJ7HQN2lYjBR83LUc+m2xNtkF06\n24pSHw6qwQ58slleaiX4nsBpwBLGV7IVqxx9ECEn32MCQQDreDsVLZjz4vUwdrerK8MFTwlF\nF36dF9chOX39+uRG/b10AlDSMW4UtrsVwX2k1ph/rCAipWax4RHgnzEkWa3hAkB5iAUwIMdG\nxdXgr4hx+4SiQ3dlS/B3+ikpgVCNvL2P+ec8nY8cHo9ArkfyaJ4pf+Rt4VRqEv0OhDc+CkXN\n2rdrAkAvk9Hv2yyscMwWy9WIM94sdAwxNUvni4hOcEZHkTw30Z/TibKoHxQvDp0Tjj065vZN\nuG1A5d6MYzV6/keFslqC\n-----END RSA PRIVATE KEY-----\n",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5eb3dddc447610003913fce6",
            "key": "members_email_auth_secret",
            "value": "e6985b27f32fbc3964a73b900e7615e0978ce760476ea6e8d40ed1e834280ac79c68d1693af600d5ea074c4819744654c92138ec099c61ce9f74b6b4575d03e0",
            "type": "string",
            "created_at": "2020-05-07T10:07:24.000Z",
            "updated_at": "2020-05-07T10:07:24.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5ee204158b3709004517babc",
            "key": "shared_views",
            "value": "{}",
            "type": "array",
            "created_at": "2020-06-11T14:35:29.000Z",
            "updated_at": "2020-06-11T14:35:29.000Z",
            "group": "views",
            "flags": null
          },
          {
            "id": "5efa0f1e8ba94c00396036cd",
            "key": "portal_name",
            "value": "true",
            "type": "boolean",
            "created_at": "2020-06-29T15:59:57.000Z",
            "updated_at": "2020-06-29T15:59:57.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "5efa0f1e8ba94c00396036ce",
            "key": "portal_button",
            "value": "false",
            "type": "boolean",
            "created_at": "2020-06-29T15:59:57.000Z",
            "updated_at": "2020-06-29T15:59:57.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "5efa0f1e8ba94c00396036cf",
            "key": "portal_plans",
            "value": "[\"free\",\"monthly\",\"yearly\"]",
            "type": "array",
            "created_at": "2020-06-29T15:59:57.000Z",
            "updated_at": "2021-02-26T22:34:01.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b3f",
            "key": "accent_color",
            "value": "#15171A",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "site",
            "flags": "PUBLIC"
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b40",
            "key": "locale",
            "value": "en",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b41",
            "key": "timezone",
            "value": "America/New_York",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b42",
            "key": "codeinjection_head",
            "value": "<style>\n       \n    .center {\n    text-align: center;\n    }\n    .icon {\n        display: inline-block;\n    }\n</style>\n<style>\n    .socials__item,\n    .post-meta__svg-icon,\n    .button--color-transparent  {\n        color: #000 !important;\n    }\n",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2024-04-27T16:03:40.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b43",
            "key": "codeinjection_foot",
            "value": "<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-51117223-3', 'auto');\n  ga('send', 'pageview');\n\n\n</script>\n<script>\n    GOOGLE ANALYTICS CODE OR ANY PREVIOUSLY ADDED SCRIPTS\n</script>\n\n<script type=\"text/javascript\">\n    var ecko_theme_config = {\n        \"disqus_id\": \"\",\n        \"disqus_autoload\": false,\n        \"search_api_key\": \"\",\n        \"social_github\": \"https://github.com/praeducer\",\n        \"social_linkedin\": \"https://www.linkedin.com/in/paulprae\",\n   \n    }\n</script>\n",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2021-03-11T00:11:51.000Z",
            "group": "site",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b49",
            "key": "stripe_plans",
            "value": "[{\"name\":\"Monthly\",\"currency\":\"usd\",\"interval\":\"month\",\"amount\":0},{\"name\":\"Yearly\",\"currency\":\"usd\",\"interval\":\"year\",\"amount\":0}]",
            "type": "array",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b4c",
            "key": "stripe_connect_livemode",
            "value": null,
            "type": "boolean",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b4d",
            "key": "stripe_connect_display_name",
            "value": null,
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b4f",
            "key": "portal_button_style",
            "value": "icon-and-text",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b50",
            "key": "portal_button_icon",
            "value": null,
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b51",
            "key": "portal_button_signup_text",
            "value": "Subscribe",
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b52",
            "key": "mailgun_domain",
            "value": null,
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "email",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b53",
            "key": "mailgun_api_key",
            "value": null,
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "email",
            "flags": null
          },
          {
            "id": "5f0dea9f3dcd0b002d7e8b54",
            "key": "mailgun_base_url",
            "value": null,
            "type": "string",
            "created_at": "2020-07-14T18:20:35.000Z",
            "updated_at": "2020-07-14T18:20:35.000Z",
            "group": "email",
            "flags": null
          },
          {
            "id": "5f1868abe1f153002d77c7ed",
            "key": "amp_gtag_id",
            "value": null,
            "type": "string",
            "created_at": "2020-07-22T18:03:50.000Z",
            "updated_at": "2020-07-22T18:03:50.000Z",
            "group": "amp",
            "flags": null
          },
          {
            "id": "5f5782c7de5ff1002daef4a5",
            "key": "routes_hash",
            "value": "3d180d52c663d173a6be791ef411ed01",
            "type": "string",
            "created_at": "2020-09-08T14:10:31.000Z",
            "updated_at": "2020-09-16T16:30:37.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "5f5782c7de5ff1002daef4a6",
            "key": "members_support_address",
            "value": "",
            "type": "string",
            "created_at": "2020-09-08T14:10:31.000Z",
            "updated_at": "2020-09-08T14:10:31.000Z",
            "group": "members",
            "flags": "PUBLIC,RO"
          },
          {
            "id": "5fd10217300ae9002d9e5377",
            "key": "email_track_opens",
            "value": "true",
            "type": "boolean",
            "created_at": "2020-12-09T17:44:59.000Z",
            "updated_at": "2020-12-09T17:44:59.000Z",
            "group": "email",
            "flags": null
          },
          {
            "id": "6012e55c41bd89002dcdaa06",
            "key": "firstpromoter",
            "value": "false",
            "type": "boolean",
            "created_at": "2021-01-28T16:49:24.000Z",
            "updated_at": "2021-01-28T16:49:24.000Z",
            "group": "firstpromoter",
            "flags": null
          },
          {
            "id": "6012e55c41bd89002dcdaa07",
            "key": "firstpromoter_id",
            "value": null,
            "type": "string",
            "created_at": "2021-01-28T16:49:24.000Z",
            "updated_at": "2021-01-28T16:49:24.000Z",
            "group": "firstpromoter",
            "flags": null
          },
          {
            "id": "61f7d19c905a14002f52cf12",
            "key": "slack_url",
            "value": "",
            "type": "string",
            "created_at": "2022-01-31T12:10:04.000Z",
            "updated_at": "2022-01-31T12:10:04.000Z",
            "group": "slack",
            "flags": null
          },
          {
            "id": "61f7d19c905a14002f52cf13",
            "key": "slack_username",
            "value": "Ghost",
            "type": "string",
            "created_at": "2022-01-31T12:10:04.000Z",
            "updated_at": "2022-01-31T12:10:04.000Z",
            "group": "slack",
            "flags": null
          },
          {
            "id": "61f7d1a1905a14002f52cf2e",
            "key": "members_signup_access",
            "value": "all",
            "type": "string",
            "created_at": "2022-01-31T12:10:09.000Z",
            "updated_at": "2022-01-31T12:10:09.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "61f7d1a5905a14002f52cf31",
            "key": "labs",
            "value": "{}",
            "type": "object",
            "created_at": "2022-01-31T12:10:13.000Z",
            "updated_at": "2022-01-31T12:10:13.000Z",
            "group": "labs",
            "flags": null
          },
          {
            "id": "61f7d1a5905a14002f52cf32",
            "key": "portal_products",
            "value": "[\"61f7d1a0905a14002f52cf1f\"]",
            "type": "array",
            "created_at": "2022-01-31T12:10:13.000Z",
            "updated_at": "2022-01-31T12:10:13.000Z",
            "group": "portal",
            "flags": null
          },
          {
            "id": "61f7d1b1014623003d31a2aa",
            "key": "members_monthly_price_id",
            "value": null,
            "type": "string",
            "created_at": "2022-01-31T12:10:25.000Z",
            "updated_at": "2022-01-31T12:10:25.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "61f7d1b1014623003d31a2ab",
            "key": "members_yearly_price_id",
            "value": null,
            "type": "string",
            "created_at": "2022-01-31T12:10:25.000Z",
            "updated_at": "2022-01-31T12:10:25.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "61f7d1b1014623003d31a2b4",
            "key": "editor_default_email_recipients",
            "value": "visibility",
            "type": "string",
            "created_at": "2022-01-31T12:10:25.000Z",
            "updated_at": "2022-01-31T12:10:25.000Z",
            "group": "editor",
            "flags": null
          },
          {
            "id": "61f7d1b1014623003d31a2b5",
            "key": "editor_default_email_recipients_filter",
            "value": "all",
            "type": "string",
            "created_at": "2022-01-31T12:10:25.000Z",
            "updated_at": "2022-01-31T12:10:25.000Z",
            "group": "editor",
            "flags": null
          },
          {
            "id": "6200fcefbe417c002f50af28",
            "key": "default_content_visibility_tiers",
            "value": "[]",
            "type": "array",
            "created_at": "2022-02-07T11:05:19.000Z",
            "updated_at": "2022-02-07T11:05:19.000Z",
            "group": "members",
            "flags": null
          },
          {
            "id": "626676ecd6a30f003179cb9b",
            "key": "version_notifications",
            "value": "[]",
            "type": "array",
            "created_at": "2022-04-25T11:24:44.000Z",
            "updated_at": "2022-04-25T11:24:44.000Z",
            "group": "core",
            "flags": null
          },
          {
            "id": "63da411ce0e34f0031cdbea5",
            "key": "comments_enabled",
            "value": "off",
            "type": "string",
            "created_at": "2023-02-01T10:38:20.000Z",
            "updated_at": null,
            "group": "comments",
            "flags": null
          },
          {
            "id": "63da41a1e0e34f0031cdbeb1",
            "key": "email_track_clicks",
            "value": "true",
            "type": "boolean",
            "created_at": "2023-02-01T10:40:34.000Z",
            "updated_at": null,
            "group": "email",
            "flags": null
          },
          {
            "id": "63da4215e0e34f0031cdbeb8",
            "key": "members_track_sources",
            "value": "true",
            "type": "boolean",
            "created_at": "2023-02-01T10:42:29.000Z",
            "updated_at": null,
            "group": "members",
            "flags": null
          },
          {
            "id": "63da4267e0e34f0031cdbebe",
            "key": "outbound_link_tagging",
            "value": "true",
            "type": "boolean",
            "created_at": "2023-02-01T10:43:51.000Z",
            "updated_at": null,
            "group": "analytics",
            "flags": null
          },
          {
            "id": "6419a67195645f0031ee902a",
            "key": "last_mentions_report_email_timestamp",
            "value": null,
            "type": "number",
            "created_at": "2023-03-21T12:43:29.000Z",
            "updated_at": null,
            "group": "core",
            "flags": null
          },
          {
            "id": "642eb4b05151970031fe4670",
            "key": "portal_signup_terms_html",
            "value": null,
            "type": "string",
            "created_at": "2023-04-06T13:01:52.000Z",
            "updated_at": null,
            "group": "portal",
            "flags": null
          },
          {
            "id": "642eb4b05151970031fe4671",
            "key": "portal_signup_checkbox_required",
            "value": "false",
            "type": "boolean",
            "created_at": "2023-04-06T13:01:52.000Z",
            "updated_at": null,
            "group": "portal",
            "flags": null
          },
          {
            "id": "6450e8b0a0f6a60008e1e4c6",
            "key": "announcement_content",
            "value": null,
            "type": "string",
            "created_at": "2023-05-02T11:40:48.000Z",
            "updated_at": null,
            "group": "announcement",
            "flags": "PUBLIC"
          },
          {
            "id": "6450e8b0a0f6a60008e1e4c7",
            "key": "announcement_background",
            "value": "dark",
            "type": "string",
            "created_at": "2023-05-02T11:40:48.000Z",
            "updated_at": null,
            "group": "announcement",
            "flags": "PUBLIC"
          },
          {
            "id": "6450e8b0a0f6a60008e1e4c8",
            "key": "pintura",
            "value": "true",
            "type": "boolean",
            "created_at": "2023-05-02T11:40:48.000Z",
            "updated_at": null,
            "group": "pintura",
            "flags": null
          },
          {
            "id": "6450e8b0a0f6a60008e1e4c9",
            "key": "pintura_js_url",
            "value": null,
            "type": "string",
            "created_at": "2023-05-02T11:40:48.000Z",
            "updated_at": null,
            "group": "pintura",
            "flags": null
          },
          {
            "id": "6450e8b0a0f6a60008e1e4ca",
            "key": "pintura_css_url",
            "value": null,
            "type": "string",
            "created_at": "2023-05-02T11:40:48.000Z",
            "updated_at": null,
            "group": "pintura",
            "flags": null
          },
          {
            "id": "6450e8b0a0f6a60008e1e4cb",
            "key": "announcement_visibility",
            "value": "[]",
            "type": "array",
            "created_at": "2023-05-02T11:40:48.000Z",
            "updated_at": null,
            "group": "announcement",
            "flags": null
          },
          {
            "id": "64db419444b7e30008021aef",
            "key": "donations_currency",
            "value": "USD",
            "type": "string",
            "created_at": "2023-08-15T10:12:52.000Z",
            "updated_at": null,
            "group": "donations",
            "flags": null
          },
          {
            "id": "64db419444b7e30008021af0",
            "key": "donations_suggested_amount",
            "value": "500",
            "type": "number",
            "created_at": "2023-08-15T10:12:52.000Z",
            "updated_at": null,
            "group": "donations",
            "flags": null
          },
          {
            "id": "64f72e012290f000080bb9db",
            "key": "recommendations_enabled",
            "value": "false",
            "type": "boolean",
            "created_at": "2023-09-05T14:32:49.000Z",
            "updated_at": null,
            "group": "recommendations",
            "flags": null
          },
          {
            "id": "65ae701793185100084d09fc",
            "key": "portal_default_plan",
            "value": "yearly",
            "type": "string",
            "created_at": "2024-01-22T13:39:35.000Z",
            "updated_at": null,
            "group": "portal",
            "flags": null
          },
          {
            "id": "67160af7d72fab0008869a6e",
            "key": "body_font",
            "value": "",
            "type": "string",
            "created_at": "2024-10-21T09:04:07.000Z",
            "updated_at": null,
            "group": "site",
            "flags": null
          },
          {
            "id": "67160af7d72fab0008869a6f",
            "key": "heading_font",
            "value": "",
            "type": "string",
            "created_at": "2024-10-21T09:04:07.000Z",
            "updated_at": null,
            "group": "site",
            "flags": null
          },
          {
            "id": "6798a95034f8ab0008526283",
            "key": "blocked_email_domains",
            "value": "[]",
            "type": "array",
            "created_at": "2025-01-28T09:54:24.000Z",
            "updated_at": null,
            "group": "members",
            "flags": null
          },
          {
            "id": "67d209eab2071300083d0041",
            "key": "require_email_mfa",
            "value": "false",
            "type": "boolean",
            "created_at": "2025-03-12T22:25:46.000Z",
            "updated_at": null,
            "group": "security",
            "flags": null
          },
          {
            "id": "684ba63092d1890008b1b52a",
            "key": "site_uuid",
            "value": "98fcea43-7fd6-4e03-bbd8-3468473b4258",
            "type": "string",
            "created_at": "2025-06-13T05:16:48.000Z",
            "updated_at": null,
            "group": "core",
            "flags": "PUBLIC,RO"
          },
          {
            "id": "685a612c4ecd2a0008722623",
            "key": "web_analytics",
            "value": "true",
            "type": "boolean",
            "created_at": "2025-06-24T09:26:20.000Z",
            "updated_at": null,
            "group": "analytics",
            "flags": null
          },
          {
            "id": "6863af8d3682630008f9ce4d",
            "key": "social_web",
            "value": "true",
            "type": "boolean",
            "created_at": "2025-07-01T10:51:09.000Z",
            "updated_at": null,
            "group": "social_web",
            "flags": null
          },
          {
            "id": "68776123ef3469000881b70d",
            "key": "explore_ping",
            "value": "true",
            "type": "boolean",
            "created_at": "2025-07-16T09:21:55.000Z",
            "updated_at": null,
            "group": "explore",
            "flags": null
          },
          {
            "id": "68776123ef3469000881b70e",
            "key": "explore_ping_growth",
            "value": "false",
            "type": "boolean",
            "created_at": "2025-07-16T09:21:55.000Z",
            "updated_at": null,
            "group": "explore",
            "flags": null
          }
        ],
        "snippets": [],
        "stripe_prices": [],
        "stripe_products": [],
        "tags": [
          {
            "id": "5a846c0542d1b300183a1f82",
            "name": "Portfolio",
            "slug": "portfolio",
            "description": null,
            "feature_image": "__GHOST_URL__/content/images/2021/03/pauls-sky-1.jpg",
            "parent_id": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "created_at": "2018-02-14T17:04:05.000Z",
            "updated_at": "2021-03-08T23:38:42.000Z",
            "og_image": null,
            "og_title": null,
            "og_description": null,
            "twitter_image": null,
            "twitter_title": null,
            "twitter_description": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "canonical_url": null,
            "accent_color": null
          },
          {
            "id": "6047a5efdb781400394a7eed",
            "name": "Blog",
            "slug": "blog",
            "description": null,
            "feature_image": "__GHOST_URL__/content/images/2021/03/pauls-sky-3.jpg",
            "parent_id": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "created_at": "2021-03-09T16:44:31.000Z",
            "updated_at": "2021-03-09T16:55:39.000Z",
            "og_image": null,
            "og_title": null,
            "og_description": null,
            "twitter_image": null,
            "twitter_title": null,
            "twitter_description": null,
            "codeinjection_head": null,
            "codeinjection_foot": null,
            "canonical_url": null,
            "accent_color": null
          }
        ],
        "users": [
          {
            "id": "1",
            "name": "Paul Prae",
            "slug": "paul-prae",
            "password": "$2a$10$xBg.frSeFOIp0n3CP2/PHuZ5H1QBAyDGrdOuavyQoXJrOeTYUyxZ6",
            "email": "",
            "profile_image": "__GHOST_URL__/content/images/2018/05/paul-prae.jpg",
            "cover_image": "__GHOST_URL__/content/images/2018/06/Paul-Prae---Hiking.jpg",
            "bio": "Paul is an AI solutions architect. He leads the delivery of scalable machine learning and data science systems to enterprise customers. He's currently building microservices using Python and Kafka.",
            "website": "http://www.paulprae.com",
            "location": "Atlanta, GA",
            "facebook": null,
            "twitter": "@praeducer",
            "accessibility": "{\"navigation\":{\"expanded\":{\"posts\":true}},\"nightShift\":true}",
            "status": "active",
            "locale": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "tour": "[\"getting-started\",\"featured-post\",\"static-post\",\"using-the-editor\"]",
            "last_seen": "2025-08-20T19:54:19.000Z",
            "created_at": "2015-07-19T21:10:32.000Z",
            "updated_at": "2025-08-20T19:54:19.000Z",
            "comment_notifications": 1,
            "free_member_signup_notification": 1,
            "paid_subscription_canceled_notification": 0,
            "paid_subscription_started_notification": 1,
            "mention_notifications": 1,
            "milestone_notifications": 1,
            "donation_notifications": 1,
            "recommendation_notifications": 1,
            "threads": null,
            "bluesky": null,
            "mastodon": null,
            "tiktok": null,
            "youtube": null,
            "instagram": null,
            "linkedin": null
          },
          {
            "id": "603cf30cdbc8770039db567e",
            "name": "Adriana Thomas",
            "slug": "adriana",
            "password": "$2a$10$/3GgUrdRIpFLwCmI/.pZlOFSr.Rar46LH80jOZN1H6U2B32aNwWbu",
            "email": "",
            "profile_image": null,
            "cover_image": null,
            "bio": null,
            "website": null,
            "location": null,
            "facebook": null,
            "twitter": null,
            "accessibility": null,
            "status": "active",
            "locale": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "tour": "[\"upload-a-theme\"]",
            "last_seen": "2021-03-11T00:11:17.000Z",
            "created_at": "2021-03-01T13:58:36.000Z",
            "updated_at": "2021-03-11T00:11:17.000Z",
            "comment_notifications": 1,
            "free_member_signup_notification": 1,
            "paid_subscription_canceled_notification": 0,
            "paid_subscription_started_notification": 1,
            "mention_notifications": 1,
            "milestone_notifications": 1,
            "donation_notifications": 1,
            "recommendation_notifications": 1,
            "threads": null,
            "bluesky": null,
            "mastodon": null,
            "tiktok": null,
            "youtube": null,
            "instagram": null,
            "linkedin": null
          },
          {
            "id": "6046a1efdb781400394a7e6d",
            "name": "kottofy",
            "slug": "kottofy",
            "password": "$2a$10$Q.eacqrgEhWoWu77M/fIAuDbPhQDP40NtS6cHFmtWLobgUUi.4yIm",
            "email": "",
            "profile_image": null,
            "cover_image": null,
            "bio": null,
            "website": null,
            "location": null,
            "facebook": null,
            "twitter": null,
            "accessibility": null,
            "status": "locked",
            "locale": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "tour": null,
            "last_seen": null,
            "created_at": "2014-08-22T22:28:53.000Z",
            "updated_at": "2014-08-22T22:28:53.000Z",
            "comment_notifications": 1,
            "free_member_signup_notification": 1,
            "paid_subscription_canceled_notification": 0,
            "paid_subscription_started_notification": 1,
            "mention_notifications": 1,
            "milestone_notifications": 1,
            "donation_notifications": 1,
            "recommendation_notifications": 1,
            "threads": null,
            "bluesky": null,
            "mastodon": null,
            "tiktok": null,
            "youtube": null,
            "instagram": null,
            "linkedin": null
          },
          {
            "id": "6046a1efdb781400394a7e6e",
            "name": "daprae",
            "slug": "daprae",
            "password": "$2a$10$JNmOzTxY6yipsE7aMcydxeaOdcO2JjZp87CgsXNm4znGOdTmybuHy",
            "email": "",
            "profile_image": null,
            "cover_image": null,
            "bio": null,
            "website": null,
            "location": null,
            "facebook": null,
            "twitter": null,
            "accessibility": null,
            "status": "locked",
            "locale": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "tour": null,
            "last_seen": null,
            "created_at": "2014-08-22T22:29:16.000Z",
            "updated_at": "2014-08-22T22:29:16.000Z",
            "comment_notifications": 1,
            "free_member_signup_notification": 1,
            "paid_subscription_canceled_notification": 0,
            "paid_subscription_started_notification": 1,
            "mention_notifications": 1,
            "milestone_notifications": 1,
            "donation_notifications": 1,
            "recommendation_notifications": 1,
            "threads": null,
            "bluesky": null,
            "mastodon": null,
            "tiktok": null,
            "youtube": null,
            "instagram": null,
            "linkedin": null
          },
          {
            "id": "6046a1efdb781400394a7e6f",
            "name": "Reggie Perry",
            "slug": "reggie",
            "password": "$2a$10$77WOB2iAE5nqqkiXapnN8u0WagFAeLmnBrIYIYoInXVTFDQsPeyWS",
            "email": "",
            "profile_image": "//www.gravatar.com/avatar/d82b0852c3b014e4c4eed9bdf38e1c00?s=250&d=mm&r=x",
            "cover_image": null,
            "bio": null,
            "website": null,
            "location": null,
            "facebook": null,
            "twitter": null,
            "accessibility": null,
            "status": "locked",
            "locale": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "tour": null,
            "last_seen": "2017-03-09T17:08:03.000Z",
            "created_at": "2017-03-06T23:22:17.000Z",
            "updated_at": "2017-03-09T17:08:03.000Z",
            "comment_notifications": 1,
            "free_member_signup_notification": 1,
            "paid_subscription_canceled_notification": 0,
            "paid_subscription_started_notification": 1,
            "mention_notifications": 1,
            "milestone_notifications": 1,
            "donation_notifications": 1,
            "recommendation_notifications": 1,
            "threads": null,
            "bluesky": null,
            "mastodon": null,
            "tiktok": null,
            "youtube": null,
            "instagram": null,
            "linkedin": null
          },
          {
            "id": "6046a1efdb781400394a7e70",
            "name": "Thomas Bailey",
            "slug": "thomas-bailey",
            "password": "$2a$10$eYOVOgmppbXZzzFP0kS74O1MsLEL6wI06XSG0rfiXS9MpVvW3y00G",
            "email": "",
            "profile_image": "__GHOST_URL__/content/images/2021/03/thomas-bailey.jpg",
            "cover_image": "__GHOST_URL__/content/images/2017/10/21617997_10154168183792325_5903358109691326481_n.jpg",
            "bio": "Computational Linguist",
            "website": "https://github.com/noise-machines",
            "location": "Athens, GA",
            "facebook": null,
            "twitter": null,
            "accessibility": null,
            "status": "locked",
            "locale": null,
            "visibility": "public",
            "meta_title": null,
            "meta_description": null,
            "tour": null,
            "last_seen": "2017-10-18T15:30:17.000Z",
            "created_at": "2017-10-09T14:58:06.000Z",
            "updated_at": "2021-03-08T23:53:29.000Z",
            "comment_notifications": 1,
            "free_member_signup_notification": 1,
            "paid_subscription_canceled_notification": 0,
            "paid_subscription_started_notification": 1,
            "mention_notifications": 1,
            "milestone_notifications": 1,
            "donation_notifications": 1,
            "recommendation_notifications": 1,
            "threads": null,
            "bluesky": null,
            "mastodon": null,
            "tiktok": null,
            "youtube": null,
            "instagram": null,
            "linkedin": null
          }
        ]
      }
    }
  ]
}